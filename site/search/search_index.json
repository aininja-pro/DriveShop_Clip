{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DriveShop Clip Tracking System","text":""},{"location":"#project-purpose","title":"Project Purpose","text":"<p>The DriveShop Clip Tracking System is a comprehensive media monitoring and analysis platform designed to track automotive journalist coverage of vehicle loans for DriveShop's Fleet Management System (FMS). It automates the discovery, analysis, and tracking of media content across web platforms and YouTube, providing a streamlined workflow for campaign managers to review and approve clips. The system is engineered to process approximately 60 loans daily, finding the first media mention for each loan within a 2-hour nightly processing window, ensuring comprehensive coverage tracking for automotive PR campaigns while maintaining cost-effective AI usage.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Media Discovery: Multi-tier web scraping system with intelligent escalation (Scrapy \u2192 Enhanced Headers \u2192 Headless Rendering)</li> <li>Smart Discovery Mode: Automatic navigation through \"reviews\", \"blog\", or \"news\" sections for unknown media sites</li> <li>YouTube Integration: Specialized handling including RSS feed monitoring and transcript extraction</li> <li>AI-Powered Analysis: GPT-4 integration for content relevance scoring, sentiment analysis, and brand message alignment</li> <li>One Clip Per Loan: Intelligent deduplication to find only the first media mention</li> <li>Person-Outlet Mapping: Automatic association of journalists with their media outlets</li> <li>Streamlit Dashboard: Password-protected web interface for reviewing and approving clips</li> <li>FMS Integration: Export functionality with <code>approved_clips.csv</code> formatted for DriveShop's system</li> <li>RSS Optimization: Direct RSS feed support for faster processing when available</li> <li>Slack Notifications: Real-time alerts for approvals, flags, and system status</li> <li>Cost-Optimized: Designed to maintain ~$1-3 daily AI costs with efficient caching</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>The system follows a modular architecture optimized for reliability and cost-effectiveness:</p>"},{"location":"#core-components","title":"Core Components","text":"<ol> <li>Data Ingestion Pipeline (<code>src/ingest/</code>)</li> <li>Processes daily loan files from FMS</li> <li>Implements smart URL discovery for unknown media outlets</li> <li> <p>Manages one-clip-per-loan logic</p> </li> <li> <p>Tiered Web Crawling Framework (<code>src/crawler/</code>, <code>src/utils/</code>)</p> </li> <li>Level 1: Basic Scrapy with 2s delay, 1 concurrent request</li> <li>Level 2: Enhanced headers and cookie management</li> <li>Level 3: Headless browser rendering for JavaScript-heavy sites</li> <li> <p>RSS feed shortcuts for supported outlets</p> </li> <li> <p>AI Analysis Module (<code>src/analysis/</code>)</p> </li> <li>OpenAI GPT-4 Turbo for cost-effective analysis</li> <li>Evaluates relevance, sentiment, and brand message alignment</li> <li> <p>~$0.01-$0.03 per analysis call</p> </li> <li> <p>Dashboard Application (<code>src/dashboard/</code>)</p> </li> <li>Streamlit-based interface (no custom theming in MVP)</li> <li>AgGrid for efficient data manipulation</li> <li> <p>Password authentication (single shared password)</p> </li> <li> <p>Data Storage Layer</p> </li> <li>Supabase (PostgreSQL) for persistent storage</li> <li>Local SQLite cache for scraping results</li> <li>CSV export for FMS integration</li> </ol>"},{"location":"#data-flow","title":"Data Flow","text":"<ol> <li>Input: Daily loan CSV from FMS or manual upload</li> <li>Discovery: Automated search with smart navigation for unknown sites</li> <li>Processing: Content extraction with tiered escalation</li> <li>Analysis: GPT-4 evaluates content (first mention only)</li> <li>Review: Manual approval via Streamlit dashboard</li> <li>Output: <code>approved_clips.csv</code> for FMS import</li> </ol>"},{"location":"#setup-instructions","title":"Setup Instructions","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11.4 (specific version required)</li> <li>Docker and Docker Compose</li> <li>AWS EC2 t3.small instance (for production)</li> <li>API keys for required services</li> </ul>"},{"location":"#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file based on <code>.env.template</code>:</p> <pre><code># OpenAI Configuration\nOPENAI_API_KEY=your_openai_api_key\n\n# Supabase Database\nSUPABASE_URL=your_supabase_url\nSUPABASE_KEY=your_supabase_anon_key\nDATABASE_PASSWORD=your_database_password\n\n# Google Search API\nGOOGLE_API_KEY=your_google_api_key\nGOOGLE_SEARCH_ENGINE_ID=your_search_engine_id\n\n# Web Scraping Services\nSCRAPING_BEE_API_KEY=your_scraping_bee_key\nSCRAPFLY_API_KEY=your_scrapfly_key\n\n# YouTube API\nYOUTUBE_API_KEY=your_youtube_api_key\n\n# Slack Notifications\nSLACK_WEBHOOK_URL=your_slack_webhook_url\n\n# Streamlit Security\nSTREAMLIT_PASSWORD=your_dashboard_password\n\n# Application Settings\nAPI_BASE_URL=http://localhost:8000\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"#local-development-setup","title":"Local Development Setup","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone &lt;repository_url&gt;    cd DriveShop_Clip</code></p> </li> <li> <p>Create Python 3.11.4 virtual environment:    <code>bash    python3.11 -m venv venv    source venv/bin/activate  # On Windows: venv\\Scripts\\activate</code></p> </li> <li> <p>Install dependencies:    <code>bash    pip install -r requirements.txt</code></p> </li> <li> <p>Install Playwright for Level 3 scraping:    <code>bash    playwright install chromium</code></p> </li> <li> <p>Run the Streamlit dashboard:    <code>bash    streamlit run src/dashboard/app.py</code></p> </li> </ol>"},{"location":"#production-deployment-aws-ec2","title":"Production Deployment (AWS EC2)","text":"<ol> <li>Launch EC2 t3.small instance in us-east-1</li> <li>Configure security group:</li> <li>Port 22 (SSH)</li> <li>Port 8501 (Streamlit)</li> <li>SSH into instance and clone repository</li> <li>Create <code>.env</code> file with production credentials</li> <li>Build and run with Docker:    <code>bash    docker-compose up -d</code></li> <li>Access dashboard at <code>http://&lt;ec2-public-ip&gt;:8501</code></li> </ol>"},{"location":"#usage-guide","title":"Usage Guide","text":""},{"location":"#1-input-file-format","title":"1. Input File Format","text":"<p>Create CSV/Excel files with these required columns: - <code>Work Order Number</code> (WO): Unique FMS identifier - <code>Loan ID</code>: Unique loan identifier - <code>First Name</code>: Journalist's first name - <code>Last Name</code>: Journalist's last name - <code>Media Outlet</code>: Known outlet or \"Unknown\" - <code>Model</code>: Vehicle model (e.g., \"X5\", \"Accord\") - <code>Start Date</code>: Loan start date - <code>End Date</code>: Loan end date - <code>URL</code> (optional): Direct link or RSS feed</p>"},{"location":"#2-dashboard-workflow","title":"2. Dashboard Workflow","text":"<ol> <li>Access: Navigate to Streamlit URL, enter password</li> <li>Upload: Use file uploader for loan CSV/Excel</li> <li>Process: Click \"Process Loans Without Clips\"</li> <li>System finds first mention only</li> <li>Uses RSS feeds when available</li> <li>Follows discovery logic for unknown sites</li> <li>Review: AgGrid displays found clips with:</li> <li>Relevance score (0-100)</li> <li>Sentiment analysis</li> <li>Brand message alignment</li> <li>Full content preview</li> <li>Actions:</li> <li>Approve/Reject individual clips</li> <li>Update media outlet assignments</li> <li>Trigger re-analysis if needed</li> <li>Export: Download <code>approved_clips.csv</code> for FMS</li> </ol>"},{"location":"#3-automated-processing","title":"3. Automated Processing","text":"<p>Nightly cron job runs at 2 AM: - Processes all pending loans - Sends Slack notifications for new clips - Completes within 2-hour window - Maintains one-clip-per-loan rule</p>"},{"location":"#4-special-features","title":"4. Special Features","text":"<ul> <li>RSS Shortcuts: Add RSS feed URLs to bypass crawling</li> <li>YouTube RSS: Use <code>https://www.youtube.com/feeds/videos.xml?channel_id=CHANNEL_ID</code></li> <li>Discovery Mode: Automatically navigates review/blog sections</li> <li>Model Variations: \"X5\" matches \"BMW X5\", \"2024 X5\", etc.</li> </ul>"},{"location":"#performance-limitations","title":"Performance &amp; Limitations","text":""},{"location":"#expected-performance","title":"Expected Performance","text":"<ul> <li>Daily Volume: ~60 loans with 2-4 URLs each</li> <li>Processing Time: &lt;2 hours for nightly batch</li> <li>Success Rate: &gt;90% content extraction</li> <li>AI Costs: $1-3 per day (~100 GPT-4 calls)</li> </ul>"},{"location":"#current-limitations-mvp","title":"Current Limitations (MVP)","text":"<ul> <li>English content only</li> <li>No direct FMS API integration</li> <li>Single password authentication</li> <li>No mobile interface</li> <li>Manual deployment process</li> <li>One clip per loan maximum</li> </ul>"},{"location":"#file-structure","title":"File Structure","text":"<pre><code>DriveShop_Clip/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u2514\u2500\u2500 app.py              # Streamlit dashboard\n\u2502   \u251c\u2500\u2500 ingest/\n\u2502   \u2502   \u251c\u2500\u2500 ingest.py           # Core processing pipeline\n\u2502   \u2502   \u2514\u2500\u2500 ingest_database.py  # Database processing\n\u2502   \u251c\u2500\u2500 analysis/\n\u2502   \u2502   \u2514\u2500\u2500 gpt_analysis.py     # AI analysis (GPT-4)\n\u2502   \u251c\u2500\u2500 crawler/\n\u2502   \u2502   \u2514\u2500\u2500 crawler/spiders/    # Scrapy configurations\n\u2502   \u251c\u2500\u2500 creatoriq/              # Future integration\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 database.py         # Supabase client\n\u2502       \u251c\u2500\u2500 crawler_manager.py  # Tiered escalation\n\u2502       \u251c\u2500\u2500 youtube_api.py      # YouTube integration\n\u2502       \u2514\u2500\u2500 model_variations.py # Vehicle matching\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 person_outlets_mapping.csv  # Journalist mappings\n\u2502   \u2514\u2500\u2500 media_sources.csv           # JS-heavy site flags\n\u251c\u2500\u2500 documentation/              # Project documentation\n\u251c\u2500\u2500 docker-compose.yml         # Container orchestration\n\u251c\u2500\u2500 Dockerfile                 # Container definition\n\u251c\u2500\u2500 requirements.txt           # Python 3.11.4 deps\n\u2514\u2500\u2500 .env.template             # Environment template\n</code></pre>"},{"location":"#deployment-notes","title":"Deployment Notes","text":""},{"location":"#production-checklist","title":"Production Checklist","text":"<ol> <li>Infrastructure:</li> <li>AWS EC2 t3.small in us-east-1</li> <li>Security groups configured</li> <li> <p>Elastic IP recommended</p> </li> <li> <p>Environment:</p> </li> <li>Production <code>.env</code> file secured</li> <li>Secrets never in Git</li> <li> <p>API keys rotated quarterly</p> </li> <li> <p>Monitoring:</p> </li> <li>Slack webhook configured</li> <li>CloudWatch for EC2 metrics</li> <li> <p>Daily cost monitoring for APIs</p> </li> <li> <p>Cron Schedule:    <code>bash    # Nightly processing at 2 AM    0 2 * * * cd /app &amp;&amp; docker-compose run app python src/ingest/ingest_database.py</code></p> </li> </ol>"},{"location":"#api-quotas-limits","title":"API Quotas &amp; Limits","text":"<ul> <li>YouTube API: 10,000 units/day quota</li> <li>OpenAI: Monitor token usage (~$1-3/day target)</li> <li>ScrapingBee/ScrapFly: Check monthly limits</li> <li>Google Search: 100 queries/day free tier</li> </ul>"},{"location":"#maintenance-tasks","title":"Maintenance Tasks","text":"<ul> <li>Weekly: Clear SQLite cache if &gt;1GB</li> <li>Monthly: Review API usage and costs</li> <li>Quarterly: Update dependencies, rotate API keys</li> <li>As Needed: Update <code>person_outlets_mapping.csv</code></li> </ul>"},{"location":"#future-enhancements-post-mvp","title":"Future Enhancements (Post-MVP)","text":"<ul> <li>Direct FMS API integration</li> <li>Multi-user authentication with roles</li> <li>Airtable or advanced database storage</li> <li>Email notification system</li> <li>Mobile-responsive interface</li> <li>Multi-language content support</li> <li>Advanced analytics dashboard</li> <li>Automated model training for relevance</li> </ul>"},{"location":"#support-troubleshooting","title":"Support &amp; Troubleshooting","text":"<ul> <li>Logs: Check <code>app.log</code> for detailed debugging</li> <li>Common Issues:</li> <li>403 errors: Site needs Level 3 escalation</li> <li>No clips found: Check discovery keywords</li> <li>Slow processing: Review concurrent limits</li> <li>Documentation: See <code>/documentation</code> folder</li> <li>Slack Channel: Real-time system notifications</li> </ul> <p>For technical support, reference the implementation plan and technical documentation, or contact the development team.</p>"},{"location":"analysis/","title":"Analysis Module Documentation","text":""},{"location":"analysis/#module-srcanalysisgpt_analysispy","title":"Module: <code>src/analysis/gpt_analysis.py</code>","text":""},{"location":"analysis/#purpose","title":"Purpose","text":"<p>The Analysis module provides AI-powered content analysis using OpenAI's GPT-4 Turbo model to evaluate media clips for relevance, sentiment, and brand alignment. It implements sophisticated parsing strategies, cost-optimization through pre-filtering, and specialized prompts for different content types (YouTube videos vs. web articles). The module is designed to extract strategic marketing insights from automotive media content while maintaining cost efficiency.</p>"},{"location":"analysis/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"analysis/#core-analysis-functions","title":"Core Analysis Functions","text":"<pre><code>def analyze_clip(content: str, make: str, model: str, \n                max_retries: int = 3, url: str = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Comprehensive automotive sentiment analysis using GPT-4.\n    Implements pre-filters to save API costs and specialized prompts.\n    Returns detailed analysis with marketing insights or None if fails.\n    \"\"\"\n\ndef analyze_clip_relevance_only(content: str, make: str, model: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Cost-optimized analysis for relevance scoring only.\n    Skips detailed sentiment analysis to reduce API costs.\n    Used in database ingestion pipeline.\n    \"\"\"\n</code></pre>"},{"location":"analysis/#json-parsing-functions","title":"JSON Parsing Functions","text":"<pre><code>def parse_json_with_fallbacks(response_text: str) -&gt; dict:\n    \"\"\"\n    Robust JSON parsing with 4 escalating strategies:\n    1. Clean and parse normally\n    2. Aggressive comma fixing\n    3. Manual field extraction with regex\n    4. Return None (no fallback data)\n    \"\"\"\n\ndef clean_json_response(response_text: str) -&gt; str:\n    \"\"\"\n    Cleans GPT responses by removing markdown blocks and fixing JSON issues.\n    Handles edge cases that commonly cause parsing failures.\n    \"\"\"\n</code></pre>"},{"location":"analysis/#helper-functions","title":"Helper Functions","text":"<pre><code>def get_openai_key() -&gt; Optional[str]:\n    \"\"\"\n    Retrieves OpenAI API key from environment variables.\n    Returns None if not configured.\n    \"\"\"\n\ndef flexible_model_match(title: str, model: str) -&gt; bool:\n    \"\"\"\n    Intelligent vehicle model matching in content.\n    Handles variations like \"X5\" matching \"BMW X5\", \"2024 X5\", etc.\n    \"\"\"\n</code></pre>"},{"location":"analysis/#gptanalyzer-class","title":"GPTAnalyzer Class","text":"<pre><code>class GPTAnalyzer:\n    \"\"\"\n    Class-based analyzer for content using OpenAI's GPT-4 Turbo.\n    Handles API calls with retry logic and rate limiting.\n    \"\"\"\n\n    def analyze_content(self, content: str, vehicle_make: str, \n                       vehicle_model: str, max_retries: int = 3,\n                       timeout: int = 60) -&gt; Dict[str, Any]:\n        \"\"\"\n        Analyzes content for relevance, sentiment, and brand messaging.\n        Includes retry logic and rate limiting.\n        \"\"\"\n</code></pre>"},{"location":"analysis/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"analysis/#inputs","title":"Inputs","text":"<ol> <li>Content Analysis:</li> <li><code>content</code>: Article text or YouTube transcript (HTML or plain text)</li> <li><code>make</code>: Vehicle manufacturer (e.g., \"Honda\")</li> <li><code>model</code>: Vehicle model (e.g., \"Accord\")</li> <li><code>url</code>: Optional URL for content type detection</li> <li> <p><code>max_retries</code>: Number of retry attempts (default: 3)</p> </li> <li> <p>Pre-Filter Thresholds:</p> </li> <li>Minimum content length: 200 characters</li> <li>Printable character ratio: &gt;80%</li> <li>Generic page indicators: &lt;2 matches</li> </ol>"},{"location":"analysis/#outputs","title":"Outputs","text":"<ol> <li> <p>Comprehensive Analysis (analyze_clip):    <code>json    {      \"relevance_score\": 0-10,      \"overall_score\": 0-10,      \"overall_sentiment\": \"positive/neutral/negative\",      \"brand_alignment\": true/false,      \"summary\": \"Executive summary for CMO briefing\",      \"recommendation\": \"Strategic marketing recommendation\",      \"aspects\": {        \"performance\": {\"score\": 0-10, \"note\": \"...\"},        \"exterior_design\": {\"score\": 0-10, \"note\": \"...\"},        \"interior_comfort\": {\"score\": 0-10, \"note\": \"...\"},        \"technology\": {\"score\": 0-10, \"note\": \"...\"},        \"value\": {\"score\": 0-10, \"note\": \"...\"}      },      \"pros\": [\"key positive 1\", \"key positive 2\"],      \"cons\": [\"key concern 1\", \"key concern 2\"],      \"key_mentions\": [\"topic 1\", \"topic 2\"],      \"video_quotes\": [\"memorable quote 1\", \"memorable quote 2\"]    }</code></p> </li> <li> <p>Relevance-Only Analysis:    <code>json    {      \"relevance_score\": 0-10    }</code></p> </li> <li> <p>Failure Response: <code>None</code> (no mock data)</p> </li> </ol>"},{"location":"analysis/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport openai  # Version 0.27.0 (older client format)\nfrom dotenv import load_dotenv\n\n# Internal Modules\nfrom src.utils.logger import setup_logger\nfrom src.utils.rate_limiter import rate_limiter\nfrom src.utils.content_extractor import extract_article_content\n</code></pre>"},{"location":"analysis/#cost-optimization-features","title":"Cost Optimization Features","text":""},{"location":"analysis/#pre-filters-save-api-costs","title":"Pre-Filters (Save API Costs)","text":"<ol> <li>Content Length Check: Skip if &lt;200 characters</li> <li>Keyword Validation: Must mention make OR model</li> <li>Content Quality Check: &gt;80% printable characters</li> <li>Generic Page Detection: Avoid category/index pages</li> </ol>"},{"location":"analysis/#processing-optimizations","title":"Processing Optimizations","text":"<ul> <li>Content truncation at 12,000 characters</li> <li>Relevance-only mode for batch processing</li> <li>Caching in upstream modules</li> <li>Smart retry with exponential backoff</li> </ul>"},{"location":"analysis/#prompt-templates","title":"Prompt Templates","text":""},{"location":"analysis/#youtube-video-analysis","title":"YouTube Video Analysis","text":"<ul> <li>Strategic marketing focus</li> <li>Creator intelligence (influence tier, audience archetype)</li> <li>Competitive intelligence</li> <li>Purchase intent signals</li> <li>Viral potential assessment</li> <li>Action items for marketing team</li> </ul>"},{"location":"analysis/#web-article-analysis","title":"Web Article Analysis","text":"<ul> <li>Publication credibility assessment</li> <li>SEO/Digital influence factor</li> <li>Editorial stance detection</li> <li>Media relationship strategy</li> <li>Brand narrative impact</li> <li>Shareable quote extraction</li> </ul>"},{"location":"analysis/#error-handling","title":"Error Handling","text":"<ol> <li>API Key Missing: Returns <code>None</code> instead of mock data</li> <li>JSON Parsing Failures: </li> <li>4 escalating strategies</li> <li>Manual field extraction fallback</li> <li>No fake data on complete failure</li> <li>API Errors:</li> <li>Rate limit handling with 30s wait</li> <li>Timeout retry with 5s wait</li> <li>Exponential backoff (2^attempt seconds)</li> <li>Content Issues:</li> <li>HTML extraction for web articles</li> <li>Truncation for oversized content</li> <li>Graceful handling of corrupted data</li> </ol>"},{"location":"analysis/#strategic-analysis-features","title":"Strategic Analysis Features","text":""},{"location":"analysis/#marketing-intelligence","title":"Marketing Intelligence","text":"<ul> <li>Impact Scoring: 1-10 scale for CMO attention</li> <li>Brand Perception: Ascending/Stable/Declining trajectory</li> <li>Messaging Opportunities: Extracted from content</li> <li>Risk Identification: Brand vulnerabilities</li> </ul>"},{"location":"analysis/#competitive-analysis","title":"Competitive Analysis","text":"<ul> <li>Positioning vs. competitors</li> <li>Market advantages highlighted</li> <li>Vulnerabilities exposed</li> <li>Differentiation opportunities</li> </ul>"},{"location":"analysis/#creatorpublication-analysis","title":"Creator/Publication Analysis","text":"<ul> <li>YouTube: Influence tier, audience archetype, credibility</li> <li>Articles: Publication credibility, reach, editorial stance</li> <li>Relationship recommendations (Engage/Monitor/Ignore)</li> </ul>"},{"location":"analysis/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Token Usage: ~4,000 tokens per analysis</li> <li>API Costs: $0.01-$0.03 per call</li> <li>Response Time: 2-10 seconds typical</li> <li>Retry Strategy: Max 3 attempts with backoff</li> <li>Rate Limiting: Integrated with global limiter</li> </ul>"},{"location":"analysis/#security-considerations","title":"Security Considerations","text":"<ul> <li>API key stored in environment variables</li> <li>No sensitive data logged</li> <li>Content sanitization before processing</li> <li>Secure API communication</li> <li>No data persistence in module</li> </ul>"},{"location":"api-utils/","title":"API Utilities Documentation","text":""},{"location":"api-utils/#module-srcutilsyoutube_apipy","title":"Module: <code>src/utils/youtube_api.py</code>","text":""},{"location":"api-utils/#purpose","title":"Purpose","text":"<p>The YouTube API module provides a clean interface to YouTube Data API v3 for searching and retrieving video information. It implements intelligent search strategies with model variations, date filtering, and comprehensive error handling while respecting API quotas.</p>"},{"location":"api-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"api-utils/#youtubeapiclient-class","title":"YouTubeAPIClient Class","text":"<pre><code>class YouTubeAPIClient:\n    \"\"\"\n    YouTube Data API v3 client with rate limiting and error handling.\n    Provides video search and channel video listing capabilities.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with API key from environment.\"\"\"\n</code></pre>"},{"location":"api-utils/#core-methods","title":"Core Methods","text":"<pre><code>def search_videos(self, make: str, model: str, journalist_name: str = None,\n                 start_date: str = None, end_date: str = None) -&gt; List[Dict]:\n    \"\"\"\n    Search YouTube videos with intelligent query building.\n    Implements model variations for better matching.\n    \"\"\"\n\ndef get_channel_videos(self, channel_id: str, max_results: int = 10) -&gt; List[Dict]:\n    \"\"\"\n    Retrieve latest videos from a specific channel.\n    Used for RSS feed alternative.\n    \"\"\"\n\ndef get_video_details(self, video_id: str) -&gt; Optional[Dict]:\n    \"\"\"\n    Get detailed information for a specific video.\n    Includes duration, views, likes, etc.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#helper-methods","title":"Helper Methods","text":"<pre><code>def generate_model_variations(model: str) -&gt; List[str]:\n    \"\"\"\n    Generate search variations for vehicle models.\n    Example: \"X5\" \u2192 [\"X5\", \"BMW X5\", \"X5 BMW\"]\n    \"\"\"\n\ndef _make_api_request(self, endpoint: str, params: Dict) -&gt; Optional[Dict]:\n    \"\"\"\n    Internal method for API requests with rate limiting.\n    Handles errors and returns parsed JSON.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"api-utils/#inputs","title":"Inputs","text":"<pre><code># Video search\n{\n    'make': 'Honda',\n    'model': 'Accord',\n    'journalist_name': 'Doug DeMuro',\n    'start_date': '2024-01-01',\n    'end_date': '2024-01-31'\n}\n\n# Channel videos\n{\n    'channel_id': 'UC123abc...',\n    'max_results': 10\n}\n</code></pre>"},{"location":"api-utils/#outputs","title":"Outputs","text":"<pre><code># Video data structure\n{\n    'video_id': 'dQw4w9WgXcQ',\n    'title': '2024 Honda Accord Review',\n    'channel': 'Car Reviews',\n    'channel_id': 'UC123...',\n    'published_at': '2024-01-15T10:00:00Z',\n    'description': 'Full review of...',\n    'thumbnail': 'https://i.ytimg.com/...',\n    'url': 'https://youtube.com/watch?v=...'\n}\n</code></pre>"},{"location":"api-utils/#dependencies","title":"Dependencies","text":"<pre><code>import os\nimport requests\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\n\nfrom src.utils.logger import logger\nfrom src.utils.rate_limiter import rate_limiter\n</code></pre>"},{"location":"api-utils/#api-integration-patterns","title":"API Integration Patterns","text":"<ol> <li>Rate Limiting: Centralized rate limiter before each API call</li> <li>Error Handling: Graceful degradation with empty results</li> <li>Query Building: Progressive search from specific to general</li> <li>Pagination: Handles up to 50 results per request</li> <li>Quota Management: Efficient API usage with targeted queries</li> </ol>"},{"location":"api-utils/#module-srcutilsyoutube_handlerpy","title":"Module: <code>src/utils/youtube_handler.py</code>","text":""},{"location":"api-utils/#purpose_1","title":"Purpose","text":"<p>The YouTube Handler provides comprehensive YouTube content extraction using multiple methods including RSS feeds, direct scraping, and API fallbacks. It specializes in extracting video metadata and transcripts without requiring API keys for most operations.</p>"},{"location":"api-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"api-utils/#core-functions","title":"Core Functions","text":"<pre><code>def extract_youtube_content(url: str, make: str = None, model: str = None,\n                          journalist_name: str = None,\n                          start_date: str = None, end_date: str = None) -&gt; Dict:\n    \"\"\"\n    Main extraction function with multiple fallback methods.\n    Attempts RSS \u2192 Direct scraping \u2192 API.\n    \"\"\"\n\ndef extract_channel_videos_from_rss(channel_url: str, limit: int = 10) -&gt; List[Dict]:\n    \"\"\"\n    Extract videos via YouTube RSS feed (no API needed).\n    Fast and reliable for recent videos.\n    \"\"\"\n\ndef extract_channel_id(channel_url: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract channel ID using multiple methods.\n    Handles various YouTube URL formats.\n    \"\"\"\n\ndef check_model_in_title(title: str, model: str) -&gt; bool:\n    \"\"\"\n    Flexible model matching in video titles.\n    Handles variations and partial matches.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#transcript-extraction","title":"Transcript Extraction","text":"<pre><code>def get_video_transcript(video_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract video transcript/captions.\n    Falls back to metadata if transcript unavailable.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#content-extraction-methods","title":"Content Extraction Methods","text":"<ol> <li>RSS Feed Method (Preferred)</li> <li>No API key required</li> <li>Returns latest 15 videos</li> <li>Structured XML data</li> <li> <p>Very fast and reliable</p> </li> <li> <p>Direct Scraping</p> </li> <li>BeautifulSoup HTML parsing</li> <li>Pattern matching for data extraction</li> <li> <p>Works when RSS unavailable</p> </li> <li> <p>API Fallback</p> </li> <li>Uses YouTube API client</li> <li>Only when other methods fail</li> <li>Preserves API quota</li> </ol>"},{"location":"api-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"api-utils/#inputs_1","title":"Inputs","text":"<pre><code>{\n    'url': 'https://youtube.com/watch?v=abc123',\n    'make': 'Toyota',\n    'model': 'Camry',\n    'journalist_name': 'Alex on Autos',\n    'start_date': '2024-01-01',\n    'end_date': '2024-01-31'\n}\n</code></pre>"},{"location":"api-utils/#outputs_1","title":"Outputs","text":"<pre><code>{\n    'content': 'Video transcript or description...',\n    'title': '2024 Toyota Camry Review',\n    'url': 'https://youtube.com/watch?v=abc123',\n    'published_date': '2024-01-15',\n    'channel': 'Alex on Autos',\n    'views': '50000',\n    'duration': 'PT15M30S'\n}\n</code></pre>"},{"location":"api-utils/#module-srcutilsgoogle_searchpy","title":"Module: <code>src/utils/google_search.py</code>","text":""},{"location":"api-utils/#purpose_2","title":"Purpose","text":"<p>The Google Search module provides web search capabilities using Google Custom Search API with Bing as a fallback. It implements intelligent query building, result filtering, and attribution verification for finding automotive journalism content.</p>"},{"location":"api-utils/#key-functionsclasses_2","title":"Key Functions/Classes","text":""},{"location":"api-utils/#core-search-functions","title":"Core Search Functions","text":"<pre><code>def search_google(query: str, site: str = None, num_results: int = 10) -&gt; List[Dict]:\n    \"\"\"\n    Search using Google Custom Search API.\n    Supports site-specific searches.\n    \"\"\"\n\ndef search_for_article(make: str, model: str, journalist_name: str,\n                      media_outlet: str = None, start_date: str = None,\n                      end_date: str = None) -&gt; List[Dict]:\n    \"\"\"\n    Comprehensive article search with multiple strategies.\n    Implements fallback from Google to Bing.\n    \"\"\"\n\nasync def search_for_article_async(make: str, model: str, \n                                  journalist_name: str, **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Async version for concurrent searches.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#result-processing","title":"Result Processing","text":"<pre><code>def filter_and_score_results(results: List[Dict], make: str, model: str,\n                           journalist_name: str, start_date: str = None) -&gt; List[Dict]:\n    \"\"\"\n    Score and filter search results.\n    Implements relevance scoring algorithm.\n    \"\"\"\n\ndef verify_author_attribution(url: str, content: str, \n                            journalist_name: str) -&gt; Dict:\n    \"\"\"\n    Verify if content is actually by the journalist.\n    Returns attribution strength.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#search-strategy","title":"Search Strategy","text":"<ol> <li>Query Building Hierarchy:</li> <li>Full query: <code>\"John Doe\" Honda Accord review site:example.com</code></li> <li>Without quotes: <code>John Doe Honda Accord review</code></li> <li>Model only: <code>Honda Accord review site:example.com</code></li> <li> <p>Broad search: <code>Honda Accord</code></p> </li> <li> <p>Result Scoring Algorithm:</p> </li> <li>URL keyword matches (+3 per keyword)</li> <li>Title matches for make/model/journalist</li> <li>Domain restrictions</li> <li> <p>Date filtering</p> </li> <li> <p>Attribution Verification:</p> </li> <li>Byline extraction</li> <li>Author meta tag checking</li> <li>Name proximity to article markers</li> </ol>"},{"location":"api-utils/#api-configuration","title":"API Configuration","text":"<pre><code># Google Custom Search\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\nGOOGLE_SEARCH_ENGINE_ID = os.getenv('GOOGLE_SEARCH_ENGINE_ID')\n\n# Bing Search (Fallback)\nBING_API_KEY = os.getenv('BING_SEARCH_API_KEY')\n</code></pre>"},{"location":"api-utils/#module-srcutilsscraping_beepy","title":"Module: <code>src/utils/scraping_bee.py</code>","text":""},{"location":"api-utils/#purpose_3","title":"Purpose","text":"<p>The ScrapingBee module provides integration with ScrapingBee API for handling JavaScript-heavy sites and bypassing anti-bot measures. It implements domain-specific configurations and intelligent retry mechanisms.</p>"},{"location":"api-utils/#key-functionsclasses_3","title":"Key Functions/Classes","text":""},{"location":"api-utils/#scrapingbeeclient-class","title":"ScrapingBeeClient Class","text":"<pre><code>class ScrapingBeeClient:\n    \"\"\"\n    ScrapingBee API client with retry logic and domain configurations.\n    Handles JavaScript rendering and premium proxies.\n    \"\"\"\n\n    def __init__(self, api_key: str = None):\n        \"\"\"Initialize with API key validation.\"\"\"\n</code></pre>"},{"location":"api-utils/#core-methods_1","title":"Core Methods","text":"<pre><code>def scrape(self, url: str, render_js: bool = True, \n          premium_proxy: bool = False, **kwargs) -&gt; Optional[str]:\n    \"\"\"\n    Scrape URL with configurable options.\n    Implements retry with exponential backoff.\n    \"\"\"\n\ndef get_credits_used(self) -&gt; Dict[str, int]:\n    \"\"\"\n    Check API credit usage.\n    Returns used and remaining credits.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#domain-specific-configurations","title":"Domain-Specific Configurations","text":"<pre><code># YouTube configuration\nif 'youtube.com' in url:\n    params.update({\n        'block_ads': True,\n        'wait': 3000,\n        'wait_for': '#description'\n    })\n\n# Spotlight configuration  \nif 'spotlightautomotive.com' in url:\n    params.update({\n        'wait': 5000,\n        'screenshot': False\n    })\n</code></pre>"},{"location":"api-utils/#retry-strategy","title":"Retry Strategy","text":"<ol> <li>Max Retries: 3 attempts</li> <li>Exponential Backoff: 1s \u2192 2s \u2192 4s</li> <li>Error-Specific Handling:</li> <li>403: Upgrade to premium proxy</li> <li>422: Validation error (no retry)</li> <li>429: Rate limit (longer wait)</li> <li>500: Server error (retry)</li> </ol>"},{"location":"api-utils/#module-srcutilsscrapfly_clientpy","title":"Module: <code>src/utils/scrapfly_client.py</code>","text":""},{"location":"api-utils/#purpose_4","title":"Purpose","text":"<p>The ScrapFly module provides the most sophisticated web scraping integration with circuit breaker pattern, advanced rate limiting, and comprehensive error handling. It serves as the primary premium scraping service.</p>"},{"location":"api-utils/#key-functionsclasses_4","title":"Key Functions/Classes","text":""},{"location":"api-utils/#scrapflyclient-class","title":"ScrapFlyClient Class","text":"<pre><code>class ScrapFlyClient:\n    \"\"\"\n    ScrapFly API client with circuit breaker and rate limiting.\n    Most robust scraping solution in the stack.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with API key and circuit breaker state.\"\"\"\n</code></pre>"},{"location":"api-utils/#core-methods_2","title":"Core Methods","text":"<pre><code>def scrape(self, url: str, render_js: bool = True,\n          country: str = \"US\", **kwargs) -&gt; Optional[str]:\n    \"\"\"\n    Scrape with circuit breaker protection.\n    Implements sophisticated rate limiting.\n    \"\"\"\n\ndef extract_scrapfly_content(self, url: str, config: Dict = None) -&gt; Optional[str]:\n    \"\"\"\n    Main extraction method with fallback strategies.\n    Handles retries and circuit breaker logic.\n    \"\"\"\n</code></pre>"},{"location":"api-utils/#circuit-breaker-implementation","title":"Circuit Breaker Implementation","text":"<pre><code>def _check_circuit_breaker(self) -&gt; bool:\n    \"\"\"Check if circuit breaker is open.\"\"\"\n\ndef _update_circuit_breaker(self, success: bool):\n    \"\"\"Update circuit breaker state based on result.\"\"\"\n\ndef _reset_circuit_breaker(self):\n    \"\"\"Reset circuit breaker after timeout.\"\"\"\n</code></pre>"},{"location":"api-utils/#advanced-features","title":"Advanced Features","text":"<ol> <li>Circuit Breaker Pattern:</li> <li>Opens after 3 consecutive failures</li> <li>5-minute timeout before reset</li> <li> <p>Prevents cascading failures</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Minimum 2-second delay between requests</li> <li>Retry-after header parsing</li> <li> <p>Progressive backoff on rate limits</p> </li> <li> <p>Configuration Options:    <code>python    {        'asp': True,           # Anti-bot bypass        'country': 'US',       # Geo-location        'rendering_wait': 3000, # JS wait time        'retry': False,        # Internal retry        'cache': True,         # Response caching        'debug': True          # Debug info    }</code></p> </li> </ol>"},{"location":"api-utils/#error-handling-sophistication","title":"Error Handling Sophistication","text":"<ol> <li>Rate Limit Detection:    ```python    # Parses multiple rate limit formats</li> <li>\"Request rate limit exceeded (2/sec)\"</li> <li>\"API rate limit exceeded\"</li> <li> <p>Retry-After headers    ```</p> </li> <li> <p>Progressive Degradation:</p> </li> <li> <p>Standard request \u2192 ASP mode \u2192 Country change \u2192 Circuit break</p> </li> <li> <p>Credit Monitoring:</p> </li> <li>Tracks usage in response headers</li> <li>Logs credit consumption</li> <li>Warns on low credits</li> </ol>"},{"location":"api-utils/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Success Rate: ~99.9% with ASP enabled</li> <li>Average Response Time: 3-5 seconds</li> <li>Circuit Breaker Recovery: 5 minutes</li> <li>Rate Limit Compliance: Automatic</li> </ul>"},{"location":"core-utils/","title":"Core Utilities Documentation","text":""},{"location":"core-utils/#module-srcutilsdatabasepy","title":"Module: <code>src/utils/database.py</code>","text":""},{"location":"core-utils/#purpose","title":"Purpose","text":"<p>The Database module provides a comprehensive interface to the Supabase PostgreSQL database, managing all data persistence for the clip tracking system. It implements intelligent retry logic, workflow management, and provides analytics capabilities while maintaining a singleton pattern for efficient connection management.</p>"},{"location":"core-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"core-utils/#data-classes","title":"Data Classes","text":"<pre><code>@dataclass\nclass ProcessingRun:\n    \"\"\"\n    Tracks batch processing metadata.\n    Attributes: id, started_at, completed_at, total_loans, \n                successful_clips, failed_clips\n    \"\"\"\n\n@dataclass  \nclass ClipData:\n    \"\"\"\n    Comprehensive clip data model.\n    Includes: loan info, content, sentiment analysis, workflow status,\n              UI display data, media outlet mapping\n    \"\"\"\n</code></pre>"},{"location":"core-utils/#databasemanager-class","title":"DatabaseManager Class","text":"<pre><code>class DatabaseManager:\n    \"\"\"\n    Singleton database manager for all Supabase operations.\n    Groups operations by functionality: runs, clips, retry logic, analytics.\n    \"\"\"\n\n    def __init__(self, supabase_url: str, supabase_key: str):\n        \"\"\"Initialize Supabase client with credentials.\"\"\"\n</code></pre>"},{"location":"core-utils/#core-operations","title":"Core Operations","text":""},{"location":"core-utils/#processing-runs","title":"Processing Runs","text":"<pre><code>def create_processing_run(self) -&gt; str:\n    \"\"\"Create new processing run, returns UUID.\"\"\"\n\ndef update_processing_run(self, run_id: str, total_loans: int, \n                         successful: int, failed: int):\n    \"\"\"Update run statistics upon completion.\"\"\"\n</code></pre>"},{"location":"core-utils/#clip-management","title":"Clip Management","text":"<pre><code>def store_clip(self, clip_data: ClipData) -&gt; bool:\n    \"\"\"Store found clip with all metadata.\"\"\"\n\ndef get_pending_clips(self, filters: Dict = None) -&gt; List[Dict]:\n    \"\"\"Retrieve clips awaiting review with optional filters.\"\"\"\n\ndef update_clip_status(self, wo_number: str, status: str) -&gt; bool:\n    \"\"\"Update clip workflow status (pending/approved/rejected).\"\"\"\n\ndef update_clip_sentiment(self, wo_number: str, sentiment_data: Dict) -&gt; bool:\n    \"\"\"Store comprehensive GPT sentiment analysis.\"\"\"\n</code></pre>"},{"location":"core-utils/#smart-retry-logic","title":"Smart Retry Logic","text":"<pre><code>def store_failed_attempt(self, loan_data: Dict, reason: str, \n                        run_id: str) -&gt; bool:\n    \"\"\"Record failed search attempt with retry scheduling.\"\"\"\n\ndef should_retry_wo(self, wo_number: str) -&gt; Tuple[bool, Optional[str]]:\n    \"\"\"\n    Intelligent retry decision based on:\n    - Failure type and count\n    - Time since last attempt\n    - Configured retry intervals\n    \"\"\"\n\ndef get_smart_retry_summary(self) -&gt; Dict:\n    \"\"\"Analytics on retry patterns and success rates.\"\"\"\n</code></pre>"},{"location":"core-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"core-utils/#inputs","title":"Inputs","text":"<ol> <li> <p>Environment Configuration:    <code>bash    SUPABASE_URL=https://xxx.supabase.co    SUPABASE_ANON_KEY=eyJhbGc...</code></p> </li> <li> <p>Clip Data Structure:    <code>python    ClipData(        wo_number=\"WO12345\",        model=\"Honda Accord\",        clip_url=\"https://...\",        content=\"Article text...\",        relevance_score=85,        sentiment_data={...},        status=\"pending\",        run_id=\"uuid-123\",        media_outlet=\"Car and Driver\"    )</code></p> </li> </ol>"},{"location":"core-utils/#outputs","title":"Outputs","text":"<ol> <li>Database Records: Direct storage to Supabase tables</li> <li>Query Results: Lists of dictionaries with clip data</li> <li>Analytics: Aggregated statistics and summaries</li> <li>Retry Decisions: Tuple of (should_retry: bool, reason: str)</li> </ol>"},{"location":"core-utils/#dependencies","title":"Dependencies","text":"<pre><code># External\nfrom supabase import create_client, Client\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Internal\nfrom src.utils.logger import logger\n</code></pre>"},{"location":"core-utils/#retry-strategy","title":"Retry Strategy","text":""},{"location":"core-utils/#retry-intervals-by-failure-type","title":"Retry Intervals by Failure Type","text":"<ul> <li>No Content Found: 7 days</li> <li>Technical Issues: 1 day  </li> <li>Timeout: 2 days</li> <li>Access Denied: 14 days</li> <li>Default: 3 days</li> </ul>"},{"location":"core-utils/#retry-decision-logic","title":"Retry Decision Logic","text":"<ol> <li>Check last attempt timestamp</li> <li>Apply interval based on failure reason</li> <li>Consider failure count (max retries)</li> <li>Return decision with explanation</li> </ol>"},{"location":"core-utils/#error-handling","title":"Error Handling","text":"<ul> <li>All operations wrapped in try-except</li> <li>Detailed error logging with context</li> <li>Graceful degradation (returns empty/False)</li> <li>Connection testing available</li> <li>No exceptions propagated to caller</li> </ul>"},{"location":"core-utils/#module-srcutilsconfigpy","title":"Module: <code>src/utils/config.py</code>","text":""},{"location":"core-utils/#purpose_1","title":"Purpose","text":"<p>The Config module centralizes all configuration constants for the web crawling system, including domain-specific strategies, content discovery patterns, and API configurations. It provides a single source of truth for all scraping-related settings.</p>"},{"location":"core-utils/#key-configuration-sections","title":"Key Configuration Sections","text":""},{"location":"core-utils/#domain-management","title":"Domain Management","text":"<pre><code>API_SCRAPER_DOMAINS = [\n    'motortrend.com',\n    'caranddriver.com',\n    'autoblog.com',\n    # ... domains requiring API scraping\n]\n\nGENERIC_INDEX_PATTERNS = [\n    r'/category/',\n    r'/tag/',\n    r'/page/\\d+',\n    # ... patterns indicating index pages\n]\n</code></pre>"},{"location":"core-utils/#content-discovery","title":"Content Discovery","text":"<pre><code>ARTICLE_INDICATORS = ['review', 'test', 'drive', 'first-look']\n\nMODEL_CLEANUP_PATTERNS = {\n    r'\\s+hybrid$': '',\n    r'\\s+phev$': '',\n    r'^20\\d{2}\\s+': '',\n    # ... model name normalization\n}\n\nSEARCH_QUERY_TEMPLATES = [\n    '\"{journalist}\" {make} {model} review',\n    'site:{domain} {make} {model}',\n    # ... search query formats\n]\n</code></pre>"},{"location":"core-utils/#api-configurations","title":"API Configurations","text":"<pre><code>SCRAPINGBEE_CONFIG = {\n    'render_js': True,\n    'premium_proxy': True,\n    'country_code': 'us',\n    'timeout': 30000\n}\n\nGOOGLE_SEARCH_CONFIG = {\n    'max_results': 10,\n    'safe_search': 'off',\n    'search_type': 'web'\n}\n\nCACHE_CONFIG = {\n    'enabled': True,\n    'ttl_hours': 168,  # 7 days\n    'max_size_mb': 500\n}\n</code></pre>"},{"location":"core-utils/#crawler-configuration","title":"Crawler Configuration","text":"<pre><code>CRAWLER_TIERS = {\n    'tier1': {'delay': 1, 'timeout': 30},\n    'tier2': {'delay': 2, 'timeout': 45},\n    'tier3': {'delay': 3, 'timeout': 60},\n    'tier4': {'premium_proxy': True},\n    'tier5': {'js_scenario': True},\n    'tier6': {'stealth_mode': True}\n}\n\nUSER_AGENTS = {\n    'chrome': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n    'firefox': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15)...',\n    'mobile': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1)...'\n}\n</code></pre>"},{"location":"core-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"core-utils/#inputs_1","title":"Inputs","text":"<ul> <li>No inputs (pure configuration module)</li> <li>Values are hardcoded constants</li> </ul>"},{"location":"core-utils/#outputs_1","title":"Outputs","text":"<ul> <li>Configuration dictionaries and lists</li> <li>Used via imports: <code>from src.utils.config import API_SCRAPER_DOMAINS</code></li> </ul>"},{"location":"core-utils/#dependencies_1","title":"Dependencies","text":"<p>None (pure Python module)</p>"},{"location":"core-utils/#usage-patterns","title":"Usage Patterns","text":"<pre><code>from src.utils.config import CRAWLER_TIERS, SCRAPINGBEE_CONFIG\n\n# Use tier configuration\ntier_config = CRAWLER_TIERS['tier3']\ndelay = tier_config['delay']\n\n# Use API configuration\napi_params = SCRAPINGBEE_CONFIG.copy()\napi_params['url'] = target_url\n</code></pre>"},{"location":"core-utils/#module-srcutilsloggerpy","title":"Module: <code>src/utils/logger.py</code>","text":""},{"location":"core-utils/#purpose_2","title":"Purpose","text":"<p>The Logger module provides standardized logging configuration across the entire application. It ensures consistent log formatting, creates necessary directory structures, and provides both console and file output for debugging and monitoring.</p>"},{"location":"core-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"core-utils/#setup-function","title":"Setup Function","text":"<pre><code>def setup_logger(name: str, level=logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Create a configured logger instance.\n\n    Args:\n        name: Logger name (typically __name__)\n        level: Logging level (default: INFO)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n</code></pre>"},{"location":"core-utils/#default-logger-instance","title":"Default Logger Instance","text":"<pre><code># Pre-configured logger for immediate use\nlogger = setup_logger('clip_tracking')\n</code></pre>"},{"location":"core-utils/#expected-inputsoutputs_2","title":"Expected Inputs/Outputs","text":""},{"location":"core-utils/#inputs_2","title":"Inputs","text":"<ol> <li>Logger Name: Module name or custom identifier</li> <li>Log Level: <code>logging.DEBUG</code>, <code>logging.INFO</code>, <code>logging.WARNING</code>, etc.</li> </ol>"},{"location":"core-utils/#outputs_2","title":"Outputs","text":"<ol> <li>Console Output: Formatted log messages to stdout</li> <li>File Output: Logs written to <code>logs/app.log</code></li> <li>Logger Instance: Configured Python logger object</li> </ol>"},{"location":"core-utils/#dependencies_2","title":"Dependencies","text":"<pre><code>import logging\nimport os\nfrom pathlib import Path\n</code></pre>"},{"location":"core-utils/#configuration-details","title":"Configuration Details","text":""},{"location":"core-utils/#log-format","title":"Log Format","text":"<pre><code>%(asctime)s - %(name)s - %(levelname)s - %(message)s\n</code></pre> <p>Example output:</p> <pre><code>2024-01-20 10:30:45 - clip_tracking - INFO - Processing started\n2024-01-20 10:30:46 - database - ERROR - Connection failed: timeout\n</code></pre>"},{"location":"core-utils/#file-structure","title":"File Structure","text":"<pre><code>project_root/\n\u251c\u2500\u2500 logs/\n\u2502   \u2514\u2500\u2500 app.log\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 utils/\n        \u2514\u2500\u2500 logger.py\n</code></pre>"},{"location":"core-utils/#usage-patterns_1","title":"Usage Patterns","text":""},{"location":"core-utils/#module-specific-logger","title":"Module-Specific Logger","text":"<pre><code>from src.utils.logger import setup_logger\n\n# Create module-specific logger\nlogger = setup_logger(__name__)\n\n# Use in module\nlogger.info(\"Starting processing\")\nlogger.error(f\"Failed to process: {error}\")\nlogger.debug(f\"Debug info: {data}\")\n</code></pre>"},{"location":"core-utils/#direct-import","title":"Direct Import","text":"<pre><code>from src.utils.logger import logger\n\n# Use pre-configured logger\nlogger.info(\"Quick logging without setup\")\n</code></pre>"},{"location":"core-utils/#error-handling_1","title":"Error Handling","text":"<ul> <li>Creates log directory if missing</li> <li>Prevents duplicate handlers</li> <li>Handles file permission issues gracefully</li> <li>Falls back to console only if file logging fails</li> </ul>"},{"location":"core-utils/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Single file handler shared across loggers</li> <li>Automatic log rotation not implemented (consider for production)</li> <li>No async logging (synchronous writes)</li> <li>Log level filtering at handler level</li> </ul>"},{"location":"crawler-utils/","title":"Crawler Utilities Documentation","text":""},{"location":"crawler-utils/#module-srcutilsenhanced_crawler_managerpy","title":"Module: <code>src/utils/enhanced_crawler_manager.py</code>","text":""},{"location":"crawler-utils/#purpose","title":"Purpose","text":"<p>The Enhanced Crawler Manager implements a sophisticated 6+ tier escalation system for web content extraction. It provides intelligent content quality detection, caching, index page discovery, and seamless integration with multiple scraping services. This is the primary crawler orchestrator used in production.</p>"},{"location":"crawler-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"crawler-utils/#enhancedcrawlermanager-class","title":"EnhancedCrawlerManager Class","text":"<pre><code>class EnhancedCrawlerManager:\n    \"\"\"\n    Advanced web content extraction with multi-tier escalation.\n    Features caching, quality checks, and intelligent routing.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with cache manager and API clients.\"\"\"\n</code></pre>"},{"location":"crawler-utils/#core-methods","title":"Core Methods","text":"<pre><code>def extract_content(self, url: str, expected_make: str = None, \n                   expected_model: str = None, journalist_name: str = None,\n                   publication_date: str = None, max_tier: int = 7) -&gt; Dict:\n    \"\"\"\n    Main extraction method with progressive escalation.\n    Returns: {content, source, tier_used, byline_author, attribution_strength}\n    \"\"\"\n\ndef is_content_high_quality(self, content: str, url: str,\n                           expected_make: str = None, \n                           expected_model: str = None) -&gt; bool:\n    \"\"\"\n    Validates content quality using multiple heuristics.\n    Detects index pages and validates relevance.\n    \"\"\"\n</code></pre>"},{"location":"crawler-utils/#tier-implementation-methods","title":"Tier Implementation Methods","text":"<pre><code>def try_tier_1_basic_http(self, url: str) -&gt; Optional[str]:\n    \"\"\"Basic HTTP request with minimal headers.\"\"\"\n\ndef try_tier_2_enhanced_http(self, url: str) -&gt; Optional[str]:\n    \"\"\"Enhanced HTTP with browser-like headers and cookies.\"\"\"\n\ndef try_tier_3_rss_feed(self, url: str, expected_make: str, \n                       expected_model: str) -&gt; Optional[str]:\n    \"\"\"Extract content via RSS feed if available.\"\"\"\n\ndef try_tier_4_scrapfly(self, url: str) -&gt; Optional[str]:\n    \"\"\"Premium scraping via ScrapFly API.\"\"\"\n\ndef try_tier_5_5_index_page_discovery(self, url: str, expected_make: str,\n                                     expected_model: str) -&gt; Optional[str]:\n    \"\"\"Discover article from index page.\"\"\"\n\ndef try_tier_6_google_search(self, url: str, expected_make: str,\n                            expected_model: str, journalist_name: str) -&gt; Optional[str]:\n    \"\"\"Find article via Google Search.\"\"\"\n</code></pre>"},{"location":"crawler-utils/#tiered-escalation-strategy","title":"Tiered Escalation Strategy","text":"<ol> <li>Tier 1 - Basic HTTP (0.5-1s)</li> <li>Simple requests with standard headers</li> <li> <p>Fastest, works for ~40% of sites</p> </li> <li> <p>Tier 2 - Enhanced HTTP (1-2s)</p> </li> <li>Browser-like headers and session management</li> <li>Content quality validation</li> <li> <p>Works for ~60% of sites</p> </li> <li> <p>Tier 3 - RSS Feed (1-2s)</p> </li> <li>Structured data extraction</li> <li>Free and reliable when available</li> <li> <p>~10% of sites support RSS</p> </li> <li> <p>Tier 4 - ScrapFly API (2-5s)</p> </li> <li>Premium residential proxies</li> <li>Handles anti-bot protection</li> <li> <p>99.9% success rate</p> </li> <li> <p>Tier 5.5 - Index Discovery (5-10s)</p> </li> <li>Crawls category/index pages</li> <li>Finds specific article links</li> <li> <p>Similar to YouTube processing</p> </li> <li> <p>Tier 6 - Google Search (3-5s)</p> </li> <li>Searches for specific article</li> <li>Uses journalist + make/model</li> <li> <p>Fallback discovery method</p> </li> <li> <p>Tier 7 - Playwright (10-20s)</p> </li> <li>Full browser automation</li> <li>Last resort option</li> <li>Handles complex JS sites</li> </ol>"},{"location":"crawler-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"crawler-utils/#inputs","title":"Inputs","text":"<pre><code>{\n    'url': 'https://example.com/article',\n    'expected_make': 'Honda',\n    'expected_model': 'Accord',\n    'journalist_name': 'John Doe',\n    'publication_date': '2024-01-15',\n    'max_tier': 7  # Stop at this tier\n}\n</code></pre>"},{"location":"crawler-utils/#outputs","title":"Outputs","text":"<pre><code>{\n    'content': 'Extracted article text...',\n    'source': 'tier_2_enhanced_http',\n    'tier_used': 2,\n    'byline_author': 'John Doe',\n    'attribution_strength': 'strong',\n    'cached': False,\n    'extraction_time': 1.5\n}\n</code></pre>"},{"location":"crawler-utils/#dependencies","title":"Dependencies","text":"<pre><code># External\nimport requests\nfrom bs4 import BeautifulSoup\nimport feedparser\n\n# Internal  \nfrom src.utils.cache_manager import CacheManager\nfrom src.utils.content_extractor import ContentExtractor\nfrom src.utils.scrapfly_client import ScrapFlyClient\nfrom src.utils.google_search import search_google\nfrom src.utils.browser_crawler import BrowserCrawler\n</code></pre>"},{"location":"crawler-utils/#module-srcutilscontent_extractorpy","title":"Module: <code>src/utils/content_extractor.py</code>","text":""},{"location":"crawler-utils/#purpose_1","title":"Purpose","text":"<p>The Content Extractor provides intelligent HTML content extraction with site-specific handlers and multiple fallback strategies. It focuses on extracting clean, relevant article text while filtering out navigation, ads, and other non-content elements.</p>"},{"location":"crawler-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"crawler-utils/#contentextractor-class","title":"ContentExtractor Class","text":"<pre><code>class ContentExtractor:\n    \"\"\"\n    Intelligent content extraction from HTML.\n    Implements site-specific and generic extraction methods.\n    \"\"\"\n</code></pre>"},{"location":"crawler-utils/#core-methods_1","title":"Core Methods","text":"<pre><code>def extract_article_content(html_content: str, url: str = None,\n                          expected_topic: str = None,\n                          extraction_type: str = \"default\") -&gt; Optional[str]:\n    \"\"\"\n    Main extraction method with multiple strategies.\n    Handles site-specific extractors and fallbacks.\n    \"\"\"\n\ndef extract_basic_content(soup: BeautifulSoup) -&gt; str:\n    \"\"\"\n    Generic content extraction using common selectors.\n    Tries multiple article body indicators.\n    \"\"\"\n\ndef extract_alternative_methods(soup: BeautifulSoup, url: str,\n                              expected_topic: str) -&gt; Optional[str]:\n    \"\"\"\n    Alternative extraction when basic method fails.\n    Includes title-based, density-based, and full-text methods.\n    \"\"\"\n</code></pre>"},{"location":"crawler-utils/#site-specific-handlers","title":"Site-Specific Handlers","text":"<pre><code>def extract_fliphtml5_content(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract from FlipHTML5 embedded viewers.\"\"\"\n\ndef extract_spotlightepnews_content(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Handle PDF viewers and flipbook format.\"\"\"\n\ndef extract_thegentlemanracer_content(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Remove sidebar content and extract main article.\"\"\"\n</code></pre>"},{"location":"crawler-utils/#content-extraction-strategies","title":"Content Extraction Strategies","text":"<ol> <li>Site-Specific Extraction</li> <li>Custom handlers for known problematic sites</li> <li> <p>Handles embedded viewers, PDFs, flipbooks</p> </li> <li> <p>Basic Extraction</p> </li> <li>Common article selectors (article, .content, etc.)</li> <li>Paragraph concatenation</li> <li> <p>Clean text output</p> </li> <li> <p>Alternative Methods</p> </li> <li>Title-based discovery</li> <li>Paragraph density analysis  </li> <li>Longest text block</li> <li> <p>Full text with filtering</p> </li> <li> <p>Content Quality Scoring</p> </li> <li>Paragraph count and structure</li> <li>Text length validation</li> <li>Navigation element detection</li> <li>Article indicator presence</li> </ol>"},{"location":"crawler-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"crawler-utils/#inputs_1","title":"Inputs","text":"<pre><code>{\n    'html_content': '&lt;html&gt;...&lt;/html&gt;',\n    'url': 'https://example.com/article',\n    'expected_topic': 'Honda Accord',\n    'extraction_type': 'default'  # or 'basic', 'alternative'\n}\n</code></pre>"},{"location":"crawler-utils/#outputs_1","title":"Outputs","text":"<ul> <li>Clean article text (string)</li> <li>None if extraction fails</li> <li>Filtered content without navigation/ads</li> </ul>"},{"location":"crawler-utils/#module-srcutilsbrowser_crawlerpy","title":"Module: <code>src/utils/browser_crawler.py</code>","text":""},{"location":"crawler-utils/#purpose_2","title":"Purpose","text":"<p>The Browser Crawler provides thread-safe headless browser automation using Playwright. It implements anti-detection measures and handles JavaScript-heavy sites that require full browser rendering.</p>"},{"location":"crawler-utils/#key-functionsclasses_2","title":"Key Functions/Classes","text":""},{"location":"crawler-utils/#browsercrawler-class","title":"BrowserCrawler Class","text":"<pre><code>class BrowserCrawler:\n    \"\"\"\n    Thread-safe Playwright browser automation.\n    Creates fresh browser instance per crawl.\n    \"\"\"\n\n    def crawl(self, url: str, wait_for: str = \"networkidle\",\n             wait_time: int = 5, scroll: bool = True) -&gt; str:\n        \"\"\"\n        Crawl URL with headless browser.\n        Returns page HTML after rendering.\n        \"\"\"\n</code></pre>"},{"location":"crawler-utils/#mockbrowsercrawler-class","title":"MockBrowserCrawler Class","text":"<pre><code>class MockBrowserCrawler:\n    \"\"\"\n    Mock implementation for testing.\n    Returns simple HTML without browser overhead.\n    \"\"\"\n</code></pre>"},{"location":"crawler-utils/#browser-configuration","title":"Browser Configuration","text":"<ol> <li>Stealth Features</li> <li>Hides webdriver properties</li> <li>Randomized viewport sizes</li> <li> <p>Realistic user agent strings</p> </li> <li> <p>Navigation Strategies</p> </li> <li><code>networkidle</code>: Wait for network quiet</li> <li><code>domcontentloaded</code>: Wait for DOM ready</li> <li> <p><code>load</code>: Wait for page load event</p> </li> <li> <p>Resource Management</p> </li> <li>Fresh browser per crawl</li> <li>Automatic cleanup</li> <li>Thread-safe operation</li> </ol>"},{"location":"crawler-utils/#expected-inputsoutputs_2","title":"Expected Inputs/Outputs","text":""},{"location":"crawler-utils/#inputs_2","title":"Inputs","text":"<pre><code>{\n    'url': 'https://example.com',\n    'wait_for': 'networkidle',\n    'wait_time': 5,\n    'scroll': True\n}\n</code></pre>"},{"location":"crawler-utils/#outputs_2","title":"Outputs","text":"<ul> <li>Rendered HTML content (string)</li> <li>Empty string on failure</li> </ul>"},{"location":"crawler-utils/#module-srcutilsdate_extractorpy","title":"Module: <code>src/utils/date_extractor.py</code>","text":""},{"location":"crawler-utils/#purpose_3","title":"Purpose","text":"<p>The Date Extractor provides comprehensive publication date extraction from web content using multiple methods including structured data, meta tags, CSS selectors, and text patterns.</p>"},{"location":"crawler-utils/#key-functionsclasses_3","title":"Key Functions/Classes","text":""},{"location":"crawler-utils/#core-functions","title":"Core Functions","text":"<pre><code>def extract_published_date(html_content: str, url: str = None) -&gt; Optional[str]:\n    \"\"\"\n    Extract publication date using multiple strategies.\n    Returns ISO format date string or None.\n    \"\"\"\n\ndef extract_date_from_structured_data(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract from JSON-LD, microdata schemas.\"\"\"\n\ndef extract_date_from_meta_tags(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract from Open Graph, Dublin Core meta tags.\"\"\"\n\ndef extract_date_from_selectors(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract using common CSS date selectors.\"\"\"\n\ndef extract_date_from_text_patterns(text: str) -&gt; Optional[str]:\n    \"\"\"Extract using regex patterns for date formats.\"\"\"\n</code></pre>"},{"location":"crawler-utils/#extraction-methods-hierarchy","title":"Extraction Methods Hierarchy","text":"<ol> <li>Structured Data (Most reliable)</li> <li>JSON-LD schemas</li> <li>Microdata markup</li> <li> <p>Schema.org properties</p> </li> <li> <p>Meta Tags</p> </li> <li>Open Graph (og:published_time)</li> <li>Dublin Core (DC.date)</li> <li> <p>Article metadata</p> </li> <li> <p>CSS Selectors</p> </li> <li>Common date classes (.date, .publish-date)</li> <li>Time elements</li> <li> <p>Site-specific patterns</p> </li> <li> <p>Text Patterns (Fallback)</p> </li> <li>Regex for various date formats</li> <li>Natural language parsing</li> <li>Sanity validation</li> </ol>"},{"location":"crawler-utils/#expected-inputsoutputs_3","title":"Expected Inputs/Outputs","text":""},{"location":"crawler-utils/#inputs_3","title":"Inputs","text":"<pre><code>{\n    'html_content': '&lt;html&gt;...&lt;/html&gt;',\n    'url': 'https://example.com/article'\n}\n</code></pre>"},{"location":"crawler-utils/#outputs_3","title":"Outputs","text":"<ul> <li>ISO format date string: \"2024-01-15\"</li> <li>None if no valid date found</li> </ul>"},{"location":"crawler-utils/#module-srcutilsescalationpy","title":"Module: <code>src/utils/escalation.py</code>","text":""},{"location":"crawler-utils/#purpose_4","title":"Purpose","text":"<p>The Escalation module manages domain-specific crawling strategies, determining the appropriate starting tier and escalation path based on site characteristics and configuration.</p>"},{"location":"crawler-utils/#key-functionsclasses_4","title":"Key Functions/Classes","text":""},{"location":"crawler-utils/#core-functions_1","title":"Core Functions","text":"<pre><code>def load_media_sources() -&gt; pd.DataFrame:\n    \"\"\"\n    Load media source configuration from CSV.\n    Contains domain-specific crawl strategies.\n    \"\"\"\n\ndef get_domain_crawl_level(domain: str, media_sources_df: pd.DataFrame) -&gt; int:\n    \"\"\"\n    Determine starting crawl level for domain.\n    Based on js_mode and other indicators.\n    \"\"\"\n\ndef requires_js_rendering(domain: str) -&gt; bool:\n    \"\"\"\n    Check if domain requires JavaScript rendering.\n    Uses configuration and heuristics.\n    \"\"\"\n</code></pre>"},{"location":"crawler-utils/#configuration-management","title":"Configuration Management","text":"<ol> <li> <p>Media Sources CSV <code>csv    domain,js_mode,start_tier    example.com,true,4    simple.com,false,1</code></p> </li> <li> <p>Domain Classification</p> </li> <li>JavaScript-heavy sites</li> <li>Simple HTML sites</li> <li>API-required sites</li> <li> <p>Premium proxy sites</p> </li> <li> <p>Escalation Rules</p> </li> <li>Start tier selection</li> <li>Max tier limits</li> <li>Skip tier options</li> </ol>"},{"location":"crawler-utils/#expected-inputsoutputs_4","title":"Expected Inputs/Outputs","text":""},{"location":"crawler-utils/#inputs_4","title":"Inputs","text":"<ul> <li>Domain name (string)</li> <li>Media sources configuration</li> </ul>"},{"location":"crawler-utils/#outputs_4","title":"Outputs","text":"<ul> <li>Starting crawl level (integer)</li> <li>JavaScript requirement (boolean)</li> <li>Escalation strategy parameters</li> </ul>"},{"location":"crawler-utils/#integration-with-crawler-manager","title":"Integration with Crawler Manager","text":"<p>The escalation module provides the intelligence for: - Selecting optimal starting tier - Avoiding unnecessary escalation - Domain-specific optimizations - Performance tuning per site</p>"},{"location":"crawler/","title":"Crawler Module Documentation","text":""},{"location":"crawler/#module-srccrawlercrawlerspidersloan_spiderpy","title":"Module: <code>src/crawler/crawler/spiders/loan_spider.py</code>","text":""},{"location":"crawler/#purpose","title":"Purpose","text":"<p>The Crawler module implements a Scrapy-based spider for discovering and extracting automotive media content from web pages. It provides intelligent content discovery with multi-level crawling capabilities, automatically following relevant links when initial pages don't contain vehicle-specific content. The spider is designed as the foundation layer for the tiered web scraping strategy, focusing on standard HTML extraction before escalation to more advanced techniques.</p>"},{"location":"crawler/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"crawler/#loanspider-class","title":"LoanSpider Class","text":"<pre><code>class LoanSpider(scrapy.Spider):\n    \"\"\"\n    Scrapy spider for crawling automotive media sites.\n    Implements smart discovery and multi-level crawling.\n    \"\"\"\n\n    name = 'loan_spider'\n    allowed_domains = []  # Set dynamically from loan URLs\n    start_urls = []      # Set dynamically from loan data\n</code></pre>"},{"location":"crawler/#initialization","title":"Initialization","text":"<pre><code>def __init__(self, loans_data: List[Dict[str, Any]] = None, *args, **kwargs):\n    \"\"\"\n    Initialize spider with loan data.\n    Dynamically generates allowed domains from URLs.\n    \"\"\"\n</code></pre>"},{"location":"crawler/#request-generation","title":"Request Generation","text":"<pre><code>def start_requests(self) -&gt; Generator[Request, None, None]:\n    \"\"\"\n    Generate initial requests from loans data.\n    Attaches loan metadata to each request.\n    \"\"\"\n\ndef _discover_relevant_pages(self, response: Response, \n                           loan_data: Dict[str, Any]) -&gt; Generator[Request, None, None]:\n    \"\"\"\n    Discover and follow links that might contain relevant content.\n    Implements smart discovery for review/blog/news sections.\n    \"\"\"\n</code></pre>"},{"location":"crawler/#content-processing","title":"Content Processing","text":"<pre><code>def parse(self, response: Response) -&gt; Generator[LoanItem, None, None]:\n    \"\"\"\n    Main parsing function that extracts content.\n    Implements two-level crawling strategy.\n    \"\"\"\n\ndef handle_error(self, failure):\n    \"\"\"\n    Handle request failures gracefully.\n    Creates error items for tracking.\n    \"\"\"\n</code></pre>"},{"location":"crawler/#extraction-functions","title":"Extraction Functions","text":"<pre><code>def _extract_title(self, response: Response) -&gt; str:\n    \"\"\"\n    Extract page title using multiple strategies.\n    Falls back from &lt;title&gt; to &lt;h1&gt; tags.\n    \"\"\"\n\ndef _extract_content(self, response: Response, make: str, model: str) -&gt; str:\n    \"\"\"\n    Extract main content using common article selectors.\n    Progressive fallback from specific to generic selectors.\n    \"\"\"\n\ndef _extract_date(self, response: Response) -&gt; Optional[str]:\n    \"\"\"\n    Extract publication date from meta tags or HTML elements.\n    Checks multiple date formats and locations.\n    \"\"\"\n</code></pre>"},{"location":"crawler/#utility-functions","title":"Utility Functions","text":"<pre><code>def _content_mentions_vehicle(self, content: str, make: str, model: str) -&gt; bool:\n    \"\"\"\n    Check if content mentions the vehicle make and model.\n    Case-insensitive matching.\n    \"\"\"\n\ndef _extract_domain(self, url: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract domain from URL for allowed_domains.\n    \"\"\"\n\ndef _is_media_file(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL points to a media file.\n    Filters out images, videos, PDFs.\n    \"\"\"\n</code></pre>"},{"location":"crawler/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"crawler/#inputs","title":"Inputs","text":"<ol> <li> <p>Loan Data Structure:    <code>python    {        'work_order': 'WO12345',        'make': 'Honda',        'model': 'Accord',        'source': 'Car and Driver',        'urls': [            'https://caranddriver.com/reviews/...',            'https://motortrend.com/...'        ]    }</code></p> </li> <li> <p>Spider Configuration:</p> </li> <li>Crawl levels: 1 (direct URL), 2 (discovered links)</li> <li>Max discovered links: 5 per page</li> <li>Follow patterns: review, test-drive, road-test, etc.</li> </ol>"},{"location":"crawler/#outputs","title":"Outputs","text":"<ol> <li>LoanItem Structure:    <code>python    LoanItem(        work_order='WO12345',        make='Honda',        model='Accord',        source='Car and Driver',        url='https://...',        content='Extracted article text...',        title='2024 Honda Accord Review',        publication_date='2024-01-15',        content_type='article',  # or 'error'        crawl_date='2024-01-20T10:30:00',        crawl_level=1,  # 1 or 2        error=None  # Error message if failed    )</code></li> </ol>"},{"location":"crawler/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport scrapy\nfrom scrapy.http import Response, Request\n\n# Standard Library\nimport re\nimport logging\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Generator, Optional\nfrom urllib.parse import urlparse\n\n# Internal Modules\nfrom crawler.items import LoanItem\n</code></pre>"},{"location":"crawler/#content-discovery-strategy","title":"Content Discovery Strategy","text":""},{"location":"crawler/#level-1-crawling-direct-urls","title":"Level 1 Crawling (Direct URLs)","text":"<ol> <li>Fetch the provided URL</li> <li>Extract content using article selectors</li> <li>Check if content mentions vehicle</li> <li>If not relevant, trigger discovery</li> </ol>"},{"location":"crawler/#level-2-crawling-discovery","title":"Level 2 Crawling (Discovery)","text":"<ol> <li>Link Pattern Matching:</li> <li>Vehicle make/model in URL</li> <li>Review keywords: \"review\", \"test-drive\", \"road-test\"</li> <li> <p>Section links: \"/review/\", \"/blog/\", \"/news/\"</p> </li> <li> <p>Discovery Limits:</p> </li> <li>Maximum 5 links per page</li> <li>Excludes media files</li> <li>Deduplicates discovered URLs</li> </ol>"},{"location":"crawler/#content-extraction-hierarchy","title":"Content Extraction Hierarchy","text":"<ol> <li>Article-Specific Selectors:</li> <li><code>article</code></li> <li><code>div.content</code></li> <li><code>div.article-content</code></li> <li><code>div.post-content</code></li> <li><code>div.entry-content</code></li> <li><code>div.main-content</code></li> <li> <p><code>.story</code></p> </li> <li> <p>Fallback Strategies:</p> </li> <li>All <code>&lt;p&gt;</code> tags</li> <li>All text from <code>&lt;body&gt;</code></li> <li>Progressive degradation ensures some content</li> </ol>"},{"location":"crawler/#error-handling","title":"Error Handling","text":"<ol> <li>Request Failures:</li> <li>Logged with full error details</li> <li>Error items created for tracking</li> <li> <p>Preserves loan metadata in error items</p> </li> <li> <p>Extraction Failures:</p> </li> <li>Empty strings returned (not None)</li> <li>Graceful degradation in selectors</li> <li> <p>No exceptions thrown to caller</p> </li> <li> <p>Discovery Failures:</p> </li> <li>Silently skips bad links</li> <li>Continues with remaining URLs</li> <li>Logs domain extraction errors</li> </ol>"},{"location":"crawler/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Concurrent Requests: Controlled by Scrapy settings</li> <li>Domain Filtering: Dynamic allowed_domains prevents sprawl</li> <li>Content Limits: No explicit size limits (handled upstream)</li> <li>Discovery Depth: Limited to 2 levels</li> <li>Link Limits: Max 5 discovered links per page</li> </ul>"},{"location":"crawler/#integration-notes","title":"Integration Notes","text":"<p>This spider is typically not used directly but through: 1. EnhancedCrawlerManager: Orchestrates tiered escalation 2. Scrapy Settings: Configured for 2s delay, 1 concurrent request 3. Item Pipeline: Results processed by Scrapy pipelines</p>"},{"location":"crawler/#limitations","title":"Limitations","text":"<ul> <li>JavaScript Sites: No JS rendering (Level 1 only)</li> <li>Authentication: No login support</li> <li>Cookies: Basic cookie jar only</li> <li>Rate Limiting: Relies on Scrapy delays</li> <li>Content Types: HTML only, no PDF extraction</li> </ul>"},{"location":"creatoriq/","title":"CreatorIQ Module Documentation","text":""},{"location":"creatoriq/#module-srccreatoriq","title":"Module: <code>src/creatoriq/</code>","text":""},{"location":"creatoriq/#purpose","title":"Purpose","text":"<p>The CreatorIQ module provides comprehensive integration with the CreatorIQ influencer marketing platform to extract social media post data from campaigns. It implements multiple authentication methods, data extraction strategies, and export capabilities to reliably capture influencer content URLs and engagement metrics. The module is designed with fallback strategies to ensure data extraction even when API access is limited.</p>"},{"location":"creatoriq/#key-components","title":"Key Components","text":""},{"location":"creatoriq/#graphql-client-graphql_clientpy","title":"GraphQL Client (<code>graphql_client.py</code>)","text":"<pre><code>class CreatorIQClient:\n    \"\"\"\n    Direct API client for CreatorIQ's GraphQL endpoint.\n    Handles authentication, pagination, and data extraction.\n    \"\"\"\n\n    def get_campaign_posts(self, campaign_id: int, limit: int = 1000) -&gt; List[Dict]:\n        \"\"\"\n        Fetches all posts for a campaign using GraphQL queries.\n        Implements cursor-based pagination.\n        \"\"\"\n</code></pre>"},{"location":"creatoriq/#csv-exporter-csv_exporterpy","title":"CSV Exporter (<code>csv_exporter.py</code>)","text":"<pre><code>class CSVExporter:\n    \"\"\"\n    Exports CreatorIQ post data to CSV format.\n    Generates detailed exports and summary statistics.\n    \"\"\"\n\n    def export_to_csv(self, posts: List[Dict], output_path: str, \n                     include_summary: bool = True):\n        \"\"\"\n        Exports posts with cleaned data and optional summary statistics.\n        \"\"\"\n</code></pre>"},{"location":"creatoriq/#authentication-handlers","title":"Authentication Handlers","text":"<ol> <li> <p>Browser Session Auth (<code>auth_headers.py</code>):    <code>python    def get_auth_headers():        \"\"\"        Extracts authentication headers from browser session.        Includes cookies, CSRF tokens, and auth tokens.        \"\"\"</code></p> </li> <li> <p>API Key Auth (<code>api_key_auth.py</code>):    <code>python    class APIKeyClient:        \"\"\"        Cleaner authentication using CreatorIQ API keys.        Preferred method when available.        \"\"\"</code></p> </li> <li> <p>Hybrid Auth (<code>hybrid_auth_client.py</code>):    <code>python    class HybridCreatorIQClient:        \"\"\"        Automatically selects best available authentication method.        Falls back gracefully between API key and browser auth.        \"\"\"</code></p> </li> <li> <p>Public Client (<code>public_client.py</code>):    <code>python    class PublicCreatorIQClient:        \"\"\"        Accesses public/shared reports without authentication.        Uses browser automation for public campaign access.        \"\"\"</code></p> </li> </ol>"},{"location":"creatoriq/#web-scraping-components","title":"Web Scraping Components","text":"<ol> <li> <p>Playwright Scraper (<code>playwright_scraper.py</code>):    <code>python    async def scrape_campaign_with_playwright(url: str, save_responses: bool = True):        \"\"\"        Browser automation with network traffic capture.        Implements infinite scrolling and response saving.        \"\"\"</code></p> </li> <li> <p>Browser Extractor (<code>browser_extractor.py</code>):    <code>python    def extract_posts_from_browser(campaign_url: str):        \"\"\"        Direct DOM extraction from rendered pages.        Scrolls and extracts post data from HTML elements.        \"\"\"</code></p> </li> <li> <p>Parser (<code>parser.py</code>):    <code>python    def extract_urls_from_html(html_content: str) -&gt; List[str]:        \"\"\"        Extracts social media URLs using multiple strategies.        Handles API responses, embedded JSON, and regex patterns.        \"\"\"</code></p> </li> </ol>"},{"location":"creatoriq/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"creatoriq/#inputs","title":"Inputs","text":"<ol> <li> <p>Campaign URL:    <code>https://app.creatoriq.com/campaigns/[CAMPAIGN_ID]/posts</code></p> </li> <li> <p>Authentication Options:</p> </li> <li>API Key: <code>CREATORIQ_API_KEY</code> environment variable</li> <li>Browser Headers: Interactive collection via terminal</li> <li> <p>Public Access: No authentication required</p> </li> <li> <p>Configuration:    <code>python    {        'limit': 1000,  # Max posts to fetch        'include_summary': True,  # Generate statistics        'save_responses': True,  # Save raw API responses        'scroll_delay': 2  # Seconds between scrolls    }</code></p> </li> </ol>"},{"location":"creatoriq/#outputs","title":"Outputs","text":"<ol> <li> <p>Post Data Structure:    <code>json    {        \"url\": \"https://www.instagram.com/p/ABC123/\",        \"creator_name\": \"John Doe\",        \"creator_username\": \"@johndoe\",        \"platform\": \"instagram\",        \"impressions\": 50000,        \"likes\": 2500,        \"comments\": 150,        \"shares\": 75,        \"engagement_rate\": 5.5,        \"caption\": \"Post caption text...\",        \"published_at\": \"2024-01-15T10:30:00Z\",        \"thumbnail_url\": \"https://...\"    }</code></p> </li> <li> <p>CSV Export Files:</p> </li> <li><code>campaign_posts.csv</code>: Detailed post data</li> <li> <p><code>campaign_summary.txt</code>: Platform breakdown and top creators</p> </li> <li> <p>Saved Responses (optional):</p> </li> <li><code>campaign_[ID]_responses.json</code>: Raw API responses</li> <li><code>campaign_[ID]_html.html</code>: Captured page HTML</li> </ol>"},{"location":"creatoriq/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport playwright  # Browser automation\nimport httpx       # HTTP client\nimport pandas      # Data processing\nfrom bs4 import BeautifulSoup  # HTML parsing\n\n# Internal Modules\nfrom src.utils.logger import get_logger\n</code></pre>"},{"location":"creatoriq/#data-extraction-strategies","title":"Data Extraction Strategies","text":""},{"location":"creatoriq/#1-graphql-api-primary","title":"1. GraphQL API (Primary)","text":"<ul> <li>Direct queries to CreatorIQ's GraphQL endpoint</li> <li>Most reliable and complete data</li> <li>Requires authentication</li> <li>Cursor-based pagination</li> </ul>"},{"location":"creatoriq/#2-network-interception","title":"2. Network Interception","text":"<ul> <li>Captures API responses during page load</li> <li>Useful when direct API access fails</li> <li>Extracts from XHR/Fetch responses</li> <li>Requires browser automation</li> </ul>"},{"location":"creatoriq/#3-embedded-json","title":"3. Embedded JSON","text":"<ul> <li>Parses <code>window.__INITIAL_STATE__</code> from page</li> <li>Contains pre-loaded campaign data</li> <li>Fast but may be incomplete</li> <li>No pagination support</li> </ul>"},{"location":"creatoriq/#4-dom-parsing","title":"4. DOM Parsing","text":"<ul> <li>Extracts from rendered HTML elements</li> <li>Last resort when APIs fail</li> <li>Handles dynamic content via scrolling</li> <li>May miss some data fields</li> </ul>"},{"location":"creatoriq/#authentication-flow","title":"Authentication Flow","text":"<pre><code>1. Check for API Key \u2192 Use APIKeyClient\n   \u2193 (if not available)\n2. Check for Browser Headers \u2192 Use BrowserAuthClient\n   \u2193 (if not available)\n3. Check if Public URL \u2192 Use PublicClient\n   \u2193 (if not available)\n4. Prompt for Authentication Method\n</code></pre>"},{"location":"creatoriq/#error-handling","title":"Error Handling","text":"<ol> <li>Authentication Failures:</li> <li>Clear error messages with solutions</li> <li>Automatic fallback to next method</li> <li> <p>Session refresh capabilities</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Configurable delays between requests</li> <li>Exponential backoff on 429 errors</li> <li> <p>Request queuing</p> </li> <li> <p>Data Extraction Failures:</p> </li> <li>Multiple extraction strategies</li> <li>Partial data recovery</li> <li> <p>Detailed error logging</p> </li> <li> <p>Network Issues:</p> </li> <li>Retry logic with backoff</li> <li>Timeout configuration</li> <li>Connection pooling</li> </ol>"},{"location":"creatoriq/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Pagination: Fetches 100 posts per request</li> <li>Scrolling: 2-second delay between scrolls</li> <li>Network Capture: ~10MB per 1000 posts</li> <li>Export Time: ~1 second per 1000 posts</li> <li>Memory Usage: Streaming for large datasets</li> </ul>"},{"location":"creatoriq/#usage-examples","title":"Usage Examples","text":""},{"location":"creatoriq/#command-line","title":"Command Line","text":"<pre><code># Extract campaign URLs\npython -m src.creatoriq.extract_campaign_urls [CAMPAIGN_URL]\n\n# Export to CSV with API key\npython -m src.creatoriq.scrape_campaign_report [CAMPAIGN_URL]\n\n# Interactive browser extraction\npython -m src.creatoriq.scrape_posts_from_browser\n</code></pre>"},{"location":"creatoriq/#python-integration","title":"Python Integration","text":"<pre><code>from src.creatoriq import HybridCreatorIQClient, CSVExporter\n\n# Initialize client\nclient = HybridCreatorIQClient()\n\n# Get campaign posts\nposts = client.get_campaign_posts(campaign_id=12345)\n\n# Export to CSV\nexporter = CSVExporter()\nexporter.export_to_csv(posts, \"output.csv\", include_summary=True)\n</code></pre>"},{"location":"creatoriq/#limitations","title":"Limitations","text":"<ul> <li>Platform Support: Instagram, TikTok, YouTube primarily</li> <li>Historical Data: Limited by CreatorIQ retention</li> <li>Real-time Updates: Requires re-scraping</li> <li>API Rate Limits: Varies by subscription tier</li> <li>Browser Detection: May trigger anti-bot measures</li> </ul>"},{"location":"dashboard/","title":"Dashboard Module Documentation","text":""},{"location":"dashboard/#module-srcdashboardapppy","title":"Module: <code>src/dashboard/app.py</code>","text":""},{"location":"dashboard/#purpose","title":"Purpose","text":"<p>The Dashboard module provides a comprehensive Streamlit-based web interface for the DriveShop Clip Tracking System. It serves as the central hub for reviewing, approving, and managing media clips discovered through automated crawling and analysis. The dashboard enables users to process loans without clips, review AI-analyzed content, manage journalist-outlet relationships, and export formatted reports for stakeholders.</p>"},{"location":"dashboard/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"dashboard/#main-application-function","title":"Main Application Function","text":"<pre><code>def main():\n    \"\"\"\n    Entry point for the Streamlit dashboard application.\n    Handles authentication, UI layout, and orchestrates all dashboard functionality.\n    \"\"\"\n</code></pre>"},{"location":"dashboard/#data-loading-functions","title":"Data Loading Functions","text":"<pre><code>def load_person_outlets_mapping():\n    \"\"\"\n    Loads Person_ID to Media Outlets mapping from JSON file.\n    Returns: Dict mapping Person_ID to list of outlet data with impressions\n    \"\"\"\n\ndef load_loans_data_for_filtering(url: str):\n    \"\"\"\n    Fetches and caches loans data from URL for filtering.\n    Returns: DataFrame with loan records or None if error\n    \"\"\"\n</code></pre>"},{"location":"dashboard/#ui-helper-functions","title":"UI Helper Functions","text":"<pre><code>def apply_custom_sidebar_styling():\n    \"\"\"\n    Applies comprehensive black sidebar CSS styling to the Streamlit app.\n    Includes hover effects, custom colors, and professional appearance.\n    \"\"\"\n\ndef update_progress(current: int, total: int, message: str):\n    \"\"\"\n    Updates processing progress in session state.\n    Used for real-time progress tracking during batch operations.\n    \"\"\"\n\ndef format_time(seconds: float) -&gt; str:\n    \"\"\"\n    Formats seconds into human-readable time string.\n    Example: 125 seconds -&gt; \"2 minutes 5 seconds\"\n    \"\"\"\n</code></pre>"},{"location":"dashboard/#data-processing-functions","title":"Data Processing Functions","text":"<pre><code>def create_reporter_name_to_id_mapping(person_outlets_data: dict) -&gt; dict:\n    \"\"\"\n    Creates mapping of reporter full names to Person_IDs.\n    Handles name variations and duplicates.\n    \"\"\"\n\ndef get_outlet_options_for_person(person_id: str, person_outlets_data: dict) -&gt; list:\n    \"\"\"\n    Returns list of outlet names associated with a Person_ID.\n    Used for dropdown population in the UI.\n    \"\"\"\n\ndef parse_url_tracking(wo_tracking: str) -&gt; dict:\n    \"\"\"\n    Parses URL tracking data from database JSON string.\n    Extracts discovered URLs and their processing status.\n    \"\"\"\n</code></pre>"},{"location":"dashboard/#export-functions","title":"Export Functions","text":"<pre><code>def create_client_excel_report(clips_df: pd.DataFrame, \n                             rejected_df: pd.DataFrame,\n                             selected_office: str,\n                             selected_makes: list,\n                             date_range: tuple) -&gt; BytesIO:\n    \"\"\"\n    Creates professional multi-sheet Excel report.\n    Includes Executive Summary, Detailed Results, Rejected Loans, and Approved Clips.\n    Returns: BytesIO object containing the Excel file\n    \"\"\"\n</code></pre>"},{"location":"dashboard/#database-interaction-functions","title":"Database Interaction Functions","text":"<pre><code>def update_person_outlets_mapping_from_url(url: str):\n    \"\"\"\n    Updates local person-outlets mapping from remote URL.\n    Refreshes both JSON and CSV versions for consistency.\n    \"\"\"\n</code></pre>"},{"location":"dashboard/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"dashboard/#inputs","title":"Inputs","text":"<ol> <li>Loans Data Source:</li> <li>Live URL: <code>https://driveshop.mystagingwebsite.com/reports/media-and-pr/loans-without-clips/download/csv</code></li> <li> <p>File Upload: CSV/XLSX files with columns:</p> <ul> <li><code>Work Order Number</code>, <code>First Name</code>, <code>Last Name</code>, <code>Media Outlet</code></li> <li><code>Model</code>, <code>Start Date</code>, <code>End Date</code>, <code>Office</code>, <code>Make</code></li> </ul> </li> <li> <p>User Interactions:</p> </li> <li>Filter selections (Office, Make, Reporter, Outlet, Date Range)</li> <li>Approval/Rejection checkboxes in AgGrid</li> <li>Media Outlet dropdown selections</li> <li>Byline Author text inputs</li> <li> <p>Sentiment analysis triggers</p> </li> <li> <p>Configuration Files:</p> </li> <li><code>data/person_outlets_mapping.json</code>: Journalist-outlet associations</li> <li>Environment variables via <code>.env</code> file</li> </ol>"},{"location":"dashboard/#outputs","title":"Outputs","text":"<ol> <li>Database Updates:</li> <li>Clip status updates (approved/rejected)</li> <li>UI state persistence</li> <li>Media outlet associations</li> <li> <p>Byline modifications</p> </li> <li> <p>Export Files:</p> </li> <li>Excel Reports (.xlsx):<ul> <li>Executive Summary sheet</li> <li>Detailed Results sheet</li> <li>Rejected Loans sheet</li> <li>Approved Clips sheet (FMS format)</li> </ul> </li> <li> <p>JSON Files: Timestamped approved clips data</p> </li> <li> <p>Visual Feedback:</p> </li> <li>Processing progress bars</li> <li>Success/error notifications</li> <li>Metrics display (total clips, approved, rejected)</li> <li>Real-time grid updates</li> </ol>"},{"location":"dashboard/#dependencies","title":"Dependencies","text":""},{"location":"dashboard/#external-libraries","title":"External Libraries","text":"<pre><code># UI Framework\nimport streamlit as st\nfrom st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode\n\n# Data Processing\nimport pandas as pd\nimport numpy as np\n\n# File Handling\nimport openpyxl\nfrom openpyxl.styles import Font, PatternFill, Alignment\nfrom openpyxl.utils import get_column_letter\n\n# System &amp; Utilities\nimport os\nimport sys\nimport json\nimport io\nimport time\nimport pickle\nimport requests\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Image Processing\nfrom PIL import Image\n</code></pre>"},{"location":"dashboard/#internal-modules","title":"Internal Modules","text":"<pre><code># Database Operations\nfrom src.utils.database import DatabaseManager\n\n# Logging\nfrom src.utils.logger import get_logger\n\n# Processing Pipeline\nfrom src.ingest.ingest_database import (\n    process_loans_batch,\n    check_all_youtube_outlets_in_mapping\n)\n\n# Analysis\nfrom src.utils.sentiment_analysis import run_sentiment_analysis\n\n# CreatorIQ Integration\nfrom src.creatoriq.scrape_campaign_report import scrape_campaign_report\nfrom src.creatoriq.scrape_post_urls import scrape_posts_for_campaign\n</code></pre>"},{"location":"dashboard/#configuration","title":"Configuration","text":""},{"location":"dashboard/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>SUPABASE_URL</code>: Database connection URL</li> <li><code>SUPABASE_KEY</code>: Database authentication key</li> <li><code>STREAMLIT_PASSWORD</code>: Dashboard access password</li> <li><code>DATABASE_PASSWORD</code>: PostgreSQL password</li> </ul>"},{"location":"dashboard/#session-state-management","title":"Session State Management","text":"<pre><code># Tracking Sets\nst.session_state.viewed_records = set()      # Viewed WO numbers\nst.session_state.approved_records = set()    # Approved WO numbers\nst.session_state.rejected_records = set()    # Rejected WO numbers\n\n# Data Storage\nst.session_state.last_saved_outlets = {}     # Media outlet selections\nst.session_state.last_saved_bylines = {}     # Byline author edits\nst.session_state.outlet_data_mapping = {}    # Full outlet data\n\n# Processing State\nst.session_state.processing_progress = {...}  # Progress tracking\nst.session_state.loans_data_loaded = False   # Data load flag\nst.session_state.batch_info = {...}          # Batch processing info\n</code></pre>"},{"location":"dashboard/#error-handling","title":"Error Handling","text":"<ul> <li>Database connection failures handled with user-friendly messages</li> <li>File upload validation with size and format checks</li> <li>API rate limiting with exponential backoff</li> <li>Graceful degradation when external services unavailable</li> <li>Comprehensive logging for debugging</li> </ul>"},{"location":"dashboard/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Caching implemented for expensive operations (<code>@st.cache_data</code>)</li> <li>Batch processing for large datasets</li> <li>Pagination support in AgGrid for large result sets</li> <li>Optimized database queries with proper indexing</li> <li>Lazy loading for media outlet data</li> </ul>"},{"location":"dashboard/#security","title":"Security","text":"<ul> <li>Password protection for dashboard access</li> <li>Environment-based configuration (no hardcoded secrets)</li> <li>SQL injection prevention through parameterized queries</li> <li>XSS protection in user input handling</li> <li>Secure file upload with validation</li> </ul>"},{"location":"ingest/","title":"Ingest Module Documentation","text":""},{"location":"ingest/#module-srcingestingestpy","title":"Module: <code>src/ingest/ingest.py</code>","text":""},{"location":"ingest/#purpose","title":"Purpose","text":"<p>The CSV-based ingest module processes loan data from CSV/Excel files to find and analyze media clips (articles and videos) about loaned vehicles. It implements concurrent processing for efficiency and outputs results to CSV files for dashboard consumption. This module maintains a guaranteed contract for the core <code>process_loan()</code> function to ensure consistent processing.</p>"},{"location":"ingest/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"ingest/#data-loading-functions","title":"Data Loading Functions","text":"<pre><code>def load_loans_data(file_path: str) -&gt; List[Dict]:\n    \"\"\"\n    Loads loan data from CSV or Excel file.\n    Returns: List of dictionaries containing loan information\n    \"\"\"\n\ndef load_loans_data_from_url(url: str) -&gt; List[Dict]:\n    \"\"\"\n    Loads loan data from a URL endpoint.\n    Handles authentication and CSV parsing.\n    Returns: List of loan dictionaries\n    \"\"\"\n</code></pre>"},{"location":"ingest/#core-processing-functions","title":"Core Processing Functions","text":"<pre><code>def process_loan(loan: dict) -&gt; Optional[dict]:\n    \"\"\"\n    PROTECTED CONTRACT: Processes a single loan to find media clips.\n    Must return dictionary with specific structure or None.\n\n    Contract Requirements:\n    - Must handle both YouTube and web URLs\n    - Must validate content dates\n    - Must return only the best clip per loan\n    - Must include all required fields in output\n    \"\"\"\n\nasync def process_loan_async(loan: dict, semaphore: asyncio.Semaphore) -&gt; Optional[dict]:\n    \"\"\"\n    Async wrapper for process_loan with semaphore control.\n    Enables concurrent processing with rate limiting.\n    \"\"\"\n</code></pre>"},{"location":"ingest/#content-extraction-functions","title":"Content Extraction Functions","text":"<pre><code>def process_youtube_url(url: str, model: str, journalist_name: str, \n                       start_date: datetime, end_date: datetime) -&gt; Optional[dict]:\n    \"\"\"\n    Extracts content from YouTube videos.\n    Attempts transcript extraction, falls back to metadata.\n    Applies flexible model matching for video titles.\n    \"\"\"\n\ndef process_web_url(url: str, model: str, journalist_name: str,\n                   start_date: datetime, end_date: datetime) -&gt; Optional[dict]:\n    \"\"\"\n    Crawls web articles and extracts content.\n    Uses multi-tier escalation strategy for difficult sites.\n    \"\"\"\n</code></pre>"},{"location":"ingest/#analysis-functions","title":"Analysis Functions","text":"<pre><code>def analyze_clip(content: str, model: str, journalist_name: str, \n                source_url: str) -&gt; dict:\n    \"\"\"\n    Analyzes clip content using GPT-4.\n    Returns relevance score, sentiment, and AI insights.\n    \"\"\"\n\ndef flexible_model_match(title: str, model: str) -&gt; bool:\n    \"\"\"\n    Intelligent matching for vehicle models in content.\n    Handles variations like \"X5\" matching \"BMW X5\", \"2024 X5\", etc.\n    \"\"\"\n</code></pre>"},{"location":"ingest/#main-entry-points","title":"Main Entry Points","text":"<pre><code>def run_ingest_concurrent(loans_data: List[dict], max_workers: int = 10) -&gt; tuple:\n    \"\"\"\n    Main entry point for concurrent processing.\n    Returns: (successful_results, rejected_loans)\n    \"\"\"\n\ndef run_ingest_test(file_path: str):\n    \"\"\"\n    Test entry point for development.\n    Processes first 5 loans from a file.\n    \"\"\"\n</code></pre>"},{"location":"ingest/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"ingest/#inputs","title":"Inputs","text":"<ol> <li> <p>Loan Data Structure:    <code>python    {        'Work Order Number': 'WO12345',        'First Name': 'John',        'Last Name': 'Doe',        'Media Outlet': 'Car Magazine',        'Model': 'BMW X5',        'Start Date': '2024-01-15',        'End Date': '2024-01-22',        'URL 1': 'https://youtube.com/watch?v=...',        'URL 2': 'https://carmagazine.com/review/...'    }</code></p> </li> <li> <p>Configuration:</p> </li> <li>Date range fallback: 90 days \u2192 180 days</li> <li>Concurrent workers: 10 (configurable)</li> <li>Content length limits for GPT analysis</li> </ol>"},{"location":"ingest/#outputs","title":"Outputs","text":"<ol> <li> <p>loan_results.csv:    <code>csv    Work Order Number,Model,Clip Link,AI Relevance Score,AI Insights,Sentiment Score,...    WO12345,BMW X5,https://...,85,\"Comprehensive review focusing on...\",0.8,...</code></p> </li> <li> <p>rejected_clips.csv:    <code>csv    Work Order Number,First Name,Last Name,Rejection Reason    WO12346,Jane,Smith,\"No valid clips found within date range\"</code></p> </li> </ol>"},{"location":"ingest/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport pandas as pd\nimport asyncio\nimport aiohttp\nfrom datetime import datetime, timedelta\nfrom dateutil import parser as date_parser\nfrom typing import List, Dict, Optional\n\n# Internal Modules\nfrom src.utils.logger import get_logger\nfrom src.utils.youtube_handler import extract_youtube_content\nfrom src.utils.crawler_manager import EnhancedCrawlerManager\nfrom src.utils.date_extractor import extract_published_date\nfrom src.analysis.gpt_analysis import analyze_content_gpt\n</code></pre>"},{"location":"ingest/#processing-pipeline","title":"Processing Pipeline","text":"<ol> <li>Load Data \u2192 Parse CSV/Excel into loan dictionaries</li> <li>Concurrent Processing \u2192 Process multiple loans simultaneously</li> <li>For Each Loan:</li> <li>Extract URLs from loan data</li> <li>Process each URL based on type (YouTube/Web)</li> <li>Validate content publication date</li> <li>Analyze content with GPT</li> <li>Select best clip (highest relevance)</li> <li>Output Results \u2192 Save to CSV files</li> </ol>"},{"location":"ingest/#error-handling","title":"Error Handling","text":"<ul> <li>Date Range Fallback: Extends from 90 to 180 days if no content found</li> <li>YouTube Fallback: Transcript \u2192 Metadata only</li> <li>Web Crawling Escalation: Basic \u2192 Enhanced \u2192 Headless browser</li> <li>Graceful Failures: Logs errors and continues processing</li> <li>Rejection Tracking: Detailed reasons for failed loans</li> </ul>"},{"location":"ingest/#module-srcingestingest_databasepy","title":"Module: <code>src/ingest/ingest_database.py</code>","text":""},{"location":"ingest/#purpose_1","title":"Purpose","text":"<p>The database-integrated ingest module provides similar functionality to the CSV version but stores results directly in Supabase. It implements smart retry logic to avoid reprocessing recent attempts and defers full GPT analysis to save costs. This module is designed for production use with real-time dashboard integration.</p>"},{"location":"ingest/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"ingest/#database-processing-functions","title":"Database Processing Functions","text":"<pre><code>def process_loan_for_database(loan: dict, db: DatabaseManager, \n                            outlets_mapping: dict, run_id: str) -&gt; tuple:\n    \"\"\"\n    Processes a loan and stores results in database.\n    Returns: (success_count, failure_count)\n    \"\"\"\n\nasync def process_loan_database_async(loan: dict, db: DatabaseManager,\n                                    outlets_mapping: dict, run_id: str,\n                                    semaphore: asyncio.Semaphore,\n                                    progress_callback=None) -&gt; tuple:\n    \"\"\"\n    Async version with progress callback support.\n    Enables real-time UI updates during processing.\n    \"\"\"\n</code></pre>"},{"location":"ingest/#validation-functions","title":"Validation Functions","text":"<pre><code>def is_url_from_authorized_outlet(url: str, journalist_name: str, \n                                 outlets_mapping: dict) -&gt; bool:\n    \"\"\"\n    Validates if URL belongs to journalist's authorized outlets.\n    Prevents processing unauthorized media sources.\n    \"\"\"\n\ndef load_person_outlets_mapping() -&gt; dict:\n    \"\"\"\n    Loads person-to-outlet authorization mapping.\n    Returns: Dict mapping person names to authorized outlets\n    \"\"\"\n</code></pre>"},{"location":"ingest/#scoring-functions","title":"Scoring Functions","text":"<pre><code>def calculate_relevance_score(content: str, model: str, \n                            use_gpt: bool = False) -&gt; float:\n    \"\"\"\n    Calculates content relevance score.\n    Can use GPT or fallback to keyword matching.\n    \"\"\"\n</code></pre>"},{"location":"ingest/#main-entry-points_1","title":"Main Entry Points","text":"<pre><code>def run_ingest_database(data_source: str):\n    \"\"\"\n    Main entry point for database ingestion.\n    Processes loans from file or URL.\n    \"\"\"\n\ndef run_ingest_database_with_filters(loans_data: List[dict], \n                                   progress_callback=None) -&gt; dict:\n    \"\"\"\n    Processes pre-filtered loans from dashboard.\n    Supports real-time progress updates.\n    Returns: Processing statistics\n    \"\"\"\n</code></pre>"},{"location":"ingest/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"ingest/#inputs_1","title":"Inputs","text":"<ul> <li>Same loan data structure as CSV version</li> <li>Additional Requirements:</li> <li>Person-to-outlet mapping JSON</li> <li>Database connection credentials</li> <li>Processing run ID for tracking</li> </ul>"},{"location":"ingest/#outputs_1","title":"Outputs","text":"<ol> <li> <p>Database Records (clips table):    <code>sql    INSERT INTO clips (        wo_number, first_name, last_name, media_outlet,        model, start_date, end_date, url, content,        published_date, relevance_score, sentiment,        ai_insights, status, run_id, created_at    )</code></p> </li> <li> <p>Failed Attempts (clips table with status='failed'):</p> </li> <li>Stored with rejection reasons</li> <li> <p>Used for smart retry logic</p> </li> <li> <p>Processing Run Statistics:    <code>python    {        'processed': 50,        'successful': 45,        'failed': 5,        'duration': 120.5    }</code></p> </li> </ol>"},{"location":"ingest/#dependencies_1","title":"Dependencies","text":"<pre><code># Database Integration\nfrom src.utils.database import DatabaseManager\n\n# Reused from CSV Version\nfrom src.ingest.ingest import (\n    process_youtube_url,\n    process_web_url,\n    flexible_model_match,\n    is_content_within_date_range\n)\n\n# Additional Utilities\nfrom src.utils.content_extractor import ContentExtractor\nfrom src.utils.sentiment_analysis import calculate_relevance_score_gpt\n</code></pre>"},{"location":"ingest/#smart-retry-logic","title":"Smart Retry Logic","text":"<ol> <li>Check Recent Attempts: Skip if processed in last 24 hours</li> <li>Track Failed URLs: Store failure reasons in database</li> <li>Homepage Detection: Filter out index/homepage URLs</li> <li>Outlet Authorization: Only process authorized outlets</li> </ol>"},{"location":"ingest/#database-schema-integration","title":"Database Schema Integration","text":"<pre><code>-- Processing Runs Table\nCREATE TABLE processing_runs (\n    id UUID PRIMARY KEY,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    total_loans INTEGER,\n    successful_clips INTEGER,\n    failed_clips INTEGER\n);\n\n-- Clips Table\nCREATE TABLE clips (\n    id SERIAL PRIMARY KEY,\n    wo_number VARCHAR,\n    url VARCHAR,\n    content TEXT,\n    relevance_score FLOAT,\n    status VARCHAR, -- 'pending', 'approved', 'rejected', 'failed'\n    run_id UUID REFERENCES processing_runs(id),\n    rejection_reason TEXT,\n    created_at TIMESTAMP\n);\n</code></pre>"},{"location":"ingest/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Concurrent Processing: Default 5 workers (configurable)</li> <li>Smart Retry: Avoids redundant API calls</li> <li>Deferred Analysis: Only relevance scoring, full GPT on demand</li> <li>Batch Operations: Efficient database inserts</li> <li>Progress Callbacks: Real-time UI updates without polling</li> </ul>"},{"location":"ingest/#error-handling_1","title":"Error Handling","text":"<ul> <li>Database Failures: Automatic reconnection with exponential backoff</li> <li>Duplicate Detection: Checks existing clips before processing</li> <li>Transaction Safety: Rollback on errors</li> <li>Comprehensive Logging: All failures tracked in database</li> <li>Graceful Degradation: Continues processing on individual failures</li> </ul>"},{"location":"module_overview/","title":"DriveShop Clip Tracking - Module Documentation Index","text":"<p>This directory contains comprehensive documentation for all modules in the DriveShop Clip Tracking system. Each module is documented with its purpose, key functions, expected inputs/outputs, and dependencies.</p>"},{"location":"module_overview/#core-application-modules","title":"Core Application Modules","text":""},{"location":"module_overview/#1-dashboard-module","title":"1. Dashboard Module","text":"<p>Location: <code>src/dashboard/app.py</code> Purpose: Streamlit-based web interface for reviewing and managing media clips Key Features: File upload, AgGrid tables, approval workflow, Excel export</p>"},{"location":"module_overview/#2-ingest-modules","title":"2. Ingest Modules","text":"<p>Location: <code>src/ingest/</code> Purpose: Data processing pipeline for finding and analyzing media clips Components: - <code>ingest.py</code> - CSV-based processing - <code>ingest_database.py</code> - Database-integrated processing with smart retry</p>"},{"location":"module_overview/#3-analysis-module","title":"3. Analysis Module","text":"<p>Location: <code>src/analysis/gpt_analysis.py</code> Purpose: AI-powered content analysis using GPT-4 Key Features: Marketing insights, sentiment analysis, cost optimization</p>"},{"location":"module_overview/#4-crawler-module","title":"4. Crawler Module","text":"<p>Location: <code>src/crawler/crawler/spiders/loan_spider.py</code> Purpose: Scrapy spider for web content discovery Key Features: Multi-level crawling, smart discovery, content extraction</p>"},{"location":"module_overview/#5-creatoriq-module","title":"5. CreatorIQ Module","text":"<p>Location: <code>src/creatoriq/</code> Purpose: Integration with CreatorIQ influencer platform Key Features: Multiple auth methods, GraphQL client, CSV export</p>"},{"location":"module_overview/#utility-modules","title":"Utility Modules","text":""},{"location":"module_overview/#core-utilities","title":"Core Utilities","text":""},{"location":"module_overview/#core-utils","title":"Core Utils","text":"<ul> <li>Database (<code>database.py</code>) - Supabase integration with retry logic</li> <li>Config (<code>config.py</code>) - Centralized configuration constants</li> <li>Logger (<code>logger.py</code>) - Standardized logging setup</li> </ul>"},{"location":"module_overview/#web-crawling-utilities","title":"Web Crawling Utilities","text":""},{"location":"module_overview/#crawler-utils","title":"Crawler Utils","text":"<ul> <li>Enhanced Crawler Manager (<code>enhanced_crawler_manager.py</code>) - 6-tier escalation system</li> <li>Content Extractor (<code>content_extractor.py</code>) - Intelligent HTML extraction</li> <li>Browser Crawler (<code>browser_crawler.py</code>) - Playwright automation</li> <li>Date Extractor (<code>date_extractor.py</code>) - Publication date detection</li> <li>Escalation (<code>escalation.py</code>) - Domain-specific strategies</li> </ul>"},{"location":"module_overview/#api-integration-utilities","title":"API Integration Utilities","text":""},{"location":"module_overview/#api-utils","title":"API Utils","text":"<ul> <li>YouTube API (<code>youtube_api.py</code>) - YouTube Data API v3 client</li> <li>YouTube Handler (<code>youtube_handler.py</code>) - Multi-method YouTube extraction</li> <li>Google Search (<code>google_search.py</code>) - Search with Bing fallback</li> <li>ScrapingBee (<code>scraping_bee.py</code>) - Premium scraping service</li> <li>ScrapFly (<code>scrapfly_client.py</code>) - Advanced scraping with circuit breaker</li> </ul>"},{"location":"module_overview/#other-utilities","title":"Other Utilities","text":""},{"location":"module_overview/#other-utils","title":"Other Utils","text":"<ul> <li>Sentiment Analysis (<code>sentiment_analysis.py</code>) - Batch GPT analysis</li> <li>Notifications (<code>notifications.py</code>) - Slack webhook integration</li> <li>Rate Limiter (<code>rate_limiter.py</code>) - Token bucket rate limiting</li> <li>Cache Manager (<code>cache_manager.py</code>) - SQLite caching (currently disabled)</li> <li>Model Variations (<code>model_variations.py</code>) - Vehicle name variations</li> </ul>"},{"location":"module_overview/#module-categories-by-function","title":"Module Categories by Function","text":""},{"location":"module_overview/#data-flow","title":"Data Flow","text":"<ol> <li>Input: Dashboard \u2192 Ingest</li> <li>Processing: Crawler Utils \u2192 API Utils</li> <li>Analysis: GPT Analysis \u2192 Sentiment Analysis</li> <li>Storage: Database Utils</li> <li>Output: Dashboard \u2192 Excel Export</li> </ol>"},{"location":"module_overview/#external-service-integration","title":"External Service Integration","text":"<ul> <li>AI: OpenAI GPT-4 (Analysis module)</li> <li>Search: Google, Bing (Search utils)</li> <li>Scraping: ScrapingBee, ScrapFly (API utils)</li> <li>Social: YouTube, CreatorIQ (Specialized modules)</li> <li>Notifications: Slack (Notifications util)</li> </ul>"},{"location":"module_overview/#system-infrastructure","title":"System Infrastructure","text":"<ul> <li>Caching: Cache Manager (disabled)</li> <li>Rate Limiting: Token bucket implementation</li> <li>Logging: Centralized logger</li> <li>Configuration: Environment-based config</li> <li>Database: Supabase PostgreSQL</li> </ul>"},{"location":"module_overview/#key-design-patterns","title":"Key Design Patterns","text":"<ol> <li>Singleton Pattern: Database manager, Cache manager</li> <li>Factory Pattern: Logger setup</li> <li>Circuit Breaker: ScrapFly client</li> <li>Token Bucket: Rate limiter</li> <li>Strategy Pattern: Tiered crawler escalation</li> <li>Observer Pattern: Progress callbacks</li> </ol>"},{"location":"module_overview/#integration-guidelines","title":"Integration Guidelines","text":""},{"location":"module_overview/#adding-new-modules","title":"Adding New Modules","text":"<ol> <li>Follow existing module structure</li> <li>Use centralized logger</li> <li>Implement error handling</li> <li>Add rate limiting for external APIs</li> <li>Document inputs/outputs clearly</li> </ol>"},{"location":"module_overview/#module-dependencies","title":"Module Dependencies","text":"<ul> <li>All modules use <code>logger.py</code></li> <li>API modules use <code>rate_limiter.py</code></li> <li>Crawlers use <code>content_extractor.py</code></li> <li>Database operations use <code>database.py</code></li> </ul>"},{"location":"module_overview/#testing-modules","title":"Testing Modules","text":"<ul> <li>Mock external services</li> <li>Use test databases</li> <li>Implement progress callbacks</li> <li>Handle async/sync patterns</li> </ul>"},{"location":"module_overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"module_overview/#bottlenecks","title":"Bottlenecks","text":"<ul> <li>GPT API calls (cost + latency)</li> <li>Premium scraping services</li> <li>Database queries for large datasets</li> </ul>"},{"location":"module_overview/#optimizations","title":"Optimizations","text":"<ul> <li>Batch processing for GPT</li> <li>Caching (when enabled)</li> <li>Concurrent processing</li> <li>Smart retry logic</li> </ul>"},{"location":"module_overview/#security-considerations","title":"Security Considerations","text":"<ul> <li>API keys in environment variables</li> <li>No hardcoded credentials</li> <li>SQL injection prevention</li> <li>XSS protection in dashboard</li> <li>Rate limiting for all external calls</li> </ul>"},{"location":"other-utils/","title":"Other Utilities Documentation","text":""},{"location":"other-utils/#module-srcutilssentiment_analysispy","title":"Module: <code>src/utils/sentiment_analysis.py</code>","text":""},{"location":"other-utils/#purpose","title":"Purpose","text":"<p>The Sentiment Analysis module provides a high-level interface for analyzing automotive media clips using GPT-4. It wraps the core analysis functionality with batch processing capabilities, progress tracking, and both synchronous and asynchronous execution patterns for UI integration.</p>"},{"location":"other-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"other-utils/#sentimentanalyzer-class","title":"SentimentAnalyzer Class","text":"<pre><code>class SentimentAnalyzer:\n    \"\"\"\n    High-level sentiment analysis for automotive clips.\n    Supports batch processing with progress tracking.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize analyzer (placeholder for future config).\"\"\"\n</code></pre>"},{"location":"other-utils/#core-methods","title":"Core Methods","text":"<pre><code>async def analyze_clip_sentiment(self, clip_data: Dict) -&gt; Dict:\n    \"\"\"\n    Analyze single clip asynchronously.\n    Returns comprehensive sentiment analysis.\n    \"\"\"\n\ndef analyze_clips_sync(self, clips: List[Dict], \n                      progress_callback=None) -&gt; List[Dict]:\n    \"\"\"\n    Synchronous batch analysis with progress updates.\n    Processes in batches of 5 to respect rate limits.\n    \"\"\"\n\nasync def analyze_clips_batch(self, clips: List[Dict],\n                            batch_size: int = 5) -&gt; List[Dict]:\n    \"\"\"\n    Asynchronous batch processing.\n    Implements concurrent processing with rate limiting.\n    \"\"\"\n</code></pre>"},{"location":"other-utils/#utility-functions","title":"Utility Functions","text":"<pre><code>def run_sentiment_analysis(clips: List[Dict], \n                          progress_callback=None) -&gt; List[Dict]:\n    \"\"\"\n    Main entry point for sentiment analysis.\n    Handles async loop creation for sync callers.\n    \"\"\"\n\ndef calculate_relevance_score_gpt(content: str, make: str, \n                                model: str) -&gt; float:\n    \"\"\"\n    Calculate relevance score using GPT.\n    Cost-optimized version for database pipeline.\n    \"\"\"\n</code></pre>"},{"location":"other-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"other-utils/#inputs","title":"Inputs","text":"<pre><code># Single clip\n{\n    'content': 'Article or transcript text...',\n    'url': 'https://example.com/review',\n    'make': 'Honda',\n    'model': 'Accord'\n}\n\n# Batch processing\nclips = [clip1, clip2, clip3, ...]\nprogress_callback = lambda current, total: print(f\"{current}/{total}\")\n</code></pre>"},{"location":"other-utils/#outputs","title":"Outputs","text":"<pre><code>{\n    'relevance_score': 85,\n    'overall_score': 8,\n    'overall_sentiment': 'positive',\n    'brand_alignment': True,\n    'summary': 'Comprehensive review highlighting...',\n    'aspects': {\n        'performance': {'score': 9, 'note': 'Excellent acceleration'},\n        'design': {'score': 7, 'note': 'Conservative but elegant'},\n        # ... other aspects\n    },\n    'pros': ['Fuel efficiency', 'Reliability', 'Tech features'],\n    'cons': ['Road noise', 'Firm suspension'],\n    'recommendation': 'Strong buy for family sedan buyers'\n}\n</code></pre>"},{"location":"other-utils/#dependencies","title":"Dependencies","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Optional, Callable\n\nfrom src.analysis.gpt_analysis import analyze_clip\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"other-utils/#processing-features","title":"Processing Features","text":"<ol> <li>Batch Size Management: Processes 5 clips at a time to avoid rate limits</li> <li>Progress Tracking: Real-time updates for UI progress bars</li> <li>Error Resilience: Continues processing even if individual clips fail</li> <li>Async/Sync Bridge: Handles async operations for sync callers</li> <li>Delay Management: 1-second delay between batches</li> </ol>"},{"location":"other-utils/#module-srcutilsnotificationspy","title":"Module: <code>src/utils/notifications.py</code>","text":""},{"location":"other-utils/#purpose_1","title":"Purpose","text":"<p>The Notifications module provides Slack webhook integration for sending real-time alerts and status updates. It implements retry logic with exponential backoff and supports formatted messages for better visibility in Slack channels.</p>"},{"location":"other-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"other-utils/#core-function","title":"Core Function","text":"<pre><code>def send_slack_message(message: str, webhook_url: str = None, \n                      max_retries: int = 3) -&gt; bool:\n    \"\"\"\n    Send notification to Slack channel.\n    Implements retry with exponential backoff.\n    \"\"\"\n</code></pre>"},{"location":"other-utils/#message-formatting","title":"Message Formatting","text":"<ol> <li>Status Prefixes:</li> <li><code>\"approved\"</code> \u2192 \u2705 prefix</li> <li><code>\"rejected\"</code> \u2192 \u274c prefix</li> <li> <p>Others \u2192 no prefix</p> </li> <li> <p>Block Format:    <code>json    {        \"blocks\": [{            \"type\": \"section\",            \"text\": {                \"type\": \"mrkdwn\",                \"text\": \"\u2705 Clip approved for Honda Accord\"            }        }]    }</code></p> </li> </ol>"},{"location":"other-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"other-utils/#inputs_1","title":"Inputs","text":"<pre><code># Simple message\nsend_slack_message(\"Processing completed\")\n\n# Status message\nsend_slack_message(\"Clip approved for Honda Accord\")\n\n# Custom webhook\nsend_slack_message(\"Alert!\", webhook_url=\"https://hooks.slack.com/...\")\n</code></pre>"},{"location":"other-utils/#outputs_1","title":"Outputs","text":"<ul> <li><code>True</code>: Message sent successfully</li> <li><code>False</code>: Failed after all retries</li> </ul>"},{"location":"other-utils/#dependencies_1","title":"Dependencies","text":"<pre><code>import os\nimport json\nimport time\nimport requests\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"other-utils/#configuration","title":"Configuration","text":"<pre><code># Environment variable\nexport SLACK_WEBHOOK_URL=\"https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\"\n</code></pre>"},{"location":"other-utils/#error-handling","title":"Error Handling","text":"<ol> <li>Retry Strategy: 3 attempts with exponential backoff (1s, 2s, 4s)</li> <li>HTTP Errors: Catches and logs all request exceptions</li> <li>Missing Webhook: Returns False with warning log</li> <li>Non-blocking: Never raises exceptions to caller</li> </ol>"},{"location":"other-utils/#module-srcutilsrate_limiterpy","title":"Module: <code>src/utils/rate_limiter.py</code>","text":""},{"location":"other-utils/#purpose_2","title":"Purpose","text":"<p>The Rate Limiter implements a token bucket algorithm to enforce rate limits on external API calls. It provides per-domain rate limiting with automatic domain detection and thread-safe operation for concurrent requests.</p>"},{"location":"other-utils/#key-functionsclasses_2","title":"Key Functions/Classes","text":""},{"location":"other-utils/#ratelimiter-class","title":"RateLimiter Class","text":"<pre><code>class RateLimiter:\n    \"\"\"\n    Token bucket rate limiter with per-domain limits.\n    Thread-safe implementation for concurrent access.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with default rate configurations.\"\"\"\n</code></pre>"},{"location":"other-utils/#core-methods_1","title":"Core Methods","text":"<pre><code>def wait_if_needed(self, url_or_domain: str, \n                  custom_rate: float = None,\n                  custom_per: float = None):\n    \"\"\"\n    Wait if rate limit would be exceeded.\n    Automatically detects and normalizes domains.\n    \"\"\"\n\ndef _ensure_bucket_exists(self, domain: str, rate: float, per: float):\n    \"\"\"\n    Create token bucket for new domain.\n    Initializes with full token capacity.\n    \"\"\"\n\ndef _refill_bucket(self, domain: str):\n    \"\"\"\n    Refill tokens based on elapsed time.\n    Implements token bucket algorithm.\n    \"\"\"\n</code></pre>"},{"location":"other-utils/#default-rate-limits","title":"Default Rate Limits","text":"<pre><code>default_rates = {\n    'openai.com': (5, 60),      # 5 requests per minute\n    'youtube.com': (10, 60),    # 10 requests per minute\n    'googleapis.com': (10, 60), # 10 requests per minute\n    # Unknown domains: 1 request per 2 seconds\n}\n</code></pre>"},{"location":"other-utils/#expected-inputsoutputs_2","title":"Expected Inputs/Outputs","text":""},{"location":"other-utils/#inputs_2","title":"Inputs","text":"<pre><code># URL-based (auto-detects domain)\nrate_limiter.wait_if_needed(\"https://api.openai.com/v1/completions\")\n\n# Domain-based\nrate_limiter.wait_if_needed(\"openai.com\")\n\n# Custom rate\nrate_limiter.wait_if_needed(\"custom-api.com\", custom_rate=100, custom_per=60)\n</code></pre>"},{"location":"other-utils/#outputs_2","title":"Outputs","text":"<ul> <li>No return value</li> <li>Blocks (sleeps) if rate limit would be exceeded</li> <li>Logs wait times when blocking</li> </ul>"},{"location":"other-utils/#dependencies_2","title":"Dependencies","text":"<pre><code>import time\nimport threading\nfrom urllib.parse import urlparse\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"other-utils/#token-bucket-algorithm","title":"Token Bucket Algorithm","text":"<ol> <li>Bucket Capacity: Equal to rate limit (e.g., 5 tokens)</li> <li>Refill Rate: Based on configured rate/period</li> <li>Token Consumption: 1 token per request</li> <li>Blocking: Waits for next token if bucket empty</li> <li>Thread Safety: Uses locks for concurrent access</li> </ol>"},{"location":"other-utils/#module-srcutilscache_managerpy","title":"Module: <code>src/utils/cache_manager.py</code>","text":""},{"location":"other-utils/#purpose_3","title":"Purpose","text":"<p>The Cache Manager provides SQLite-based caching for web scraping results with automatic expiration. IMPORTANT: Caching is currently DISABLED - the <code>get_cached_result</code> method always returns None to ensure fresh data retrieval.</p>"},{"location":"other-utils/#key-functionsclasses_3","title":"Key Functions/Classes","text":""},{"location":"other-utils/#cachemanager-class","title":"CacheManager Class","text":"<pre><code>class CacheManager:\n    \"\"\"\n    SQLite-based cache for scraping results.\n    Currently DISABLED - always returns cache miss.\n    \"\"\"\n\n    def __init__(self, db_path: str = None):\n        \"\"\"Initialize SQLite database with schema.\"\"\"\n</code></pre>"},{"location":"other-utils/#core-methods_2","title":"Core Methods","text":"<pre><code>def get_cached_result(self, person_id: str, domain: str,\n                     make: str, model: str) -&gt; Optional[Dict]:\n    \"\"\"\n    Check for cached result. \n    CURRENTLY DISABLED - always returns None.\n    \"\"\"\n\ndef store_result(self, person_id: str, domain: str, make: str,\n                model: str, url: str, content: str,\n                metadata: Dict = None) -&gt; bool:\n    \"\"\"\n    Store scraping result with 24-hour TTL.\n    Still functional for future use.\n    \"\"\"\n\ndef cleanup_expired(self) -&gt; int:\n    \"\"\"\n    Remove expired cache entries.\n    Returns count of deleted entries.\n    \"\"\"\n\ndef get_cache_stats(self) -&gt; Dict:\n    \"\"\"\n    Get cache statistics.\n    Returns entry counts and sizes.\n    \"\"\"\n</code></pre>"},{"location":"other-utils/#database-schema","title":"Database Schema","text":"<pre><code>CREATE TABLE scraping_cache (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    person_id TEXT NOT NULL,\n    domain TEXT NOT NULL,\n    make TEXT NOT NULL,\n    model TEXT NOT NULL,\n    url TEXT NOT NULL,\n    content TEXT NOT NULL,\n    metadata TEXT,  -- JSON string\n    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    expires_at TIMESTAMP NOT NULL,\n    UNIQUE(person_id, domain, make, model)\n);\n\n-- Indexes for performance\nCREATE INDEX idx_cache_lookup ON scraping_cache(person_id, domain, make, model);\nCREATE INDEX idx_cache_expiry ON scraping_cache(expires_at);\n</code></pre>"},{"location":"other-utils/#configuration_1","title":"Configuration","text":"<ul> <li>Database Path: <code>data/scraping_cache.db</code></li> <li>TTL: 24 hours (86,400 seconds)</li> <li>Cache Status: DISABLED for reliability</li> </ul>"},{"location":"other-utils/#dependencies_3","title":"Dependencies","text":"<pre><code>import sqlite3\nimport json\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"other-utils/#module-srcutilsmodel_variationspy","title":"Module: <code>src/utils/model_variations.py</code>","text":""},{"location":"other-utils/#purpose_4","title":"Purpose","text":"<p>The Model Variations module generates multiple formatting variations of vehicle model names to improve search coverage across different automotive websites. It handles common patterns like hyphenation, spacing, and make-model combinations.</p>"},{"location":"other-utils/#key-functionsclasses_4","title":"Key Functions/Classes","text":""},{"location":"other-utils/#core-function_1","title":"Core Function","text":"<pre><code>def generate_model_variations(make: str, model: str) -&gt; List[str]:\n    \"\"\"\n    Generate model name variations for better search coverage.\n    Handles spacing, hyphenation, and make combinations.\n    \"\"\"\n</code></pre>"},{"location":"other-utils/#variation-strategies","title":"Variation Strategies","text":"<ol> <li>Space/Hyphen Variations:</li> <li>\"CX-90\" \u2192 [\"cx-90\", \"cx 90\", \"cx90\"]</li> <li> <p>Handles both directions: with/without spaces/hyphens</p> </li> <li> <p>Make Prefix Combinations:</p> </li> <li>Adds make name: \"accord\" \u2192 \"honda accord\"</li> <li> <p>Both spaced and unspaced versions</p> </li> <li> <p>Numeric Patterns:</p> </li> <li>\"3 Series\" \u2194 \"3series\"</li> <li> <p>\"ES 350\" \u2194 \"ES350\"</p> </li> <li> <p>Abbreviation Handling:</p> </li> <li>Very limited due to ambiguity concerns</li> <li>Only complete word matches</li> </ol>"},{"location":"other-utils/#expected-inputsoutputs_3","title":"Expected Inputs/Outputs","text":""},{"location":"other-utils/#inputs_3","title":"Inputs","text":"<pre><code>generate_model_variations(\"Mazda\", \"CX-90\")\n</code></pre>"},{"location":"other-utils/#outputs_3","title":"Outputs","text":"<pre><code>[\n    \"cx-90\",\n    \"cx 90\", \n    \"cx90\",\n    \"mazda cx-90\",\n    \"mazda cx 90\",\n    \"mazda cx90\",\n    \"mazdacx-90\",\n    \"mazdacx 90\",\n    \"mazdacx90\"\n]\n</code></pre>"},{"location":"other-utils/#dependencies_4","title":"Dependencies","text":"<pre><code>import re\nfrom typing import List\n</code></pre>"},{"location":"other-utils/#variation-examples","title":"Variation Examples","text":"Make Model Key Variations Lexus ES 350 es350, lexus es 350 BMW 3 Series 3series, bmw 3 series Honda CR-V cr-v, crv, cr v, honda crv Tesla Model 3 model3, tesla model 3"},{"location":"other-utils/#design-decisions","title":"Design Decisions","text":"<ol> <li>Conservative Approach: Avoids aggressive abbreviations</li> <li>Case Insensitive: All variations in lowercase</li> <li>De-duplication: Uses sets to prevent duplicates</li> <li>Performance: Pre-compiled regex patterns</li> </ol>"},{"location":"overview/","title":"DriveShop Clip Tracking System","text":""},{"location":"overview/#project-purpose","title":"Project Purpose","text":"<p>The DriveShop Clip Tracking System is a comprehensive media monitoring and analysis platform designed to track automotive journalist coverage of vehicle loans for DriveShop's Fleet Management System (FMS). It automates the discovery, analysis, and tracking of media content across web platforms and YouTube, providing a streamlined workflow for campaign managers to review and approve clips. The system is engineered to process approximately 60 loans daily, finding the first media mention for each loan within a 2-hour nightly processing window, ensuring comprehensive coverage tracking for automotive PR campaigns while maintaining cost-effective AI usage.</p>"},{"location":"overview/#key-features","title":"Key Features","text":"<ul> <li>Automated Media Discovery: Multi-tier web scraping system with intelligent escalation (Scrapy \u2192 Enhanced Headers \u2192 Headless Rendering)</li> <li>Smart Discovery Mode: Automatic navigation through \"reviews\", \"blog\", or \"news\" sections for unknown media sites</li> <li>YouTube Integration: Specialized handling including RSS feed monitoring and transcript extraction</li> <li>AI-Powered Analysis: GPT-4 integration for content relevance scoring, sentiment analysis, and brand message alignment</li> <li>One Clip Per Loan: Intelligent deduplication to find only the first media mention</li> <li>Person-Outlet Mapping: Automatic association of journalists with their media outlets</li> <li>Streamlit Dashboard: Password-protected web interface for reviewing and approving clips</li> <li>FMS Integration: Export functionality with <code>approved_clips.csv</code> formatted for DriveShop's system</li> <li>RSS Optimization: Direct RSS feed support for faster processing when available</li> <li>Slack Notifications: Real-time alerts for approvals, flags, and system status</li> <li>Cost-Optimized: Designed to maintain ~$1-3 daily AI costs with efficient caching</li> </ul>"},{"location":"overview/#architecture-overview","title":"Architecture Overview","text":"<p>The system follows a modular architecture optimized for reliability and cost-effectiveness:</p>"},{"location":"overview/#core-components","title":"Core Components","text":"<ol> <li>Data Ingestion Pipeline (<code>src/ingest/</code>)</li> <li>Processes daily loan files from FMS</li> <li>Implements smart URL discovery for unknown media outlets</li> <li> <p>Manages one-clip-per-loan logic</p> </li> <li> <p>Tiered Web Crawling Framework (<code>src/crawler/</code>, <code>src/utils/</code>)</p> </li> <li>Level 1: Basic Scrapy with 2s delay, 1 concurrent request</li> <li>Level 2: Enhanced headers and cookie management</li> <li>Level 3: Headless browser rendering for JavaScript-heavy sites</li> <li> <p>RSS feed shortcuts for supported outlets</p> </li> <li> <p>AI Analysis Module (<code>src/analysis/</code>)</p> </li> <li>OpenAI GPT-4 Turbo for cost-effective analysis</li> <li>Evaluates relevance, sentiment, and brand message alignment</li> <li> <p>~$0.01-$0.03 per analysis call</p> </li> <li> <p>Dashboard Application (<code>src/dashboard/</code>)</p> </li> <li>Streamlit-based interface (no custom theming in MVP)</li> <li>AgGrid for efficient data manipulation</li> <li> <p>Password authentication (single shared password)</p> </li> <li> <p>Data Storage Layer</p> </li> <li>Supabase (PostgreSQL) for persistent storage</li> <li>Local SQLite cache for scraping results</li> <li>CSV export for FMS integration</li> </ol>"},{"location":"overview/#data-flow","title":"Data Flow","text":"<ol> <li>Input: Daily loan CSV from FMS or manual upload</li> <li>Discovery: Automated search with smart navigation for unknown sites</li> <li>Processing: Content extraction with tiered escalation</li> <li>Analysis: GPT-4 evaluates content (first mention only)</li> <li>Review: Manual approval via Streamlit dashboard</li> <li>Output: <code>approved_clips.csv</code> for FMS import</li> </ol>"},{"location":"overview/#setup-instructions","title":"Setup Instructions","text":""},{"location":"overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11.4 (specific version required)</li> <li>Docker and Docker Compose</li> <li>AWS EC2 t3.small instance (for production)</li> <li>API keys for required services</li> </ul>"},{"location":"overview/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file based on <code>.env.template</code>:</p> <pre><code># OpenAI Configuration\nOPENAI_API_KEY=your_openai_api_key\n\n# Supabase Database\nSUPABASE_URL=your_supabase_url\nSUPABASE_KEY=your_supabase_anon_key\nDATABASE_PASSWORD=your_database_password\n\n# Google Search API\nGOOGLE_API_KEY=your_google_api_key\nGOOGLE_SEARCH_ENGINE_ID=your_search_engine_id\n\n# Web Scraping Services\nSCRAPING_BEE_API_KEY=your_scraping_bee_key\nSCRAPFLY_API_KEY=your_scrapfly_key\n\n# YouTube API\nYOUTUBE_API_KEY=your_youtube_api_key\n\n# Slack Notifications\nSLACK_WEBHOOK_URL=your_slack_webhook_url\n\n# Streamlit Security\nSTREAMLIT_PASSWORD=your_dashboard_password\n\n# Application Settings\nAPI_BASE_URL=http://localhost:8000\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"overview/#local-development-setup","title":"Local Development Setup","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone &lt;repository_url&gt;    cd DriveShop_Clip</code></p> </li> <li> <p>Create Python 3.11.4 virtual environment:    <code>bash    python3.11 -m venv venv    source venv/bin/activate  # On Windows: venv\\Scripts\\activate</code></p> </li> <li> <p>Install dependencies:    <code>bash    pip install -r requirements.txt</code></p> </li> <li> <p>Install Playwright for Level 3 scraping:    <code>bash    playwright install chromium</code></p> </li> <li> <p>Run the Streamlit dashboard:    <code>bash    streamlit run src/dashboard/app.py</code></p> </li> </ol>"},{"location":"overview/#production-deployment-aws-ec2","title":"Production Deployment (AWS EC2)","text":"<ol> <li>Launch EC2 t3.small instance in us-east-1</li> <li>Configure security group:</li> <li>Port 22 (SSH)</li> <li>Port 8501 (Streamlit)</li> <li>SSH into instance and clone repository</li> <li>Create <code>.env</code> file with production credentials</li> <li>Build and run with Docker:    <code>bash    docker-compose up -d</code></li> <li>Access dashboard at <code>http://&lt;ec2-public-ip&gt;:8501</code></li> </ol>"},{"location":"overview/#usage-guide","title":"Usage Guide","text":""},{"location":"overview/#1-input-file-format","title":"1. Input File Format","text":"<p>Create CSV/Excel files with these required columns: - <code>Work Order Number</code> (WO): Unique FMS identifier - <code>Loan ID</code>: Unique loan identifier - <code>First Name</code>: Journalist's first name - <code>Last Name</code>: Journalist's last name - <code>Media Outlet</code>: Known outlet or \"Unknown\" - <code>Model</code>: Vehicle model (e.g., \"X5\", \"Accord\") - <code>Start Date</code>: Loan start date - <code>End Date</code>: Loan end date - <code>URL</code> (optional): Direct link or RSS feed</p>"},{"location":"overview/#2-dashboard-workflow","title":"2. Dashboard Workflow","text":"<ol> <li>Access: Navigate to Streamlit URL, enter password</li> <li>Upload: Use file uploader for loan CSV/Excel</li> <li>Process: Click \"Process Loans Without Clips\"</li> <li>System finds first mention only</li> <li>Uses RSS feeds when available</li> <li>Follows discovery logic for unknown sites</li> <li>Review: AgGrid displays found clips with:</li> <li>Relevance score (0-100)</li> <li>Sentiment analysis</li> <li>Brand message alignment</li> <li>Full content preview</li> <li>Actions:</li> <li>Approve/Reject individual clips</li> <li>Update media outlet assignments</li> <li>Trigger re-analysis if needed</li> <li>Export: Download <code>approved_clips.csv</code> for FMS</li> </ol>"},{"location":"overview/#3-automated-processing","title":"3. Automated Processing","text":"<p>Nightly cron job runs at 2 AM: - Processes all pending loans - Sends Slack notifications for new clips - Completes within 2-hour window - Maintains one-clip-per-loan rule</p>"},{"location":"overview/#4-special-features","title":"4. Special Features","text":"<ul> <li>RSS Shortcuts: Add RSS feed URLs to bypass crawling</li> <li>YouTube RSS: Use <code>https://www.youtube.com/feeds/videos.xml?channel_id=CHANNEL_ID</code></li> <li>Discovery Mode: Automatically navigates review/blog sections</li> <li>Model Variations: \"X5\" matches \"BMW X5\", \"2024 X5\", etc.</li> </ul>"},{"location":"overview/#performance-limitations","title":"Performance &amp; Limitations","text":""},{"location":"overview/#expected-performance","title":"Expected Performance","text":"<ul> <li>Daily Volume: ~60 loans with 2-4 URLs each</li> <li>Processing Time: &lt;2 hours for nightly batch</li> <li>Success Rate: &gt;90% content extraction</li> <li>AI Costs: $1-3 per day (~100 GPT-4 calls)</li> </ul>"},{"location":"overview/#current-limitations-mvp","title":"Current Limitations (MVP)","text":"<ul> <li>English content only</li> <li>No direct FMS API integration</li> <li>Single password authentication</li> <li>No mobile interface</li> <li>Manual deployment process</li> <li>One clip per loan maximum</li> </ul>"},{"location":"overview/#file-structure","title":"File Structure","text":"<pre><code>DriveShop_Clip/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 dashboard/\n\u2502   \u2502   \u2514\u2500\u2500 app.py              # Streamlit dashboard\n\u2502   \u251c\u2500\u2500 ingest/\n\u2502   \u2502   \u251c\u2500\u2500 ingest.py           # Core processing pipeline\n\u2502   \u2502   \u2514\u2500\u2500 ingest_database.py  # Database processing\n\u2502   \u251c\u2500\u2500 analysis/\n\u2502   \u2502   \u2514\u2500\u2500 gpt_analysis.py     # AI analysis (GPT-4)\n\u2502   \u251c\u2500\u2500 crawler/\n\u2502   \u2502   \u2514\u2500\u2500 crawler/spiders/    # Scrapy configurations\n\u2502   \u251c\u2500\u2500 creatoriq/              # Future integration\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 database.py         # Supabase client\n\u2502       \u251c\u2500\u2500 crawler_manager.py  # Tiered escalation\n\u2502       \u251c\u2500\u2500 youtube_api.py      # YouTube integration\n\u2502       \u2514\u2500\u2500 model_variations.py # Vehicle matching\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 person_outlets_mapping.csv  # Journalist mappings\n\u2502   \u2514\u2500\u2500 media_sources.csv           # JS-heavy site flags\n\u251c\u2500\u2500 documentation/              # Project documentation\n\u251c\u2500\u2500 docker-compose.yml         # Container orchestration\n\u251c\u2500\u2500 Dockerfile                 # Container definition\n\u251c\u2500\u2500 requirements.txt           # Python 3.11.4 deps\n\u2514\u2500\u2500 .env.template             # Environment template\n</code></pre>"},{"location":"overview/#deployment-notes","title":"Deployment Notes","text":""},{"location":"overview/#production-checklist","title":"Production Checklist","text":"<ol> <li>Infrastructure:</li> <li>AWS EC2 t3.small in us-east-1</li> <li>Security groups configured</li> <li> <p>Elastic IP recommended</p> </li> <li> <p>Environment:</p> </li> <li>Production <code>.env</code> file secured</li> <li>Secrets never in Git</li> <li> <p>API keys rotated quarterly</p> </li> <li> <p>Monitoring:</p> </li> <li>Slack webhook configured</li> <li>CloudWatch for EC2 metrics</li> <li> <p>Daily cost monitoring for APIs</p> </li> <li> <p>Cron Schedule:    <code>bash    # Nightly processing at 2 AM    0 2 * * * cd /app &amp;&amp; docker-compose run app python src/ingest/ingest_database.py</code></p> </li> </ol>"},{"location":"overview/#api-quotas-limits","title":"API Quotas &amp; Limits","text":"<ul> <li>YouTube API: 10,000 units/day quota</li> <li>OpenAI: Monitor token usage (~$1-3/day target)</li> <li>ScrapingBee/ScrapFly: Check monthly limits</li> <li>Google Search: 100 queries/day free tier</li> </ul>"},{"location":"overview/#maintenance-tasks","title":"Maintenance Tasks","text":"<ul> <li>Weekly: Clear SQLite cache if &gt;1GB</li> <li>Monthly: Review API usage and costs</li> <li>Quarterly: Update dependencies, rotate API keys</li> <li>As Needed: Update <code>person_outlets_mapping.csv</code></li> </ul>"},{"location":"overview/#future-enhancements-post-mvp","title":"Future Enhancements (Post-MVP)","text":"<ul> <li>Direct FMS API integration</li> <li>Multi-user authentication with roles</li> <li>Airtable or advanced database storage</li> <li>Email notification system</li> <li>Mobile-responsive interface</li> <li>Multi-language content support</li> <li>Advanced analytics dashboard</li> <li>Automated model training for relevance</li> </ul>"},{"location":"overview/#support-troubleshooting","title":"Support &amp; Troubleshooting","text":"<ul> <li>Logs: Check <code>app.log</code> for detailed debugging</li> <li>Common Issues:</li> <li>403 errors: Site needs Level 3 escalation</li> <li>No clips found: Check discovery keywords</li> <li>Slow processing: Review concurrent limits</li> <li>Documentation: See <code>/documentation</code> folder</li> <li>Slack Channel: Real-time system notifications</li> </ul> <p>For technical support, reference the implementation plan and technical documentation, or contact the development team.</p>"},{"location":"modules/","title":"DriveShop Clip Tracking - Module Documentation Index","text":"<p>This directory contains comprehensive documentation for all modules in the DriveShop Clip Tracking system. Each module is documented with its purpose, key functions, expected inputs/outputs, and dependencies.</p>"},{"location":"modules/#core-application-modules","title":"Core Application Modules","text":""},{"location":"modules/#1-dashboard-module","title":"1. Dashboard Module","text":"<p>Location: <code>src/dashboard/app.py</code> Purpose: Streamlit-based web interface for reviewing and managing media clips Key Features: File upload, AgGrid tables, approval workflow, Excel export</p>"},{"location":"modules/#2-ingest-modules","title":"2. Ingest Modules","text":"<p>Location: <code>src/ingest/</code> Purpose: Data processing pipeline for finding and analyzing media clips Components: - <code>ingest.py</code> - CSV-based processing - <code>ingest_database.py</code> - Database-integrated processing with smart retry</p>"},{"location":"modules/#3-analysis-module","title":"3. Analysis Module","text":"<p>Location: <code>src/analysis/gpt_analysis.py</code> Purpose: AI-powered content analysis using GPT-4 Key Features: Marketing insights, sentiment analysis, cost optimization</p>"},{"location":"modules/#4-crawler-module","title":"4. Crawler Module","text":"<p>Location: <code>src/crawler/crawler/spiders/loan_spider.py</code> Purpose: Scrapy spider for web content discovery Key Features: Multi-level crawling, smart discovery, content extraction</p>"},{"location":"modules/#5-creatoriq-module","title":"5. CreatorIQ Module","text":"<p>Location: <code>src/creatoriq/</code> Purpose: Integration with CreatorIQ influencer platform Key Features: Multiple auth methods, GraphQL client, CSV export</p>"},{"location":"modules/#utility-modules","title":"Utility Modules","text":""},{"location":"modules/#core-utilities","title":"Core Utilities","text":""},{"location":"modules/#core-utils","title":"Core Utils","text":"<ul> <li>Database (<code>database.py</code>) - Supabase integration with retry logic</li> <li>Config (<code>config.py</code>) - Centralized configuration constants</li> <li>Logger (<code>logger.py</code>) - Standardized logging setup</li> </ul>"},{"location":"modules/#web-crawling-utilities","title":"Web Crawling Utilities","text":""},{"location":"modules/#crawler-utils","title":"Crawler Utils","text":"<ul> <li>Enhanced Crawler Manager (<code>enhanced_crawler_manager.py</code>) - 6-tier escalation system</li> <li>Content Extractor (<code>content_extractor.py</code>) - Intelligent HTML extraction</li> <li>Browser Crawler (<code>browser_crawler.py</code>) - Playwright automation</li> <li>Date Extractor (<code>date_extractor.py</code>) - Publication date detection</li> <li>Escalation (<code>escalation.py</code>) - Domain-specific strategies</li> </ul>"},{"location":"modules/#api-integration-utilities","title":"API Integration Utilities","text":""},{"location":"modules/#api-utils","title":"API Utils","text":"<ul> <li>YouTube API (<code>youtube_api.py</code>) - YouTube Data API v3 client</li> <li>YouTube Handler (<code>youtube_handler.py</code>) - Multi-method YouTube extraction</li> <li>Google Search (<code>google_search.py</code>) - Search with Bing fallback</li> <li>ScrapingBee (<code>scraping_bee.py</code>) - Premium scraping service</li> <li>ScrapFly (<code>scrapfly_client.py</code>) - Advanced scraping with circuit breaker</li> </ul>"},{"location":"modules/#other-utilities","title":"Other Utilities","text":""},{"location":"modules/#other-utils","title":"Other Utils","text":"<ul> <li>Sentiment Analysis (<code>sentiment_analysis.py</code>) - Batch GPT analysis</li> <li>Notifications (<code>notifications.py</code>) - Slack webhook integration</li> <li>Rate Limiter (<code>rate_limiter.py</code>) - Token bucket rate limiting</li> <li>Cache Manager (<code>cache_manager.py</code>) - SQLite caching (currently disabled)</li> <li>Model Variations (<code>model_variations.py</code>) - Vehicle name variations</li> </ul>"},{"location":"modules/#module-categories-by-function","title":"Module Categories by Function","text":""},{"location":"modules/#data-flow","title":"Data Flow","text":"<ol> <li>Input: Dashboard \u2192 Ingest</li> <li>Processing: Crawler Utils \u2192 API Utils</li> <li>Analysis: GPT Analysis \u2192 Sentiment Analysis</li> <li>Storage: Database Utils</li> <li>Output: Dashboard \u2192 Excel Export</li> </ol>"},{"location":"modules/#external-service-integration","title":"External Service Integration","text":"<ul> <li>AI: OpenAI GPT-4 (Analysis module)</li> <li>Search: Google, Bing (Search utils)</li> <li>Scraping: ScrapingBee, ScrapFly (API utils)</li> <li>Social: YouTube, CreatorIQ (Specialized modules)</li> <li>Notifications: Slack (Notifications util)</li> </ul>"},{"location":"modules/#system-infrastructure","title":"System Infrastructure","text":"<ul> <li>Caching: Cache Manager (disabled)</li> <li>Rate Limiting: Token bucket implementation</li> <li>Logging: Centralized logger</li> <li>Configuration: Environment-based config</li> <li>Database: Supabase PostgreSQL</li> </ul>"},{"location":"modules/#key-design-patterns","title":"Key Design Patterns","text":"<ol> <li>Singleton Pattern: Database manager, Cache manager</li> <li>Factory Pattern: Logger setup</li> <li>Circuit Breaker: ScrapFly client</li> <li>Token Bucket: Rate limiter</li> <li>Strategy Pattern: Tiered crawler escalation</li> <li>Observer Pattern: Progress callbacks</li> </ol>"},{"location":"modules/#integration-guidelines","title":"Integration Guidelines","text":""},{"location":"modules/#adding-new-modules","title":"Adding New Modules","text":"<ol> <li>Follow existing module structure</li> <li>Use centralized logger</li> <li>Implement error handling</li> <li>Add rate limiting for external APIs</li> <li>Document inputs/outputs clearly</li> </ol>"},{"location":"modules/#module-dependencies","title":"Module Dependencies","text":"<ul> <li>All modules use <code>logger.py</code></li> <li>API modules use <code>rate_limiter.py</code></li> <li>Crawlers use <code>content_extractor.py</code></li> <li>Database operations use <code>database.py</code></li> </ul>"},{"location":"modules/#testing-modules","title":"Testing Modules","text":"<ul> <li>Mock external services</li> <li>Use test databases</li> <li>Implement progress callbacks</li> <li>Handle async/sync patterns</li> </ul>"},{"location":"modules/#performance-considerations","title":"Performance Considerations","text":""},{"location":"modules/#bottlenecks","title":"Bottlenecks","text":"<ul> <li>GPT API calls (cost + latency)</li> <li>Premium scraping services</li> <li>Database queries for large datasets</li> </ul>"},{"location":"modules/#optimizations","title":"Optimizations","text":"<ul> <li>Batch processing for GPT</li> <li>Caching (when enabled)</li> <li>Concurrent processing</li> <li>Smart retry logic</li> </ul>"},{"location":"modules/#security-considerations","title":"Security Considerations","text":"<ul> <li>API keys in environment variables</li> <li>No hardcoded credentials</li> <li>SQL injection prevention</li> <li>XSS protection in dashboard</li> <li>Rate limiting for all external calls</li> </ul>"},{"location":"modules/analysis/","title":"Analysis Module Documentation","text":""},{"location":"modules/analysis/#module-srcanalysisgpt_analysispy","title":"Module: <code>src/analysis/gpt_analysis.py</code>","text":""},{"location":"modules/analysis/#purpose","title":"Purpose","text":"<p>The Analysis module provides AI-powered content analysis using OpenAI's GPT-4 Turbo model to evaluate media clips for relevance, sentiment, and brand alignment. It implements sophisticated parsing strategies, cost-optimization through pre-filtering, and specialized prompts for different content types (YouTube videos vs. web articles). The module is designed to extract strategic marketing insights from automotive media content while maintaining cost efficiency.</p>"},{"location":"modules/analysis/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/analysis/#core-analysis-functions","title":"Core Analysis Functions","text":"<pre><code>def analyze_clip(content: str, make: str, model: str, \n                max_retries: int = 3, url: str = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Comprehensive automotive sentiment analysis using GPT-4.\n    Implements pre-filters to save API costs and specialized prompts.\n    Returns detailed analysis with marketing insights or None if fails.\n    \"\"\"\n\ndef analyze_clip_relevance_only(content: str, make: str, model: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Cost-optimized analysis for relevance scoring only.\n    Skips detailed sentiment analysis to reduce API costs.\n    Used in database ingestion pipeline.\n    \"\"\"\n</code></pre>"},{"location":"modules/analysis/#json-parsing-functions","title":"JSON Parsing Functions","text":"<pre><code>def parse_json_with_fallbacks(response_text: str) -&gt; dict:\n    \"\"\"\n    Robust JSON parsing with 4 escalating strategies:\n    1. Clean and parse normally\n    2. Aggressive comma fixing\n    3. Manual field extraction with regex\n    4. Return None (no fallback data)\n    \"\"\"\n\ndef clean_json_response(response_text: str) -&gt; str:\n    \"\"\"\n    Cleans GPT responses by removing markdown blocks and fixing JSON issues.\n    Handles edge cases that commonly cause parsing failures.\n    \"\"\"\n</code></pre>"},{"location":"modules/analysis/#helper-functions","title":"Helper Functions","text":"<pre><code>def get_openai_key() -&gt; Optional[str]:\n    \"\"\"\n    Retrieves OpenAI API key from environment variables.\n    Returns None if not configured.\n    \"\"\"\n\ndef flexible_model_match(title: str, model: str) -&gt; bool:\n    \"\"\"\n    Intelligent vehicle model matching in content.\n    Handles variations like \"X5\" matching \"BMW X5\", \"2024 X5\", etc.\n    \"\"\"\n</code></pre>"},{"location":"modules/analysis/#gptanalyzer-class","title":"GPTAnalyzer Class","text":"<pre><code>class GPTAnalyzer:\n    \"\"\"\n    Class-based analyzer for content using OpenAI's GPT-4 Turbo.\n    Handles API calls with retry logic and rate limiting.\n    \"\"\"\n\n    def analyze_content(self, content: str, vehicle_make: str, \n                       vehicle_model: str, max_retries: int = 3,\n                       timeout: int = 60) -&gt; Dict[str, Any]:\n        \"\"\"\n        Analyzes content for relevance, sentiment, and brand messaging.\n        Includes retry logic and rate limiting.\n        \"\"\"\n</code></pre>"},{"location":"modules/analysis/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/analysis/#inputs","title":"Inputs","text":"<ol> <li>Content Analysis:</li> <li><code>content</code>: Article text or YouTube transcript (HTML or plain text)</li> <li><code>make</code>: Vehicle manufacturer (e.g., \"Honda\")</li> <li><code>model</code>: Vehicle model (e.g., \"Accord\")</li> <li><code>url</code>: Optional URL for content type detection</li> <li> <p><code>max_retries</code>: Number of retry attempts (default: 3)</p> </li> <li> <p>Pre-Filter Thresholds:</p> </li> <li>Minimum content length: 200 characters</li> <li>Printable character ratio: &gt;80%</li> <li>Generic page indicators: &lt;2 matches</li> </ol>"},{"location":"modules/analysis/#outputs","title":"Outputs","text":"<ol> <li> <p>Comprehensive Analysis (analyze_clip):    <code>json    {      \"relevance_score\": 0-10,      \"overall_score\": 0-10,      \"overall_sentiment\": \"positive/neutral/negative\",      \"brand_alignment\": true/false,      \"summary\": \"Executive summary for CMO briefing\",      \"recommendation\": \"Strategic marketing recommendation\",      \"aspects\": {        \"performance\": {\"score\": 0-10, \"note\": \"...\"},        \"exterior_design\": {\"score\": 0-10, \"note\": \"...\"},        \"interior_comfort\": {\"score\": 0-10, \"note\": \"...\"},        \"technology\": {\"score\": 0-10, \"note\": \"...\"},        \"value\": {\"score\": 0-10, \"note\": \"...\"}      },      \"pros\": [\"key positive 1\", \"key positive 2\"],      \"cons\": [\"key concern 1\", \"key concern 2\"],      \"key_mentions\": [\"topic 1\", \"topic 2\"],      \"video_quotes\": [\"memorable quote 1\", \"memorable quote 2\"]    }</code></p> </li> <li> <p>Relevance-Only Analysis:    <code>json    {      \"relevance_score\": 0-10    }</code></p> </li> <li> <p>Failure Response: <code>None</code> (no mock data)</p> </li> </ol>"},{"location":"modules/analysis/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport openai  # Version 0.27.0 (older client format)\nfrom dotenv import load_dotenv\n\n# Internal Modules\nfrom src.utils.logger import setup_logger\nfrom src.utils.rate_limiter import rate_limiter\nfrom src.utils.content_extractor import extract_article_content\n</code></pre>"},{"location":"modules/analysis/#cost-optimization-features","title":"Cost Optimization Features","text":""},{"location":"modules/analysis/#pre-filters-save-api-costs","title":"Pre-Filters (Save API Costs)","text":"<ol> <li>Content Length Check: Skip if &lt;200 characters</li> <li>Keyword Validation: Must mention make OR model</li> <li>Content Quality Check: &gt;80% printable characters</li> <li>Generic Page Detection: Avoid category/index pages</li> </ol>"},{"location":"modules/analysis/#processing-optimizations","title":"Processing Optimizations","text":"<ul> <li>Content truncation at 12,000 characters</li> <li>Relevance-only mode for batch processing</li> <li>Caching in upstream modules</li> <li>Smart retry with exponential backoff</li> </ul>"},{"location":"modules/analysis/#prompt-templates","title":"Prompt Templates","text":""},{"location":"modules/analysis/#youtube-video-analysis","title":"YouTube Video Analysis","text":"<ul> <li>Strategic marketing focus</li> <li>Creator intelligence (influence tier, audience archetype)</li> <li>Competitive intelligence</li> <li>Purchase intent signals</li> <li>Viral potential assessment</li> <li>Action items for marketing team</li> </ul>"},{"location":"modules/analysis/#web-article-analysis","title":"Web Article Analysis","text":"<ul> <li>Publication credibility assessment</li> <li>SEO/Digital influence factor</li> <li>Editorial stance detection</li> <li>Media relationship strategy</li> <li>Brand narrative impact</li> <li>Shareable quote extraction</li> </ul>"},{"location":"modules/analysis/#error-handling","title":"Error Handling","text":"<ol> <li>API Key Missing: Returns <code>None</code> instead of mock data</li> <li>JSON Parsing Failures: </li> <li>4 escalating strategies</li> <li>Manual field extraction fallback</li> <li>No fake data on complete failure</li> <li>API Errors:</li> <li>Rate limit handling with 30s wait</li> <li>Timeout retry with 5s wait</li> <li>Exponential backoff (2^attempt seconds)</li> <li>Content Issues:</li> <li>HTML extraction for web articles</li> <li>Truncation for oversized content</li> <li>Graceful handling of corrupted data</li> </ol>"},{"location":"modules/analysis/#strategic-analysis-features","title":"Strategic Analysis Features","text":""},{"location":"modules/analysis/#marketing-intelligence","title":"Marketing Intelligence","text":"<ul> <li>Impact Scoring: 1-10 scale for CMO attention</li> <li>Brand Perception: Ascending/Stable/Declining trajectory</li> <li>Messaging Opportunities: Extracted from content</li> <li>Risk Identification: Brand vulnerabilities</li> </ul>"},{"location":"modules/analysis/#competitive-analysis","title":"Competitive Analysis","text":"<ul> <li>Positioning vs. competitors</li> <li>Market advantages highlighted</li> <li>Vulnerabilities exposed</li> <li>Differentiation opportunities</li> </ul>"},{"location":"modules/analysis/#creatorpublication-analysis","title":"Creator/Publication Analysis","text":"<ul> <li>YouTube: Influence tier, audience archetype, credibility</li> <li>Articles: Publication credibility, reach, editorial stance</li> <li>Relationship recommendations (Engage/Monitor/Ignore)</li> </ul>"},{"location":"modules/analysis/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Token Usage: ~4,000 tokens per analysis</li> <li>API Costs: $0.01-$0.03 per call</li> <li>Response Time: 2-10 seconds typical</li> <li>Retry Strategy: Max 3 attempts with backoff</li> <li>Rate Limiting: Integrated with global limiter</li> </ul>"},{"location":"modules/analysis/#security-considerations","title":"Security Considerations","text":"<ul> <li>API key stored in environment variables</li> <li>No sensitive data logged</li> <li>Content sanitization before processing</li> <li>Secure API communication</li> <li>No data persistence in module</li> </ul>"},{"location":"modules/api-utils/","title":"API Utilities Documentation","text":""},{"location":"modules/api-utils/#module-srcutilsyoutube_apipy","title":"Module: <code>src/utils/youtube_api.py</code>","text":""},{"location":"modules/api-utils/#purpose","title":"Purpose","text":"<p>The YouTube API module provides a clean interface to YouTube Data API v3 for searching and retrieving video information. It implements intelligent search strategies with model variations, date filtering, and comprehensive error handling while respecting API quotas.</p>"},{"location":"modules/api-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/api-utils/#youtubeapiclient-class","title":"YouTubeAPIClient Class","text":"<pre><code>class YouTubeAPIClient:\n    \"\"\"\n    YouTube Data API v3 client with rate limiting and error handling.\n    Provides video search and channel video listing capabilities.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with API key from environment.\"\"\"\n</code></pre>"},{"location":"modules/api-utils/#core-methods","title":"Core Methods","text":"<pre><code>def search_videos(self, make: str, model: str, journalist_name: str = None,\n                 start_date: str = None, end_date: str = None) -&gt; List[Dict]:\n    \"\"\"\n    Search YouTube videos with intelligent query building.\n    Implements model variations for better matching.\n    \"\"\"\n\ndef get_channel_videos(self, channel_id: str, max_results: int = 10) -&gt; List[Dict]:\n    \"\"\"\n    Retrieve latest videos from a specific channel.\n    Used for RSS feed alternative.\n    \"\"\"\n\ndef get_video_details(self, video_id: str) -&gt; Optional[Dict]:\n    \"\"\"\n    Get detailed information for a specific video.\n    Includes duration, views, likes, etc.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#helper-methods","title":"Helper Methods","text":"<pre><code>def generate_model_variations(model: str) -&gt; List[str]:\n    \"\"\"\n    Generate search variations for vehicle models.\n    Example: \"X5\" \u2192 [\"X5\", \"BMW X5\", \"X5 BMW\"]\n    \"\"\"\n\ndef _make_api_request(self, endpoint: str, params: Dict) -&gt; Optional[Dict]:\n    \"\"\"\n    Internal method for API requests with rate limiting.\n    Handles errors and returns parsed JSON.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/api-utils/#inputs","title":"Inputs","text":"<pre><code># Video search\n{\n    'make': 'Honda',\n    'model': 'Accord',\n    'journalist_name': 'Doug DeMuro',\n    'start_date': '2024-01-01',\n    'end_date': '2024-01-31'\n}\n\n# Channel videos\n{\n    'channel_id': 'UC123abc...',\n    'max_results': 10\n}\n</code></pre>"},{"location":"modules/api-utils/#outputs","title":"Outputs","text":"<pre><code># Video data structure\n{\n    'video_id': 'dQw4w9WgXcQ',\n    'title': '2024 Honda Accord Review',\n    'channel': 'Car Reviews',\n    'channel_id': 'UC123...',\n    'published_at': '2024-01-15T10:00:00Z',\n    'description': 'Full review of...',\n    'thumbnail': 'https://i.ytimg.com/...',\n    'url': 'https://youtube.com/watch?v=...'\n}\n</code></pre>"},{"location":"modules/api-utils/#dependencies","title":"Dependencies","text":"<pre><code>import os\nimport requests\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\n\nfrom src.utils.logger import logger\nfrom src.utils.rate_limiter import rate_limiter\n</code></pre>"},{"location":"modules/api-utils/#api-integration-patterns","title":"API Integration Patterns","text":"<ol> <li>Rate Limiting: Centralized rate limiter before each API call</li> <li>Error Handling: Graceful degradation with empty results</li> <li>Query Building: Progressive search from specific to general</li> <li>Pagination: Handles up to 50 results per request</li> <li>Quota Management: Efficient API usage with targeted queries</li> </ol>"},{"location":"modules/api-utils/#module-srcutilsyoutube_handlerpy","title":"Module: <code>src/utils/youtube_handler.py</code>","text":""},{"location":"modules/api-utils/#purpose_1","title":"Purpose","text":"<p>The YouTube Handler provides comprehensive YouTube content extraction using multiple methods including RSS feeds, direct scraping, and API fallbacks. It specializes in extracting video metadata and transcripts without requiring API keys for most operations.</p>"},{"location":"modules/api-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"modules/api-utils/#core-functions","title":"Core Functions","text":"<pre><code>def extract_youtube_content(url: str, make: str = None, model: str = None,\n                          journalist_name: str = None,\n                          start_date: str = None, end_date: str = None) -&gt; Dict:\n    \"\"\"\n    Main extraction function with multiple fallback methods.\n    Attempts RSS \u2192 Direct scraping \u2192 API.\n    \"\"\"\n\ndef extract_channel_videos_from_rss(channel_url: str, limit: int = 10) -&gt; List[Dict]:\n    \"\"\"\n    Extract videos via YouTube RSS feed (no API needed).\n    Fast and reliable for recent videos.\n    \"\"\"\n\ndef extract_channel_id(channel_url: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract channel ID using multiple methods.\n    Handles various YouTube URL formats.\n    \"\"\"\n\ndef check_model_in_title(title: str, model: str) -&gt; bool:\n    \"\"\"\n    Flexible model matching in video titles.\n    Handles variations and partial matches.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#transcript-extraction","title":"Transcript Extraction","text":"<pre><code>def get_video_transcript(video_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract video transcript/captions.\n    Falls back to metadata if transcript unavailable.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#content-extraction-methods","title":"Content Extraction Methods","text":"<ol> <li>RSS Feed Method (Preferred)</li> <li>No API key required</li> <li>Returns latest 15 videos</li> <li>Structured XML data</li> <li> <p>Very fast and reliable</p> </li> <li> <p>Direct Scraping</p> </li> <li>BeautifulSoup HTML parsing</li> <li>Pattern matching for data extraction</li> <li> <p>Works when RSS unavailable</p> </li> <li> <p>API Fallback</p> </li> <li>Uses YouTube API client</li> <li>Only when other methods fail</li> <li>Preserves API quota</li> </ol>"},{"location":"modules/api-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"modules/api-utils/#inputs_1","title":"Inputs","text":"<pre><code>{\n    'url': 'https://youtube.com/watch?v=abc123',\n    'make': 'Toyota',\n    'model': 'Camry',\n    'journalist_name': 'Alex on Autos',\n    'start_date': '2024-01-01',\n    'end_date': '2024-01-31'\n}\n</code></pre>"},{"location":"modules/api-utils/#outputs_1","title":"Outputs","text":"<pre><code>{\n    'content': 'Video transcript or description...',\n    'title': '2024 Toyota Camry Review',\n    'url': 'https://youtube.com/watch?v=abc123',\n    'published_date': '2024-01-15',\n    'channel': 'Alex on Autos',\n    'views': '50000',\n    'duration': 'PT15M30S'\n}\n</code></pre>"},{"location":"modules/api-utils/#module-srcutilsgoogle_searchpy","title":"Module: <code>src/utils/google_search.py</code>","text":""},{"location":"modules/api-utils/#purpose_2","title":"Purpose","text":"<p>The Google Search module provides web search capabilities using Google Custom Search API with Bing as a fallback. It implements intelligent query building, result filtering, and attribution verification for finding automotive journalism content.</p>"},{"location":"modules/api-utils/#key-functionsclasses_2","title":"Key Functions/Classes","text":""},{"location":"modules/api-utils/#core-search-functions","title":"Core Search Functions","text":"<pre><code>def search_google(query: str, site: str = None, num_results: int = 10) -&gt; List[Dict]:\n    \"\"\"\n    Search using Google Custom Search API.\n    Supports site-specific searches.\n    \"\"\"\n\ndef search_for_article(make: str, model: str, journalist_name: str,\n                      media_outlet: str = None, start_date: str = None,\n                      end_date: str = None) -&gt; List[Dict]:\n    \"\"\"\n    Comprehensive article search with multiple strategies.\n    Implements fallback from Google to Bing.\n    \"\"\"\n\nasync def search_for_article_async(make: str, model: str, \n                                  journalist_name: str, **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Async version for concurrent searches.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#result-processing","title":"Result Processing","text":"<pre><code>def filter_and_score_results(results: List[Dict], make: str, model: str,\n                           journalist_name: str, start_date: str = None) -&gt; List[Dict]:\n    \"\"\"\n    Score and filter search results.\n    Implements relevance scoring algorithm.\n    \"\"\"\n\ndef verify_author_attribution(url: str, content: str, \n                            journalist_name: str) -&gt; Dict:\n    \"\"\"\n    Verify if content is actually by the journalist.\n    Returns attribution strength.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#search-strategy","title":"Search Strategy","text":"<ol> <li>Query Building Hierarchy:</li> <li>Full query: <code>\"John Doe\" Honda Accord review site:example.com</code></li> <li>Without quotes: <code>John Doe Honda Accord review</code></li> <li>Model only: <code>Honda Accord review site:example.com</code></li> <li> <p>Broad search: <code>Honda Accord</code></p> </li> <li> <p>Result Scoring Algorithm:</p> </li> <li>URL keyword matches (+3 per keyword)</li> <li>Title matches for make/model/journalist</li> <li>Domain restrictions</li> <li> <p>Date filtering</p> </li> <li> <p>Attribution Verification:</p> </li> <li>Byline extraction</li> <li>Author meta tag checking</li> <li>Name proximity to article markers</li> </ol>"},{"location":"modules/api-utils/#api-configuration","title":"API Configuration","text":"<pre><code># Google Custom Search\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\nGOOGLE_SEARCH_ENGINE_ID = os.getenv('GOOGLE_SEARCH_ENGINE_ID')\n\n# Bing Search (Fallback)\nBING_API_KEY = os.getenv('BING_SEARCH_API_KEY')\n</code></pre>"},{"location":"modules/api-utils/#module-srcutilsscraping_beepy","title":"Module: <code>src/utils/scraping_bee.py</code>","text":""},{"location":"modules/api-utils/#purpose_3","title":"Purpose","text":"<p>The ScrapingBee module provides integration with ScrapingBee API for handling JavaScript-heavy sites and bypassing anti-bot measures. It implements domain-specific configurations and intelligent retry mechanisms.</p>"},{"location":"modules/api-utils/#key-functionsclasses_3","title":"Key Functions/Classes","text":""},{"location":"modules/api-utils/#scrapingbeeclient-class","title":"ScrapingBeeClient Class","text":"<pre><code>class ScrapingBeeClient:\n    \"\"\"\n    ScrapingBee API client with retry logic and domain configurations.\n    Handles JavaScript rendering and premium proxies.\n    \"\"\"\n\n    def __init__(self, api_key: str = None):\n        \"\"\"Initialize with API key validation.\"\"\"\n</code></pre>"},{"location":"modules/api-utils/#core-methods_1","title":"Core Methods","text":"<pre><code>def scrape(self, url: str, render_js: bool = True, \n          premium_proxy: bool = False, **kwargs) -&gt; Optional[str]:\n    \"\"\"\n    Scrape URL with configurable options.\n    Implements retry with exponential backoff.\n    \"\"\"\n\ndef get_credits_used(self) -&gt; Dict[str, int]:\n    \"\"\"\n    Check API credit usage.\n    Returns used and remaining credits.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#domain-specific-configurations","title":"Domain-Specific Configurations","text":"<pre><code># YouTube configuration\nif 'youtube.com' in url:\n    params.update({\n        'block_ads': True,\n        'wait': 3000,\n        'wait_for': '#description'\n    })\n\n# Spotlight configuration  \nif 'spotlightautomotive.com' in url:\n    params.update({\n        'wait': 5000,\n        'screenshot': False\n    })\n</code></pre>"},{"location":"modules/api-utils/#retry-strategy","title":"Retry Strategy","text":"<ol> <li>Max Retries: 3 attempts</li> <li>Exponential Backoff: 1s \u2192 2s \u2192 4s</li> <li>Error-Specific Handling:</li> <li>403: Upgrade to premium proxy</li> <li>422: Validation error (no retry)</li> <li>429: Rate limit (longer wait)</li> <li>500: Server error (retry)</li> </ol>"},{"location":"modules/api-utils/#module-srcutilsscrapfly_clientpy","title":"Module: <code>src/utils/scrapfly_client.py</code>","text":""},{"location":"modules/api-utils/#purpose_4","title":"Purpose","text":"<p>The ScrapFly module provides the most sophisticated web scraping integration with circuit breaker pattern, advanced rate limiting, and comprehensive error handling. It serves as the primary premium scraping service.</p>"},{"location":"modules/api-utils/#key-functionsclasses_4","title":"Key Functions/Classes","text":""},{"location":"modules/api-utils/#scrapflyclient-class","title":"ScrapFlyClient Class","text":"<pre><code>class ScrapFlyClient:\n    \"\"\"\n    ScrapFly API client with circuit breaker and rate limiting.\n    Most robust scraping solution in the stack.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with API key and circuit breaker state.\"\"\"\n</code></pre>"},{"location":"modules/api-utils/#core-methods_2","title":"Core Methods","text":"<pre><code>def scrape(self, url: str, render_js: bool = True,\n          country: str = \"US\", **kwargs) -&gt; Optional[str]:\n    \"\"\"\n    Scrape with circuit breaker protection.\n    Implements sophisticated rate limiting.\n    \"\"\"\n\ndef extract_scrapfly_content(self, url: str, config: Dict = None) -&gt; Optional[str]:\n    \"\"\"\n    Main extraction method with fallback strategies.\n    Handles retries and circuit breaker logic.\n    \"\"\"\n</code></pre>"},{"location":"modules/api-utils/#circuit-breaker-implementation","title":"Circuit Breaker Implementation","text":"<pre><code>def _check_circuit_breaker(self) -&gt; bool:\n    \"\"\"Check if circuit breaker is open.\"\"\"\n\ndef _update_circuit_breaker(self, success: bool):\n    \"\"\"Update circuit breaker state based on result.\"\"\"\n\ndef _reset_circuit_breaker(self):\n    \"\"\"Reset circuit breaker after timeout.\"\"\"\n</code></pre>"},{"location":"modules/api-utils/#advanced-features","title":"Advanced Features","text":"<ol> <li>Circuit Breaker Pattern:</li> <li>Opens after 3 consecutive failures</li> <li>5-minute timeout before reset</li> <li> <p>Prevents cascading failures</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Minimum 2-second delay between requests</li> <li>Retry-after header parsing</li> <li> <p>Progressive backoff on rate limits</p> </li> <li> <p>Configuration Options:    <code>python    {        'asp': True,           # Anti-bot bypass        'country': 'US',       # Geo-location        'rendering_wait': 3000, # JS wait time        'retry': False,        # Internal retry        'cache': True,         # Response caching        'debug': True          # Debug info    }</code></p> </li> </ol>"},{"location":"modules/api-utils/#error-handling-sophistication","title":"Error Handling Sophistication","text":"<ol> <li>Rate Limit Detection:    ```python    # Parses multiple rate limit formats</li> <li>\"Request rate limit exceeded (2/sec)\"</li> <li>\"API rate limit exceeded\"</li> <li> <p>Retry-After headers    ```</p> </li> <li> <p>Progressive Degradation:</p> </li> <li> <p>Standard request \u2192 ASP mode \u2192 Country change \u2192 Circuit break</p> </li> <li> <p>Credit Monitoring:</p> </li> <li>Tracks usage in response headers</li> <li>Logs credit consumption</li> <li>Warns on low credits</li> </ol>"},{"location":"modules/api-utils/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Success Rate: ~99.9% with ASP enabled</li> <li>Average Response Time: 3-5 seconds</li> <li>Circuit Breaker Recovery: 5 minutes</li> <li>Rate Limit Compliance: Automatic</li> </ul>"},{"location":"modules/core-utils/","title":"Core Utilities Documentation","text":""},{"location":"modules/core-utils/#module-srcutilsdatabasepy","title":"Module: <code>src/utils/database.py</code>","text":""},{"location":"modules/core-utils/#purpose","title":"Purpose","text":"<p>The Database module provides a comprehensive interface to the Supabase PostgreSQL database, managing all data persistence for the clip tracking system. It implements intelligent retry logic, workflow management, and provides analytics capabilities while maintaining a singleton pattern for efficient connection management.</p>"},{"location":"modules/core-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/core-utils/#data-classes","title":"Data Classes","text":"<pre><code>@dataclass\nclass ProcessingRun:\n    \"\"\"\n    Tracks batch processing metadata.\n    Attributes: id, started_at, completed_at, total_loans, \n                successful_clips, failed_clips\n    \"\"\"\n\n@dataclass  \nclass ClipData:\n    \"\"\"\n    Comprehensive clip data model.\n    Includes: loan info, content, sentiment analysis, workflow status,\n              UI display data, media outlet mapping\n    \"\"\"\n</code></pre>"},{"location":"modules/core-utils/#databasemanager-class","title":"DatabaseManager Class","text":"<pre><code>class DatabaseManager:\n    \"\"\"\n    Singleton database manager for all Supabase operations.\n    Groups operations by functionality: runs, clips, retry logic, analytics.\n    \"\"\"\n\n    def __init__(self, supabase_url: str, supabase_key: str):\n        \"\"\"Initialize Supabase client with credentials.\"\"\"\n</code></pre>"},{"location":"modules/core-utils/#core-operations","title":"Core Operations","text":""},{"location":"modules/core-utils/#processing-runs","title":"Processing Runs","text":"<pre><code>def create_processing_run(self) -&gt; str:\n    \"\"\"Create new processing run, returns UUID.\"\"\"\n\ndef update_processing_run(self, run_id: str, total_loans: int, \n                         successful: int, failed: int):\n    \"\"\"Update run statistics upon completion.\"\"\"\n</code></pre>"},{"location":"modules/core-utils/#clip-management","title":"Clip Management","text":"<pre><code>def store_clip(self, clip_data: ClipData) -&gt; bool:\n    \"\"\"Store found clip with all metadata.\"\"\"\n\ndef get_pending_clips(self, filters: Dict = None) -&gt; List[Dict]:\n    \"\"\"Retrieve clips awaiting review with optional filters.\"\"\"\n\ndef update_clip_status(self, wo_number: str, status: str) -&gt; bool:\n    \"\"\"Update clip workflow status (pending/approved/rejected).\"\"\"\n\ndef update_clip_sentiment(self, wo_number: str, sentiment_data: Dict) -&gt; bool:\n    \"\"\"Store comprehensive GPT sentiment analysis.\"\"\"\n</code></pre>"},{"location":"modules/core-utils/#smart-retry-logic","title":"Smart Retry Logic","text":"<pre><code>def store_failed_attempt(self, loan_data: Dict, reason: str, \n                        run_id: str) -&gt; bool:\n    \"\"\"Record failed search attempt with retry scheduling.\"\"\"\n\ndef should_retry_wo(self, wo_number: str) -&gt; Tuple[bool, Optional[str]]:\n    \"\"\"\n    Intelligent retry decision based on:\n    - Failure type and count\n    - Time since last attempt\n    - Configured retry intervals\n    \"\"\"\n\ndef get_smart_retry_summary(self) -&gt; Dict:\n    \"\"\"Analytics on retry patterns and success rates.\"\"\"\n</code></pre>"},{"location":"modules/core-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/core-utils/#inputs","title":"Inputs","text":"<ol> <li> <p>Environment Configuration:    <code>bash    SUPABASE_URL=https://xxx.supabase.co    SUPABASE_ANON_KEY=eyJhbGc...</code></p> </li> <li> <p>Clip Data Structure:    <code>python    ClipData(        wo_number=\"WO12345\",        model=\"Honda Accord\",        clip_url=\"https://...\",        content=\"Article text...\",        relevance_score=85,        sentiment_data={...},        status=\"pending\",        run_id=\"uuid-123\",        media_outlet=\"Car and Driver\"    )</code></p> </li> </ol>"},{"location":"modules/core-utils/#outputs","title":"Outputs","text":"<ol> <li>Database Records: Direct storage to Supabase tables</li> <li>Query Results: Lists of dictionaries with clip data</li> <li>Analytics: Aggregated statistics and summaries</li> <li>Retry Decisions: Tuple of (should_retry: bool, reason: str)</li> </ol>"},{"location":"modules/core-utils/#dependencies","title":"Dependencies","text":"<pre><code># External\nfrom supabase import create_client, Client\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Internal\nfrom src.utils.logger import logger\n</code></pre>"},{"location":"modules/core-utils/#retry-strategy","title":"Retry Strategy","text":""},{"location":"modules/core-utils/#retry-intervals-by-failure-type","title":"Retry Intervals by Failure Type","text":"<ul> <li>No Content Found: 7 days</li> <li>Technical Issues: 1 day  </li> <li>Timeout: 2 days</li> <li>Access Denied: 14 days</li> <li>Default: 3 days</li> </ul>"},{"location":"modules/core-utils/#retry-decision-logic","title":"Retry Decision Logic","text":"<ol> <li>Check last attempt timestamp</li> <li>Apply interval based on failure reason</li> <li>Consider failure count (max retries)</li> <li>Return decision with explanation</li> </ol>"},{"location":"modules/core-utils/#error-handling","title":"Error Handling","text":"<ul> <li>All operations wrapped in try-except</li> <li>Detailed error logging with context</li> <li>Graceful degradation (returns empty/False)</li> <li>Connection testing available</li> <li>No exceptions propagated to caller</li> </ul>"},{"location":"modules/core-utils/#module-srcutilsconfigpy","title":"Module: <code>src/utils/config.py</code>","text":""},{"location":"modules/core-utils/#purpose_1","title":"Purpose","text":"<p>The Config module centralizes all configuration constants for the web crawling system, including domain-specific strategies, content discovery patterns, and API configurations. It provides a single source of truth for all scraping-related settings.</p>"},{"location":"modules/core-utils/#key-configuration-sections","title":"Key Configuration Sections","text":""},{"location":"modules/core-utils/#domain-management","title":"Domain Management","text":"<pre><code>API_SCRAPER_DOMAINS = [\n    'motortrend.com',\n    'caranddriver.com',\n    'autoblog.com',\n    # ... domains requiring API scraping\n]\n\nGENERIC_INDEX_PATTERNS = [\n    r'/category/',\n    r'/tag/',\n    r'/page/\\d+',\n    # ... patterns indicating index pages\n]\n</code></pre>"},{"location":"modules/core-utils/#content-discovery","title":"Content Discovery","text":"<pre><code>ARTICLE_INDICATORS = ['review', 'test', 'drive', 'first-look']\n\nMODEL_CLEANUP_PATTERNS = {\n    r'\\s+hybrid$': '',\n    r'\\s+phev$': '',\n    r'^20\\d{2}\\s+': '',\n    # ... model name normalization\n}\n\nSEARCH_QUERY_TEMPLATES = [\n    '\"{journalist}\" {make} {model} review',\n    'site:{domain} {make} {model}',\n    # ... search query formats\n]\n</code></pre>"},{"location":"modules/core-utils/#api-configurations","title":"API Configurations","text":"<pre><code>SCRAPINGBEE_CONFIG = {\n    'render_js': True,\n    'premium_proxy': True,\n    'country_code': 'us',\n    'timeout': 30000\n}\n\nGOOGLE_SEARCH_CONFIG = {\n    'max_results': 10,\n    'safe_search': 'off',\n    'search_type': 'web'\n}\n\nCACHE_CONFIG = {\n    'enabled': True,\n    'ttl_hours': 168,  # 7 days\n    'max_size_mb': 500\n}\n</code></pre>"},{"location":"modules/core-utils/#crawler-configuration","title":"Crawler Configuration","text":"<pre><code>CRAWLER_TIERS = {\n    'tier1': {'delay': 1, 'timeout': 30},\n    'tier2': {'delay': 2, 'timeout': 45},\n    'tier3': {'delay': 3, 'timeout': 60},\n    'tier4': {'premium_proxy': True},\n    'tier5': {'js_scenario': True},\n    'tier6': {'stealth_mode': True}\n}\n\nUSER_AGENTS = {\n    'chrome': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n    'firefox': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15)...',\n    'mobile': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1)...'\n}\n</code></pre>"},{"location":"modules/core-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"modules/core-utils/#inputs_1","title":"Inputs","text":"<ul> <li>No inputs (pure configuration module)</li> <li>Values are hardcoded constants</li> </ul>"},{"location":"modules/core-utils/#outputs_1","title":"Outputs","text":"<ul> <li>Configuration dictionaries and lists</li> <li>Used via imports: <code>from src.utils.config import API_SCRAPER_DOMAINS</code></li> </ul>"},{"location":"modules/core-utils/#dependencies_1","title":"Dependencies","text":"<p>None (pure Python module)</p>"},{"location":"modules/core-utils/#usage-patterns","title":"Usage Patterns","text":"<pre><code>from src.utils.config import CRAWLER_TIERS, SCRAPINGBEE_CONFIG\n\n# Use tier configuration\ntier_config = CRAWLER_TIERS['tier3']\ndelay = tier_config['delay']\n\n# Use API configuration\napi_params = SCRAPINGBEE_CONFIG.copy()\napi_params['url'] = target_url\n</code></pre>"},{"location":"modules/core-utils/#module-srcutilsloggerpy","title":"Module: <code>src/utils/logger.py</code>","text":""},{"location":"modules/core-utils/#purpose_2","title":"Purpose","text":"<p>The Logger module provides standardized logging configuration across the entire application. It ensures consistent log formatting, creates necessary directory structures, and provides both console and file output for debugging and monitoring.</p>"},{"location":"modules/core-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"modules/core-utils/#setup-function","title":"Setup Function","text":"<pre><code>def setup_logger(name: str, level=logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Create a configured logger instance.\n\n    Args:\n        name: Logger name (typically __name__)\n        level: Logging level (default: INFO)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n</code></pre>"},{"location":"modules/core-utils/#default-logger-instance","title":"Default Logger Instance","text":"<pre><code># Pre-configured logger for immediate use\nlogger = setup_logger('clip_tracking')\n</code></pre>"},{"location":"modules/core-utils/#expected-inputsoutputs_2","title":"Expected Inputs/Outputs","text":""},{"location":"modules/core-utils/#inputs_2","title":"Inputs","text":"<ol> <li>Logger Name: Module name or custom identifier</li> <li>Log Level: <code>logging.DEBUG</code>, <code>logging.INFO</code>, <code>logging.WARNING</code>, etc.</li> </ol>"},{"location":"modules/core-utils/#outputs_2","title":"Outputs","text":"<ol> <li>Console Output: Formatted log messages to stdout</li> <li>File Output: Logs written to <code>logs/app.log</code></li> <li>Logger Instance: Configured Python logger object</li> </ol>"},{"location":"modules/core-utils/#dependencies_2","title":"Dependencies","text":"<pre><code>import logging\nimport os\nfrom pathlib import Path\n</code></pre>"},{"location":"modules/core-utils/#configuration-details","title":"Configuration Details","text":""},{"location":"modules/core-utils/#log-format","title":"Log Format","text":"<pre><code>%(asctime)s - %(name)s - %(levelname)s - %(message)s\n</code></pre> <p>Example output:</p> <pre><code>2024-01-20 10:30:45 - clip_tracking - INFO - Processing started\n2024-01-20 10:30:46 - database - ERROR - Connection failed: timeout\n</code></pre>"},{"location":"modules/core-utils/#file-structure","title":"File Structure","text":"<pre><code>project_root/\n\u251c\u2500\u2500 logs/\n\u2502   \u2514\u2500\u2500 app.log\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 utils/\n        \u2514\u2500\u2500 logger.py\n</code></pre>"},{"location":"modules/core-utils/#usage-patterns_1","title":"Usage Patterns","text":""},{"location":"modules/core-utils/#module-specific-logger","title":"Module-Specific Logger","text":"<pre><code>from src.utils.logger import setup_logger\n\n# Create module-specific logger\nlogger = setup_logger(__name__)\n\n# Use in module\nlogger.info(\"Starting processing\")\nlogger.error(f\"Failed to process: {error}\")\nlogger.debug(f\"Debug info: {data}\")\n</code></pre>"},{"location":"modules/core-utils/#direct-import","title":"Direct Import","text":"<pre><code>from src.utils.logger import logger\n\n# Use pre-configured logger\nlogger.info(\"Quick logging without setup\")\n</code></pre>"},{"location":"modules/core-utils/#error-handling_1","title":"Error Handling","text":"<ul> <li>Creates log directory if missing</li> <li>Prevents duplicate handlers</li> <li>Handles file permission issues gracefully</li> <li>Falls back to console only if file logging fails</li> </ul>"},{"location":"modules/core-utils/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Single file handler shared across loggers</li> <li>Automatic log rotation not implemented (consider for production)</li> <li>No async logging (synchronous writes)</li> <li>Log level filtering at handler level</li> </ul>"},{"location":"modules/crawler-utils/","title":"Crawler Utilities Documentation","text":""},{"location":"modules/crawler-utils/#module-srcutilsenhanced_crawler_managerpy","title":"Module: <code>src/utils/enhanced_crawler_manager.py</code>","text":""},{"location":"modules/crawler-utils/#purpose","title":"Purpose","text":"<p>The Enhanced Crawler Manager implements a sophisticated 6+ tier escalation system for web content extraction. It provides intelligent content quality detection, caching, index page discovery, and seamless integration with multiple scraping services. This is the primary crawler orchestrator used in production.</p>"},{"location":"modules/crawler-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/crawler-utils/#enhancedcrawlermanager-class","title":"EnhancedCrawlerManager Class","text":"<pre><code>class EnhancedCrawlerManager:\n    \"\"\"\n    Advanced web content extraction with multi-tier escalation.\n    Features caching, quality checks, and intelligent routing.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with cache manager and API clients.\"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#core-methods","title":"Core Methods","text":"<pre><code>def extract_content(self, url: str, expected_make: str = None, \n                   expected_model: str = None, journalist_name: str = None,\n                   publication_date: str = None, max_tier: int = 7) -&gt; Dict:\n    \"\"\"\n    Main extraction method with progressive escalation.\n    Returns: {content, source, tier_used, byline_author, attribution_strength}\n    \"\"\"\n\ndef is_content_high_quality(self, content: str, url: str,\n                           expected_make: str = None, \n                           expected_model: str = None) -&gt; bool:\n    \"\"\"\n    Validates content quality using multiple heuristics.\n    Detects index pages and validates relevance.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#tier-implementation-methods","title":"Tier Implementation Methods","text":"<pre><code>def try_tier_1_basic_http(self, url: str) -&gt; Optional[str]:\n    \"\"\"Basic HTTP request with minimal headers.\"\"\"\n\ndef try_tier_2_enhanced_http(self, url: str) -&gt; Optional[str]:\n    \"\"\"Enhanced HTTP with browser-like headers and cookies.\"\"\"\n\ndef try_tier_3_rss_feed(self, url: str, expected_make: str, \n                       expected_model: str) -&gt; Optional[str]:\n    \"\"\"Extract content via RSS feed if available.\"\"\"\n\ndef try_tier_4_scrapfly(self, url: str) -&gt; Optional[str]:\n    \"\"\"Premium scraping via ScrapFly API.\"\"\"\n\ndef try_tier_5_5_index_page_discovery(self, url: str, expected_make: str,\n                                     expected_model: str) -&gt; Optional[str]:\n    \"\"\"Discover article from index page.\"\"\"\n\ndef try_tier_6_google_search(self, url: str, expected_make: str,\n                            expected_model: str, journalist_name: str) -&gt; Optional[str]:\n    \"\"\"Find article via Google Search.\"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#tiered-escalation-strategy","title":"Tiered Escalation Strategy","text":"<ol> <li>Tier 1 - Basic HTTP (0.5-1s)</li> <li>Simple requests with standard headers</li> <li> <p>Fastest, works for ~40% of sites</p> </li> <li> <p>Tier 2 - Enhanced HTTP (1-2s)</p> </li> <li>Browser-like headers and session management</li> <li>Content quality validation</li> <li> <p>Works for ~60% of sites</p> </li> <li> <p>Tier 3 - RSS Feed (1-2s)</p> </li> <li>Structured data extraction</li> <li>Free and reliable when available</li> <li> <p>~10% of sites support RSS</p> </li> <li> <p>Tier 4 - ScrapFly API (2-5s)</p> </li> <li>Premium residential proxies</li> <li>Handles anti-bot protection</li> <li> <p>99.9% success rate</p> </li> <li> <p>Tier 5.5 - Index Discovery (5-10s)</p> </li> <li>Crawls category/index pages</li> <li>Finds specific article links</li> <li> <p>Similar to YouTube processing</p> </li> <li> <p>Tier 6 - Google Search (3-5s)</p> </li> <li>Searches for specific article</li> <li>Uses journalist + make/model</li> <li> <p>Fallback discovery method</p> </li> <li> <p>Tier 7 - Playwright (10-20s)</p> </li> <li>Full browser automation</li> <li>Last resort option</li> <li>Handles complex JS sites</li> </ol>"},{"location":"modules/crawler-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/crawler-utils/#inputs","title":"Inputs","text":"<pre><code>{\n    'url': 'https://example.com/article',\n    'expected_make': 'Honda',\n    'expected_model': 'Accord',\n    'journalist_name': 'John Doe',\n    'publication_date': '2024-01-15',\n    'max_tier': 7  # Stop at this tier\n}\n</code></pre>"},{"location":"modules/crawler-utils/#outputs","title":"Outputs","text":"<pre><code>{\n    'content': 'Extracted article text...',\n    'source': 'tier_2_enhanced_http',\n    'tier_used': 2,\n    'byline_author': 'John Doe',\n    'attribution_strength': 'strong',\n    'cached': False,\n    'extraction_time': 1.5\n}\n</code></pre>"},{"location":"modules/crawler-utils/#dependencies","title":"Dependencies","text":"<pre><code># External\nimport requests\nfrom bs4 import BeautifulSoup\nimport feedparser\n\n# Internal  \nfrom src.utils.cache_manager import CacheManager\nfrom src.utils.content_extractor import ContentExtractor\nfrom src.utils.scrapfly_client import ScrapFlyClient\nfrom src.utils.google_search import search_google\nfrom src.utils.browser_crawler import BrowserCrawler\n</code></pre>"},{"location":"modules/crawler-utils/#module-srcutilscontent_extractorpy","title":"Module: <code>src/utils/content_extractor.py</code>","text":""},{"location":"modules/crawler-utils/#purpose_1","title":"Purpose","text":"<p>The Content Extractor provides intelligent HTML content extraction with site-specific handlers and multiple fallback strategies. It focuses on extracting clean, relevant article text while filtering out navigation, ads, and other non-content elements.</p>"},{"location":"modules/crawler-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"modules/crawler-utils/#contentextractor-class","title":"ContentExtractor Class","text":"<pre><code>class ContentExtractor:\n    \"\"\"\n    Intelligent content extraction from HTML.\n    Implements site-specific and generic extraction methods.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#core-methods_1","title":"Core Methods","text":"<pre><code>def extract_article_content(html_content: str, url: str = None,\n                          expected_topic: str = None,\n                          extraction_type: str = \"default\") -&gt; Optional[str]:\n    \"\"\"\n    Main extraction method with multiple strategies.\n    Handles site-specific extractors and fallbacks.\n    \"\"\"\n\ndef extract_basic_content(soup: BeautifulSoup) -&gt; str:\n    \"\"\"\n    Generic content extraction using common selectors.\n    Tries multiple article body indicators.\n    \"\"\"\n\ndef extract_alternative_methods(soup: BeautifulSoup, url: str,\n                              expected_topic: str) -&gt; Optional[str]:\n    \"\"\"\n    Alternative extraction when basic method fails.\n    Includes title-based, density-based, and full-text methods.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#site-specific-handlers","title":"Site-Specific Handlers","text":"<pre><code>def extract_fliphtml5_content(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract from FlipHTML5 embedded viewers.\"\"\"\n\ndef extract_spotlightepnews_content(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Handle PDF viewers and flipbook format.\"\"\"\n\ndef extract_thegentlemanracer_content(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Remove sidebar content and extract main article.\"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#content-extraction-strategies","title":"Content Extraction Strategies","text":"<ol> <li>Site-Specific Extraction</li> <li>Custom handlers for known problematic sites</li> <li> <p>Handles embedded viewers, PDFs, flipbooks</p> </li> <li> <p>Basic Extraction</p> </li> <li>Common article selectors (article, .content, etc.)</li> <li>Paragraph concatenation</li> <li> <p>Clean text output</p> </li> <li> <p>Alternative Methods</p> </li> <li>Title-based discovery</li> <li>Paragraph density analysis  </li> <li>Longest text block</li> <li> <p>Full text with filtering</p> </li> <li> <p>Content Quality Scoring</p> </li> <li>Paragraph count and structure</li> <li>Text length validation</li> <li>Navigation element detection</li> <li>Article indicator presence</li> </ol>"},{"location":"modules/crawler-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"modules/crawler-utils/#inputs_1","title":"Inputs","text":"<pre><code>{\n    'html_content': '&lt;html&gt;...&lt;/html&gt;',\n    'url': 'https://example.com/article',\n    'expected_topic': 'Honda Accord',\n    'extraction_type': 'default'  # or 'basic', 'alternative'\n}\n</code></pre>"},{"location":"modules/crawler-utils/#outputs_1","title":"Outputs","text":"<ul> <li>Clean article text (string)</li> <li>None if extraction fails</li> <li>Filtered content without navigation/ads</li> </ul>"},{"location":"modules/crawler-utils/#module-srcutilsbrowser_crawlerpy","title":"Module: <code>src/utils/browser_crawler.py</code>","text":""},{"location":"modules/crawler-utils/#purpose_2","title":"Purpose","text":"<p>The Browser Crawler provides thread-safe headless browser automation using Playwright. It implements anti-detection measures and handles JavaScript-heavy sites that require full browser rendering.</p>"},{"location":"modules/crawler-utils/#key-functionsclasses_2","title":"Key Functions/Classes","text":""},{"location":"modules/crawler-utils/#browsercrawler-class","title":"BrowserCrawler Class","text":"<pre><code>class BrowserCrawler:\n    \"\"\"\n    Thread-safe Playwright browser automation.\n    Creates fresh browser instance per crawl.\n    \"\"\"\n\n    def crawl(self, url: str, wait_for: str = \"networkidle\",\n             wait_time: int = 5, scroll: bool = True) -&gt; str:\n        \"\"\"\n        Crawl URL with headless browser.\n        Returns page HTML after rendering.\n        \"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#mockbrowsercrawler-class","title":"MockBrowserCrawler Class","text":"<pre><code>class MockBrowserCrawler:\n    \"\"\"\n    Mock implementation for testing.\n    Returns simple HTML without browser overhead.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#browser-configuration","title":"Browser Configuration","text":"<ol> <li>Stealth Features</li> <li>Hides webdriver properties</li> <li>Randomized viewport sizes</li> <li> <p>Realistic user agent strings</p> </li> <li> <p>Navigation Strategies</p> </li> <li><code>networkidle</code>: Wait for network quiet</li> <li><code>domcontentloaded</code>: Wait for DOM ready</li> <li> <p><code>load</code>: Wait for page load event</p> </li> <li> <p>Resource Management</p> </li> <li>Fresh browser per crawl</li> <li>Automatic cleanup</li> <li>Thread-safe operation</li> </ol>"},{"location":"modules/crawler-utils/#expected-inputsoutputs_2","title":"Expected Inputs/Outputs","text":""},{"location":"modules/crawler-utils/#inputs_2","title":"Inputs","text":"<pre><code>{\n    'url': 'https://example.com',\n    'wait_for': 'networkidle',\n    'wait_time': 5,\n    'scroll': True\n}\n</code></pre>"},{"location":"modules/crawler-utils/#outputs_2","title":"Outputs","text":"<ul> <li>Rendered HTML content (string)</li> <li>Empty string on failure</li> </ul>"},{"location":"modules/crawler-utils/#module-srcutilsdate_extractorpy","title":"Module: <code>src/utils/date_extractor.py</code>","text":""},{"location":"modules/crawler-utils/#purpose_3","title":"Purpose","text":"<p>The Date Extractor provides comprehensive publication date extraction from web content using multiple methods including structured data, meta tags, CSS selectors, and text patterns.</p>"},{"location":"modules/crawler-utils/#key-functionsclasses_3","title":"Key Functions/Classes","text":""},{"location":"modules/crawler-utils/#core-functions","title":"Core Functions","text":"<pre><code>def extract_published_date(html_content: str, url: str = None) -&gt; Optional[str]:\n    \"\"\"\n    Extract publication date using multiple strategies.\n    Returns ISO format date string or None.\n    \"\"\"\n\ndef extract_date_from_structured_data(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract from JSON-LD, microdata schemas.\"\"\"\n\ndef extract_date_from_meta_tags(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract from Open Graph, Dublin Core meta tags.\"\"\"\n\ndef extract_date_from_selectors(soup: BeautifulSoup) -&gt; Optional[str]:\n    \"\"\"Extract using common CSS date selectors.\"\"\"\n\ndef extract_date_from_text_patterns(text: str) -&gt; Optional[str]:\n    \"\"\"Extract using regex patterns for date formats.\"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#extraction-methods-hierarchy","title":"Extraction Methods Hierarchy","text":"<ol> <li>Structured Data (Most reliable)</li> <li>JSON-LD schemas</li> <li>Microdata markup</li> <li> <p>Schema.org properties</p> </li> <li> <p>Meta Tags</p> </li> <li>Open Graph (og:published_time)</li> <li>Dublin Core (DC.date)</li> <li> <p>Article metadata</p> </li> <li> <p>CSS Selectors</p> </li> <li>Common date classes (.date, .publish-date)</li> <li>Time elements</li> <li> <p>Site-specific patterns</p> </li> <li> <p>Text Patterns (Fallback)</p> </li> <li>Regex for various date formats</li> <li>Natural language parsing</li> <li>Sanity validation</li> </ol>"},{"location":"modules/crawler-utils/#expected-inputsoutputs_3","title":"Expected Inputs/Outputs","text":""},{"location":"modules/crawler-utils/#inputs_3","title":"Inputs","text":"<pre><code>{\n    'html_content': '&lt;html&gt;...&lt;/html&gt;',\n    'url': 'https://example.com/article'\n}\n</code></pre>"},{"location":"modules/crawler-utils/#outputs_3","title":"Outputs","text":"<ul> <li>ISO format date string: \"2024-01-15\"</li> <li>None if no valid date found</li> </ul>"},{"location":"modules/crawler-utils/#module-srcutilsescalationpy","title":"Module: <code>src/utils/escalation.py</code>","text":""},{"location":"modules/crawler-utils/#purpose_4","title":"Purpose","text":"<p>The Escalation module manages domain-specific crawling strategies, determining the appropriate starting tier and escalation path based on site characteristics and configuration.</p>"},{"location":"modules/crawler-utils/#key-functionsclasses_4","title":"Key Functions/Classes","text":""},{"location":"modules/crawler-utils/#core-functions_1","title":"Core Functions","text":"<pre><code>def load_media_sources() -&gt; pd.DataFrame:\n    \"\"\"\n    Load media source configuration from CSV.\n    Contains domain-specific crawl strategies.\n    \"\"\"\n\ndef get_domain_crawl_level(domain: str, media_sources_df: pd.DataFrame) -&gt; int:\n    \"\"\"\n    Determine starting crawl level for domain.\n    Based on js_mode and other indicators.\n    \"\"\"\n\ndef requires_js_rendering(domain: str) -&gt; bool:\n    \"\"\"\n    Check if domain requires JavaScript rendering.\n    Uses configuration and heuristics.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler-utils/#configuration-management","title":"Configuration Management","text":"<ol> <li> <p>Media Sources CSV <code>csv    domain,js_mode,start_tier    example.com,true,4    simple.com,false,1</code></p> </li> <li> <p>Domain Classification</p> </li> <li>JavaScript-heavy sites</li> <li>Simple HTML sites</li> <li>API-required sites</li> <li> <p>Premium proxy sites</p> </li> <li> <p>Escalation Rules</p> </li> <li>Start tier selection</li> <li>Max tier limits</li> <li>Skip tier options</li> </ol>"},{"location":"modules/crawler-utils/#expected-inputsoutputs_4","title":"Expected Inputs/Outputs","text":""},{"location":"modules/crawler-utils/#inputs_4","title":"Inputs","text":"<ul> <li>Domain name (string)</li> <li>Media sources configuration</li> </ul>"},{"location":"modules/crawler-utils/#outputs_4","title":"Outputs","text":"<ul> <li>Starting crawl level (integer)</li> <li>JavaScript requirement (boolean)</li> <li>Escalation strategy parameters</li> </ul>"},{"location":"modules/crawler-utils/#integration-with-crawler-manager","title":"Integration with Crawler Manager","text":"<p>The escalation module provides the intelligence for: - Selecting optimal starting tier - Avoiding unnecessary escalation - Domain-specific optimizations - Performance tuning per site</p>"},{"location":"modules/crawler/","title":"Crawler Module Documentation","text":""},{"location":"modules/crawler/#module-srccrawlercrawlerspidersloan_spiderpy","title":"Module: <code>src/crawler/crawler/spiders/loan_spider.py</code>","text":""},{"location":"modules/crawler/#purpose","title":"Purpose","text":"<p>The Crawler module implements a Scrapy-based spider for discovering and extracting automotive media content from web pages. It provides intelligent content discovery with multi-level crawling capabilities, automatically following relevant links when initial pages don't contain vehicle-specific content. The spider is designed as the foundation layer for the tiered web scraping strategy, focusing on standard HTML extraction before escalation to more advanced techniques.</p>"},{"location":"modules/crawler/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/crawler/#loanspider-class","title":"LoanSpider Class","text":"<pre><code>class LoanSpider(scrapy.Spider):\n    \"\"\"\n    Scrapy spider for crawling automotive media sites.\n    Implements smart discovery and multi-level crawling.\n    \"\"\"\n\n    name = 'loan_spider'\n    allowed_domains = []  # Set dynamically from loan URLs\n    start_urls = []      # Set dynamically from loan data\n</code></pre>"},{"location":"modules/crawler/#initialization","title":"Initialization","text":"<pre><code>def __init__(self, loans_data: List[Dict[str, Any]] = None, *args, **kwargs):\n    \"\"\"\n    Initialize spider with loan data.\n    Dynamically generates allowed domains from URLs.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler/#request-generation","title":"Request Generation","text":"<pre><code>def start_requests(self) -&gt; Generator[Request, None, None]:\n    \"\"\"\n    Generate initial requests from loans data.\n    Attaches loan metadata to each request.\n    \"\"\"\n\ndef _discover_relevant_pages(self, response: Response, \n                           loan_data: Dict[str, Any]) -&gt; Generator[Request, None, None]:\n    \"\"\"\n    Discover and follow links that might contain relevant content.\n    Implements smart discovery for review/blog/news sections.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler/#content-processing","title":"Content Processing","text":"<pre><code>def parse(self, response: Response) -&gt; Generator[LoanItem, None, None]:\n    \"\"\"\n    Main parsing function that extracts content.\n    Implements two-level crawling strategy.\n    \"\"\"\n\ndef handle_error(self, failure):\n    \"\"\"\n    Handle request failures gracefully.\n    Creates error items for tracking.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler/#extraction-functions","title":"Extraction Functions","text":"<pre><code>def _extract_title(self, response: Response) -&gt; str:\n    \"\"\"\n    Extract page title using multiple strategies.\n    Falls back from &lt;title&gt; to &lt;h1&gt; tags.\n    \"\"\"\n\ndef _extract_content(self, response: Response, make: str, model: str) -&gt; str:\n    \"\"\"\n    Extract main content using common article selectors.\n    Progressive fallback from specific to generic selectors.\n    \"\"\"\n\ndef _extract_date(self, response: Response) -&gt; Optional[str]:\n    \"\"\"\n    Extract publication date from meta tags or HTML elements.\n    Checks multiple date formats and locations.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler/#utility-functions","title":"Utility Functions","text":"<pre><code>def _content_mentions_vehicle(self, content: str, make: str, model: str) -&gt; bool:\n    \"\"\"\n    Check if content mentions the vehicle make and model.\n    Case-insensitive matching.\n    \"\"\"\n\ndef _extract_domain(self, url: str) -&gt; Optional[str]:\n    \"\"\"\n    Extract domain from URL for allowed_domains.\n    \"\"\"\n\ndef _is_media_file(self, url: str) -&gt; bool:\n    \"\"\"\n    Check if URL points to a media file.\n    Filters out images, videos, PDFs.\n    \"\"\"\n</code></pre>"},{"location":"modules/crawler/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/crawler/#inputs","title":"Inputs","text":"<ol> <li> <p>Loan Data Structure:    <code>python    {        'work_order': 'WO12345',        'make': 'Honda',        'model': 'Accord',        'source': 'Car and Driver',        'urls': [            'https://caranddriver.com/reviews/...',            'https://motortrend.com/...'        ]    }</code></p> </li> <li> <p>Spider Configuration:</p> </li> <li>Crawl levels: 1 (direct URL), 2 (discovered links)</li> <li>Max discovered links: 5 per page</li> <li>Follow patterns: review, test-drive, road-test, etc.</li> </ol>"},{"location":"modules/crawler/#outputs","title":"Outputs","text":"<ol> <li>LoanItem Structure:    <code>python    LoanItem(        work_order='WO12345',        make='Honda',        model='Accord',        source='Car and Driver',        url='https://...',        content='Extracted article text...',        title='2024 Honda Accord Review',        publication_date='2024-01-15',        content_type='article',  # or 'error'        crawl_date='2024-01-20T10:30:00',        crawl_level=1,  # 1 or 2        error=None  # Error message if failed    )</code></li> </ol>"},{"location":"modules/crawler/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport scrapy\nfrom scrapy.http import Response, Request\n\n# Standard Library\nimport re\nimport logging\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Generator, Optional\nfrom urllib.parse import urlparse\n\n# Internal Modules\nfrom crawler.items import LoanItem\n</code></pre>"},{"location":"modules/crawler/#content-discovery-strategy","title":"Content Discovery Strategy","text":""},{"location":"modules/crawler/#level-1-crawling-direct-urls","title":"Level 1 Crawling (Direct URLs)","text":"<ol> <li>Fetch the provided URL</li> <li>Extract content using article selectors</li> <li>Check if content mentions vehicle</li> <li>If not relevant, trigger discovery</li> </ol>"},{"location":"modules/crawler/#level-2-crawling-discovery","title":"Level 2 Crawling (Discovery)","text":"<ol> <li>Link Pattern Matching:</li> <li>Vehicle make/model in URL</li> <li>Review keywords: \"review\", \"test-drive\", \"road-test\"</li> <li> <p>Section links: \"/review/\", \"/blog/\", \"/news/\"</p> </li> <li> <p>Discovery Limits:</p> </li> <li>Maximum 5 links per page</li> <li>Excludes media files</li> <li>Deduplicates discovered URLs</li> </ol>"},{"location":"modules/crawler/#content-extraction-hierarchy","title":"Content Extraction Hierarchy","text":"<ol> <li>Article-Specific Selectors:</li> <li><code>article</code></li> <li><code>div.content</code></li> <li><code>div.article-content</code></li> <li><code>div.post-content</code></li> <li><code>div.entry-content</code></li> <li><code>div.main-content</code></li> <li> <p><code>.story</code></p> </li> <li> <p>Fallback Strategies:</p> </li> <li>All <code>&lt;p&gt;</code> tags</li> <li>All text from <code>&lt;body&gt;</code></li> <li>Progressive degradation ensures some content</li> </ol>"},{"location":"modules/crawler/#error-handling","title":"Error Handling","text":"<ol> <li>Request Failures:</li> <li>Logged with full error details</li> <li>Error items created for tracking</li> <li> <p>Preserves loan metadata in error items</p> </li> <li> <p>Extraction Failures:</p> </li> <li>Empty strings returned (not None)</li> <li>Graceful degradation in selectors</li> <li> <p>No exceptions thrown to caller</p> </li> <li> <p>Discovery Failures:</p> </li> <li>Silently skips bad links</li> <li>Continues with remaining URLs</li> <li>Logs domain extraction errors</li> </ol>"},{"location":"modules/crawler/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Concurrent Requests: Controlled by Scrapy settings</li> <li>Domain Filtering: Dynamic allowed_domains prevents sprawl</li> <li>Content Limits: No explicit size limits (handled upstream)</li> <li>Discovery Depth: Limited to 2 levels</li> <li>Link Limits: Max 5 discovered links per page</li> </ul>"},{"location":"modules/crawler/#integration-notes","title":"Integration Notes","text":"<p>This spider is typically not used directly but through: 1. EnhancedCrawlerManager: Orchestrates tiered escalation 2. Scrapy Settings: Configured for 2s delay, 1 concurrent request 3. Item Pipeline: Results processed by Scrapy pipelines</p>"},{"location":"modules/crawler/#limitations","title":"Limitations","text":"<ul> <li>JavaScript Sites: No JS rendering (Level 1 only)</li> <li>Authentication: No login support</li> <li>Cookies: Basic cookie jar only</li> <li>Rate Limiting: Relies on Scrapy delays</li> <li>Content Types: HTML only, no PDF extraction</li> </ul>"},{"location":"modules/creatoriq/","title":"CreatorIQ Module Documentation","text":""},{"location":"modules/creatoriq/#module-srccreatoriq","title":"Module: <code>src/creatoriq/</code>","text":""},{"location":"modules/creatoriq/#purpose","title":"Purpose","text":"<p>The CreatorIQ module provides comprehensive integration with the CreatorIQ influencer marketing platform to extract social media post data from campaigns. It implements multiple authentication methods, data extraction strategies, and export capabilities to reliably capture influencer content URLs and engagement metrics. The module is designed with fallback strategies to ensure data extraction even when API access is limited.</p>"},{"location":"modules/creatoriq/#key-components","title":"Key Components","text":""},{"location":"modules/creatoriq/#graphql-client-graphql_clientpy","title":"GraphQL Client (<code>graphql_client.py</code>)","text":"<pre><code>class CreatorIQClient:\n    \"\"\"\n    Direct API client for CreatorIQ's GraphQL endpoint.\n    Handles authentication, pagination, and data extraction.\n    \"\"\"\n\n    def get_campaign_posts(self, campaign_id: int, limit: int = 1000) -&gt; List[Dict]:\n        \"\"\"\n        Fetches all posts for a campaign using GraphQL queries.\n        Implements cursor-based pagination.\n        \"\"\"\n</code></pre>"},{"location":"modules/creatoriq/#csv-exporter-csv_exporterpy","title":"CSV Exporter (<code>csv_exporter.py</code>)","text":"<pre><code>class CSVExporter:\n    \"\"\"\n    Exports CreatorIQ post data to CSV format.\n    Generates detailed exports and summary statistics.\n    \"\"\"\n\n    def export_to_csv(self, posts: List[Dict], output_path: str, \n                     include_summary: bool = True):\n        \"\"\"\n        Exports posts with cleaned data and optional summary statistics.\n        \"\"\"\n</code></pre>"},{"location":"modules/creatoriq/#authentication-handlers","title":"Authentication Handlers","text":"<ol> <li> <p>Browser Session Auth (<code>auth_headers.py</code>):    <code>python    def get_auth_headers():        \"\"\"        Extracts authentication headers from browser session.        Includes cookies, CSRF tokens, and auth tokens.        \"\"\"</code></p> </li> <li> <p>API Key Auth (<code>api_key_auth.py</code>):    <code>python    class APIKeyClient:        \"\"\"        Cleaner authentication using CreatorIQ API keys.        Preferred method when available.        \"\"\"</code></p> </li> <li> <p>Hybrid Auth (<code>hybrid_auth_client.py</code>):    <code>python    class HybridCreatorIQClient:        \"\"\"        Automatically selects best available authentication method.        Falls back gracefully between API key and browser auth.        \"\"\"</code></p> </li> <li> <p>Public Client (<code>public_client.py</code>):    <code>python    class PublicCreatorIQClient:        \"\"\"        Accesses public/shared reports without authentication.        Uses browser automation for public campaign access.        \"\"\"</code></p> </li> </ol>"},{"location":"modules/creatoriq/#web-scraping-components","title":"Web Scraping Components","text":"<ol> <li> <p>Playwright Scraper (<code>playwright_scraper.py</code>):    <code>python    async def scrape_campaign_with_playwright(url: str, save_responses: bool = True):        \"\"\"        Browser automation with network traffic capture.        Implements infinite scrolling and response saving.        \"\"\"</code></p> </li> <li> <p>Browser Extractor (<code>browser_extractor.py</code>):    <code>python    def extract_posts_from_browser(campaign_url: str):        \"\"\"        Direct DOM extraction from rendered pages.        Scrolls and extracts post data from HTML elements.        \"\"\"</code></p> </li> <li> <p>Parser (<code>parser.py</code>):    <code>python    def extract_urls_from_html(html_content: str) -&gt; List[str]:        \"\"\"        Extracts social media URLs using multiple strategies.        Handles API responses, embedded JSON, and regex patterns.        \"\"\"</code></p> </li> </ol>"},{"location":"modules/creatoriq/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/creatoriq/#inputs","title":"Inputs","text":"<ol> <li> <p>Campaign URL:    <code>https://app.creatoriq.com/campaigns/[CAMPAIGN_ID]/posts</code></p> </li> <li> <p>Authentication Options:</p> </li> <li>API Key: <code>CREATORIQ_API_KEY</code> environment variable</li> <li>Browser Headers: Interactive collection via terminal</li> <li> <p>Public Access: No authentication required</p> </li> <li> <p>Configuration:    <code>python    {        'limit': 1000,  # Max posts to fetch        'include_summary': True,  # Generate statistics        'save_responses': True,  # Save raw API responses        'scroll_delay': 2  # Seconds between scrolls    }</code></p> </li> </ol>"},{"location":"modules/creatoriq/#outputs","title":"Outputs","text":"<ol> <li> <p>Post Data Structure:    <code>json    {        \"url\": \"https://www.instagram.com/p/ABC123/\",        \"creator_name\": \"John Doe\",        \"creator_username\": \"@johndoe\",        \"platform\": \"instagram\",        \"impressions\": 50000,        \"likes\": 2500,        \"comments\": 150,        \"shares\": 75,        \"engagement_rate\": 5.5,        \"caption\": \"Post caption text...\",        \"published_at\": \"2024-01-15T10:30:00Z\",        \"thumbnail_url\": \"https://...\"    }</code></p> </li> <li> <p>CSV Export Files:</p> </li> <li><code>campaign_posts.csv</code>: Detailed post data</li> <li> <p><code>campaign_summary.txt</code>: Platform breakdown and top creators</p> </li> <li> <p>Saved Responses (optional):</p> </li> <li><code>campaign_[ID]_responses.json</code>: Raw API responses</li> <li><code>campaign_[ID]_html.html</code>: Captured page HTML</li> </ol>"},{"location":"modules/creatoriq/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport playwright  # Browser automation\nimport httpx       # HTTP client\nimport pandas      # Data processing\nfrom bs4 import BeautifulSoup  # HTML parsing\n\n# Internal Modules\nfrom src.utils.logger import get_logger\n</code></pre>"},{"location":"modules/creatoriq/#data-extraction-strategies","title":"Data Extraction Strategies","text":""},{"location":"modules/creatoriq/#1-graphql-api-primary","title":"1. GraphQL API (Primary)","text":"<ul> <li>Direct queries to CreatorIQ's GraphQL endpoint</li> <li>Most reliable and complete data</li> <li>Requires authentication</li> <li>Cursor-based pagination</li> </ul>"},{"location":"modules/creatoriq/#2-network-interception","title":"2. Network Interception","text":"<ul> <li>Captures API responses during page load</li> <li>Useful when direct API access fails</li> <li>Extracts from XHR/Fetch responses</li> <li>Requires browser automation</li> </ul>"},{"location":"modules/creatoriq/#3-embedded-json","title":"3. Embedded JSON","text":"<ul> <li>Parses <code>window.__INITIAL_STATE__</code> from page</li> <li>Contains pre-loaded campaign data</li> <li>Fast but may be incomplete</li> <li>No pagination support</li> </ul>"},{"location":"modules/creatoriq/#4-dom-parsing","title":"4. DOM Parsing","text":"<ul> <li>Extracts from rendered HTML elements</li> <li>Last resort when APIs fail</li> <li>Handles dynamic content via scrolling</li> <li>May miss some data fields</li> </ul>"},{"location":"modules/creatoriq/#authentication-flow","title":"Authentication Flow","text":"<pre><code>1. Check for API Key \u2192 Use APIKeyClient\n   \u2193 (if not available)\n2. Check for Browser Headers \u2192 Use BrowserAuthClient\n   \u2193 (if not available)\n3. Check if Public URL \u2192 Use PublicClient\n   \u2193 (if not available)\n4. Prompt for Authentication Method\n</code></pre>"},{"location":"modules/creatoriq/#error-handling","title":"Error Handling","text":"<ol> <li>Authentication Failures:</li> <li>Clear error messages with solutions</li> <li>Automatic fallback to next method</li> <li> <p>Session refresh capabilities</p> </li> <li> <p>Rate Limiting:</p> </li> <li>Configurable delays between requests</li> <li>Exponential backoff on 429 errors</li> <li> <p>Request queuing</p> </li> <li> <p>Data Extraction Failures:</p> </li> <li>Multiple extraction strategies</li> <li>Partial data recovery</li> <li> <p>Detailed error logging</p> </li> <li> <p>Network Issues:</p> </li> <li>Retry logic with backoff</li> <li>Timeout configuration</li> <li>Connection pooling</li> </ol>"},{"location":"modules/creatoriq/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Pagination: Fetches 100 posts per request</li> <li>Scrolling: 2-second delay between scrolls</li> <li>Network Capture: ~10MB per 1000 posts</li> <li>Export Time: ~1 second per 1000 posts</li> <li>Memory Usage: Streaming for large datasets</li> </ul>"},{"location":"modules/creatoriq/#usage-examples","title":"Usage Examples","text":""},{"location":"modules/creatoriq/#command-line","title":"Command Line","text":"<pre><code># Extract campaign URLs\npython -m src.creatoriq.extract_campaign_urls [CAMPAIGN_URL]\n\n# Export to CSV with API key\npython -m src.creatoriq.scrape_campaign_report [CAMPAIGN_URL]\n\n# Interactive browser extraction\npython -m src.creatoriq.scrape_posts_from_browser\n</code></pre>"},{"location":"modules/creatoriq/#python-integration","title":"Python Integration","text":"<pre><code>from src.creatoriq import HybridCreatorIQClient, CSVExporter\n\n# Initialize client\nclient = HybridCreatorIQClient()\n\n# Get campaign posts\nposts = client.get_campaign_posts(campaign_id=12345)\n\n# Export to CSV\nexporter = CSVExporter()\nexporter.export_to_csv(posts, \"output.csv\", include_summary=True)\n</code></pre>"},{"location":"modules/creatoriq/#limitations","title":"Limitations","text":"<ul> <li>Platform Support: Instagram, TikTok, YouTube primarily</li> <li>Historical Data: Limited by CreatorIQ retention</li> <li>Real-time Updates: Requires re-scraping</li> <li>API Rate Limits: Varies by subscription tier</li> <li>Browser Detection: May trigger anti-bot measures</li> </ul>"},{"location":"modules/dashboard/","title":"Dashboard Module Documentation","text":""},{"location":"modules/dashboard/#module-srcdashboardapppy","title":"Module: <code>src/dashboard/app.py</code>","text":""},{"location":"modules/dashboard/#purpose","title":"Purpose","text":"<p>The Dashboard module provides a comprehensive Streamlit-based web interface for the DriveShop Clip Tracking System. It serves as the central hub for reviewing, approving, and managing media clips discovered through automated crawling and analysis. The dashboard enables users to process loans without clips, review AI-analyzed content, manage journalist-outlet relationships, and export formatted reports for stakeholders.</p>"},{"location":"modules/dashboard/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/dashboard/#main-application-function","title":"Main Application Function","text":"<pre><code>def main():\n    \"\"\"\n    Entry point for the Streamlit dashboard application.\n    Handles authentication, UI layout, and orchestrates all dashboard functionality.\n    \"\"\"\n</code></pre>"},{"location":"modules/dashboard/#data-loading-functions","title":"Data Loading Functions","text":"<pre><code>def load_person_outlets_mapping():\n    \"\"\"\n    Loads Person_ID to Media Outlets mapping from JSON file.\n    Returns: Dict mapping Person_ID to list of outlet data with impressions\n    \"\"\"\n\ndef load_loans_data_for_filtering(url: str):\n    \"\"\"\n    Fetches and caches loans data from URL for filtering.\n    Returns: DataFrame with loan records or None if error\n    \"\"\"\n</code></pre>"},{"location":"modules/dashboard/#ui-helper-functions","title":"UI Helper Functions","text":"<pre><code>def apply_custom_sidebar_styling():\n    \"\"\"\n    Applies comprehensive black sidebar CSS styling to the Streamlit app.\n    Includes hover effects, custom colors, and professional appearance.\n    \"\"\"\n\ndef update_progress(current: int, total: int, message: str):\n    \"\"\"\n    Updates processing progress in session state.\n    Used for real-time progress tracking during batch operations.\n    \"\"\"\n\ndef format_time(seconds: float) -&gt; str:\n    \"\"\"\n    Formats seconds into human-readable time string.\n    Example: 125 seconds -&gt; \"2 minutes 5 seconds\"\n    \"\"\"\n</code></pre>"},{"location":"modules/dashboard/#data-processing-functions","title":"Data Processing Functions","text":"<pre><code>def create_reporter_name_to_id_mapping(person_outlets_data: dict) -&gt; dict:\n    \"\"\"\n    Creates mapping of reporter full names to Person_IDs.\n    Handles name variations and duplicates.\n    \"\"\"\n\ndef get_outlet_options_for_person(person_id: str, person_outlets_data: dict) -&gt; list:\n    \"\"\"\n    Returns list of outlet names associated with a Person_ID.\n    Used for dropdown population in the UI.\n    \"\"\"\n\ndef parse_url_tracking(wo_tracking: str) -&gt; dict:\n    \"\"\"\n    Parses URL tracking data from database JSON string.\n    Extracts discovered URLs and their processing status.\n    \"\"\"\n</code></pre>"},{"location":"modules/dashboard/#export-functions","title":"Export Functions","text":"<pre><code>def create_client_excel_report(clips_df: pd.DataFrame, \n                             rejected_df: pd.DataFrame,\n                             selected_office: str,\n                             selected_makes: list,\n                             date_range: tuple) -&gt; BytesIO:\n    \"\"\"\n    Creates professional multi-sheet Excel report.\n    Includes Executive Summary, Detailed Results, Rejected Loans, and Approved Clips.\n    Returns: BytesIO object containing the Excel file\n    \"\"\"\n</code></pre>"},{"location":"modules/dashboard/#database-interaction-functions","title":"Database Interaction Functions","text":"<pre><code>def update_person_outlets_mapping_from_url(url: str):\n    \"\"\"\n    Updates local person-outlets mapping from remote URL.\n    Refreshes both JSON and CSV versions for consistency.\n    \"\"\"\n</code></pre>"},{"location":"modules/dashboard/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/dashboard/#inputs","title":"Inputs","text":"<ol> <li>Loans Data Source:</li> <li>Live URL: <code>https://driveshop.mystagingwebsite.com/reports/media-and-pr/loans-without-clips/download/csv</code></li> <li> <p>File Upload: CSV/XLSX files with columns:</p> <ul> <li><code>Work Order Number</code>, <code>First Name</code>, <code>Last Name</code>, <code>Media Outlet</code></li> <li><code>Model</code>, <code>Start Date</code>, <code>End Date</code>, <code>Office</code>, <code>Make</code></li> </ul> </li> <li> <p>User Interactions:</p> </li> <li>Filter selections (Office, Make, Reporter, Outlet, Date Range)</li> <li>Approval/Rejection checkboxes in AgGrid</li> <li>Media Outlet dropdown selections</li> <li>Byline Author text inputs</li> <li> <p>Sentiment analysis triggers</p> </li> <li> <p>Configuration Files:</p> </li> <li><code>data/person_outlets_mapping.json</code>: Journalist-outlet associations</li> <li>Environment variables via <code>.env</code> file</li> </ol>"},{"location":"modules/dashboard/#outputs","title":"Outputs","text":"<ol> <li>Database Updates:</li> <li>Clip status updates (approved/rejected)</li> <li>UI state persistence</li> <li>Media outlet associations</li> <li> <p>Byline modifications</p> </li> <li> <p>Export Files:</p> </li> <li>Excel Reports (.xlsx):<ul> <li>Executive Summary sheet</li> <li>Detailed Results sheet</li> <li>Rejected Loans sheet</li> <li>Approved Clips sheet (FMS format)</li> </ul> </li> <li> <p>JSON Files: Timestamped approved clips data</p> </li> <li> <p>Visual Feedback:</p> </li> <li>Processing progress bars</li> <li>Success/error notifications</li> <li>Metrics display (total clips, approved, rejected)</li> <li>Real-time grid updates</li> </ol>"},{"location":"modules/dashboard/#dependencies","title":"Dependencies","text":""},{"location":"modules/dashboard/#external-libraries","title":"External Libraries","text":"<pre><code># UI Framework\nimport streamlit as st\nfrom st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode\n\n# Data Processing\nimport pandas as pd\nimport numpy as np\n\n# File Handling\nimport openpyxl\nfrom openpyxl.styles import Font, PatternFill, Alignment\nfrom openpyxl.utils import get_column_letter\n\n# System &amp; Utilities\nimport os\nimport sys\nimport json\nimport io\nimport time\nimport pickle\nimport requests\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Image Processing\nfrom PIL import Image\n</code></pre>"},{"location":"modules/dashboard/#internal-modules","title":"Internal Modules","text":"<pre><code># Database Operations\nfrom src.utils.database import DatabaseManager\n\n# Logging\nfrom src.utils.logger import get_logger\n\n# Processing Pipeline\nfrom src.ingest.ingest_database import (\n    process_loans_batch,\n    check_all_youtube_outlets_in_mapping\n)\n\n# Analysis\nfrom src.utils.sentiment_analysis import run_sentiment_analysis\n\n# CreatorIQ Integration\nfrom src.creatoriq.scrape_campaign_report import scrape_campaign_report\nfrom src.creatoriq.scrape_post_urls import scrape_posts_for_campaign\n</code></pre>"},{"location":"modules/dashboard/#configuration","title":"Configuration","text":""},{"location":"modules/dashboard/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>SUPABASE_URL</code>: Database connection URL</li> <li><code>SUPABASE_KEY</code>: Database authentication key</li> <li><code>STREAMLIT_PASSWORD</code>: Dashboard access password</li> <li><code>DATABASE_PASSWORD</code>: PostgreSQL password</li> </ul>"},{"location":"modules/dashboard/#session-state-management","title":"Session State Management","text":"<pre><code># Tracking Sets\nst.session_state.viewed_records = set()      # Viewed WO numbers\nst.session_state.approved_records = set()    # Approved WO numbers\nst.session_state.rejected_records = set()    # Rejected WO numbers\n\n# Data Storage\nst.session_state.last_saved_outlets = {}     # Media outlet selections\nst.session_state.last_saved_bylines = {}     # Byline author edits\nst.session_state.outlet_data_mapping = {}    # Full outlet data\n\n# Processing State\nst.session_state.processing_progress = {...}  # Progress tracking\nst.session_state.loans_data_loaded = False   # Data load flag\nst.session_state.batch_info = {...}          # Batch processing info\n</code></pre>"},{"location":"modules/dashboard/#error-handling","title":"Error Handling","text":"<ul> <li>Database connection failures handled with user-friendly messages</li> <li>File upload validation with size and format checks</li> <li>API rate limiting with exponential backoff</li> <li>Graceful degradation when external services unavailable</li> <li>Comprehensive logging for debugging</li> </ul>"},{"location":"modules/dashboard/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Caching implemented for expensive operations (<code>@st.cache_data</code>)</li> <li>Batch processing for large datasets</li> <li>Pagination support in AgGrid for large result sets</li> <li>Optimized database queries with proper indexing</li> <li>Lazy loading for media outlet data</li> </ul>"},{"location":"modules/dashboard/#security","title":"Security","text":"<ul> <li>Password protection for dashboard access</li> <li>Environment-based configuration (no hardcoded secrets)</li> <li>SQL injection prevention through parameterized queries</li> <li>XSS protection in user input handling</li> <li>Secure file upload with validation</li> </ul>"},{"location":"modules/ingest/","title":"Ingest Module Documentation","text":""},{"location":"modules/ingest/#module-srcingestingestpy","title":"Module: <code>src/ingest/ingest.py</code>","text":""},{"location":"modules/ingest/#purpose","title":"Purpose","text":"<p>The CSV-based ingest module processes loan data from CSV/Excel files to find and analyze media clips (articles and videos) about loaned vehicles. It implements concurrent processing for efficiency and outputs results to CSV files for dashboard consumption. This module maintains a guaranteed contract for the core <code>process_loan()</code> function to ensure consistent processing.</p>"},{"location":"modules/ingest/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/ingest/#data-loading-functions","title":"Data Loading Functions","text":"<pre><code>def load_loans_data(file_path: str) -&gt; List[Dict]:\n    \"\"\"\n    Loads loan data from CSV or Excel file.\n    Returns: List of dictionaries containing loan information\n    \"\"\"\n\ndef load_loans_data_from_url(url: str) -&gt; List[Dict]:\n    \"\"\"\n    Loads loan data from a URL endpoint.\n    Handles authentication and CSV parsing.\n    Returns: List of loan dictionaries\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#core-processing-functions","title":"Core Processing Functions","text":"<pre><code>def process_loan(loan: dict) -&gt; Optional[dict]:\n    \"\"\"\n    PROTECTED CONTRACT: Processes a single loan to find media clips.\n    Must return dictionary with specific structure or None.\n\n    Contract Requirements:\n    - Must handle both YouTube and web URLs\n    - Must validate content dates\n    - Must return only the best clip per loan\n    - Must include all required fields in output\n    \"\"\"\n\nasync def process_loan_async(loan: dict, semaphore: asyncio.Semaphore) -&gt; Optional[dict]:\n    \"\"\"\n    Async wrapper for process_loan with semaphore control.\n    Enables concurrent processing with rate limiting.\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#content-extraction-functions","title":"Content Extraction Functions","text":"<pre><code>def process_youtube_url(url: str, model: str, journalist_name: str, \n                       start_date: datetime, end_date: datetime) -&gt; Optional[dict]:\n    \"\"\"\n    Extracts content from YouTube videos.\n    Attempts transcript extraction, falls back to metadata.\n    Applies flexible model matching for video titles.\n    \"\"\"\n\ndef process_web_url(url: str, model: str, journalist_name: str,\n                   start_date: datetime, end_date: datetime) -&gt; Optional[dict]:\n    \"\"\"\n    Crawls web articles and extracts content.\n    Uses multi-tier escalation strategy for difficult sites.\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#analysis-functions","title":"Analysis Functions","text":"<pre><code>def analyze_clip(content: str, model: str, journalist_name: str, \n                source_url: str) -&gt; dict:\n    \"\"\"\n    Analyzes clip content using GPT-4.\n    Returns relevance score, sentiment, and AI insights.\n    \"\"\"\n\ndef flexible_model_match(title: str, model: str) -&gt; bool:\n    \"\"\"\n    Intelligent matching for vehicle models in content.\n    Handles variations like \"X5\" matching \"BMW X5\", \"2024 X5\", etc.\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#main-entry-points","title":"Main Entry Points","text":"<pre><code>def run_ingest_concurrent(loans_data: List[dict], max_workers: int = 10) -&gt; tuple:\n    \"\"\"\n    Main entry point for concurrent processing.\n    Returns: (successful_results, rejected_loans)\n    \"\"\"\n\ndef run_ingest_test(file_path: str):\n    \"\"\"\n    Test entry point for development.\n    Processes first 5 loans from a file.\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/ingest/#inputs","title":"Inputs","text":"<ol> <li> <p>Loan Data Structure:    <code>python    {        'Work Order Number': 'WO12345',        'First Name': 'John',        'Last Name': 'Doe',        'Media Outlet': 'Car Magazine',        'Model': 'BMW X5',        'Start Date': '2024-01-15',        'End Date': '2024-01-22',        'URL 1': 'https://youtube.com/watch?v=...',        'URL 2': 'https://carmagazine.com/review/...'    }</code></p> </li> <li> <p>Configuration:</p> </li> <li>Date range fallback: 90 days \u2192 180 days</li> <li>Concurrent workers: 10 (configurable)</li> <li>Content length limits for GPT analysis</li> </ol>"},{"location":"modules/ingest/#outputs","title":"Outputs","text":"<ol> <li> <p>loan_results.csv:    <code>csv    Work Order Number,Model,Clip Link,AI Relevance Score,AI Insights,Sentiment Score,...    WO12345,BMW X5,https://...,85,\"Comprehensive review focusing on...\",0.8,...</code></p> </li> <li> <p>rejected_clips.csv:    <code>csv    Work Order Number,First Name,Last Name,Rejection Reason    WO12346,Jane,Smith,\"No valid clips found within date range\"</code></p> </li> </ol>"},{"location":"modules/ingest/#dependencies","title":"Dependencies","text":"<pre><code># External Libraries\nimport pandas as pd\nimport asyncio\nimport aiohttp\nfrom datetime import datetime, timedelta\nfrom dateutil import parser as date_parser\nfrom typing import List, Dict, Optional\n\n# Internal Modules\nfrom src.utils.logger import get_logger\nfrom src.utils.youtube_handler import extract_youtube_content\nfrom src.utils.crawler_manager import EnhancedCrawlerManager\nfrom src.utils.date_extractor import extract_published_date\nfrom src.analysis.gpt_analysis import analyze_content_gpt\n</code></pre>"},{"location":"modules/ingest/#processing-pipeline","title":"Processing Pipeline","text":"<ol> <li>Load Data \u2192 Parse CSV/Excel into loan dictionaries</li> <li>Concurrent Processing \u2192 Process multiple loans simultaneously</li> <li>For Each Loan:</li> <li>Extract URLs from loan data</li> <li>Process each URL based on type (YouTube/Web)</li> <li>Validate content publication date</li> <li>Analyze content with GPT</li> <li>Select best clip (highest relevance)</li> <li>Output Results \u2192 Save to CSV files</li> </ol>"},{"location":"modules/ingest/#error-handling","title":"Error Handling","text":"<ul> <li>Date Range Fallback: Extends from 90 to 180 days if no content found</li> <li>YouTube Fallback: Transcript \u2192 Metadata only</li> <li>Web Crawling Escalation: Basic \u2192 Enhanced \u2192 Headless browser</li> <li>Graceful Failures: Logs errors and continues processing</li> <li>Rejection Tracking: Detailed reasons for failed loans</li> </ul>"},{"location":"modules/ingest/#module-srcingestingest_databasepy","title":"Module: <code>src/ingest/ingest_database.py</code>","text":""},{"location":"modules/ingest/#purpose_1","title":"Purpose","text":"<p>The database-integrated ingest module provides similar functionality to the CSV version but stores results directly in Supabase. It implements smart retry logic to avoid reprocessing recent attempts and defers full GPT analysis to save costs. This module is designed for production use with real-time dashboard integration.</p>"},{"location":"modules/ingest/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"modules/ingest/#database-processing-functions","title":"Database Processing Functions","text":"<pre><code>def process_loan_for_database(loan: dict, db: DatabaseManager, \n                            outlets_mapping: dict, run_id: str) -&gt; tuple:\n    \"\"\"\n    Processes a loan and stores results in database.\n    Returns: (success_count, failure_count)\n    \"\"\"\n\nasync def process_loan_database_async(loan: dict, db: DatabaseManager,\n                                    outlets_mapping: dict, run_id: str,\n                                    semaphore: asyncio.Semaphore,\n                                    progress_callback=None) -&gt; tuple:\n    \"\"\"\n    Async version with progress callback support.\n    Enables real-time UI updates during processing.\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#validation-functions","title":"Validation Functions","text":"<pre><code>def is_url_from_authorized_outlet(url: str, journalist_name: str, \n                                 outlets_mapping: dict) -&gt; bool:\n    \"\"\"\n    Validates if URL belongs to journalist's authorized outlets.\n    Prevents processing unauthorized media sources.\n    \"\"\"\n\ndef load_person_outlets_mapping() -&gt; dict:\n    \"\"\"\n    Loads person-to-outlet authorization mapping.\n    Returns: Dict mapping person names to authorized outlets\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#scoring-functions","title":"Scoring Functions","text":"<pre><code>def calculate_relevance_score(content: str, model: str, \n                            use_gpt: bool = False) -&gt; float:\n    \"\"\"\n    Calculates content relevance score.\n    Can use GPT or fallback to keyword matching.\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#main-entry-points_1","title":"Main Entry Points","text":"<pre><code>def run_ingest_database(data_source: str):\n    \"\"\"\n    Main entry point for database ingestion.\n    Processes loans from file or URL.\n    \"\"\"\n\ndef run_ingest_database_with_filters(loans_data: List[dict], \n                                   progress_callback=None) -&gt; dict:\n    \"\"\"\n    Processes pre-filtered loans from dashboard.\n    Supports real-time progress updates.\n    Returns: Processing statistics\n    \"\"\"\n</code></pre>"},{"location":"modules/ingest/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"modules/ingest/#inputs_1","title":"Inputs","text":"<ul> <li>Same loan data structure as CSV version</li> <li>Additional Requirements:</li> <li>Person-to-outlet mapping JSON</li> <li>Database connection credentials</li> <li>Processing run ID for tracking</li> </ul>"},{"location":"modules/ingest/#outputs_1","title":"Outputs","text":"<ol> <li> <p>Database Records (clips table):    <code>sql    INSERT INTO clips (        wo_number, first_name, last_name, media_outlet,        model, start_date, end_date, url, content,        published_date, relevance_score, sentiment,        ai_insights, status, run_id, created_at    )</code></p> </li> <li> <p>Failed Attempts (clips table with status='failed'):</p> </li> <li>Stored with rejection reasons</li> <li> <p>Used for smart retry logic</p> </li> <li> <p>Processing Run Statistics:    <code>python    {        'processed': 50,        'successful': 45,        'failed': 5,        'duration': 120.5    }</code></p> </li> </ol>"},{"location":"modules/ingest/#dependencies_1","title":"Dependencies","text":"<pre><code># Database Integration\nfrom src.utils.database import DatabaseManager\n\n# Reused from CSV Version\nfrom src.ingest.ingest import (\n    process_youtube_url,\n    process_web_url,\n    flexible_model_match,\n    is_content_within_date_range\n)\n\n# Additional Utilities\nfrom src.utils.content_extractor import ContentExtractor\nfrom src.utils.sentiment_analysis import calculate_relevance_score_gpt\n</code></pre>"},{"location":"modules/ingest/#smart-retry-logic","title":"Smart Retry Logic","text":"<ol> <li>Check Recent Attempts: Skip if processed in last 24 hours</li> <li>Track Failed URLs: Store failure reasons in database</li> <li>Homepage Detection: Filter out index/homepage URLs</li> <li>Outlet Authorization: Only process authorized outlets</li> </ol>"},{"location":"modules/ingest/#database-schema-integration","title":"Database Schema Integration","text":"<pre><code>-- Processing Runs Table\nCREATE TABLE processing_runs (\n    id UUID PRIMARY KEY,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    total_loans INTEGER,\n    successful_clips INTEGER,\n    failed_clips INTEGER\n);\n\n-- Clips Table\nCREATE TABLE clips (\n    id SERIAL PRIMARY KEY,\n    wo_number VARCHAR,\n    url VARCHAR,\n    content TEXT,\n    relevance_score FLOAT,\n    status VARCHAR, -- 'pending', 'approved', 'rejected', 'failed'\n    run_id UUID REFERENCES processing_runs(id),\n    rejection_reason TEXT,\n    created_at TIMESTAMP\n);\n</code></pre>"},{"location":"modules/ingest/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Concurrent Processing: Default 5 workers (configurable)</li> <li>Smart Retry: Avoids redundant API calls</li> <li>Deferred Analysis: Only relevance scoring, full GPT on demand</li> <li>Batch Operations: Efficient database inserts</li> <li>Progress Callbacks: Real-time UI updates without polling</li> </ul>"},{"location":"modules/ingest/#error-handling_1","title":"Error Handling","text":"<ul> <li>Database Failures: Automatic reconnection with exponential backoff</li> <li>Duplicate Detection: Checks existing clips before processing</li> <li>Transaction Safety: Rollback on errors</li> <li>Comprehensive Logging: All failures tracked in database</li> <li>Graceful Degradation: Continues processing on individual failures</li> </ul>"},{"location":"modules/other-utils/","title":"Other Utilities Documentation","text":""},{"location":"modules/other-utils/#module-srcutilssentiment_analysispy","title":"Module: <code>src/utils/sentiment_analysis.py</code>","text":""},{"location":"modules/other-utils/#purpose","title":"Purpose","text":"<p>The Sentiment Analysis module provides a high-level interface for analyzing automotive media clips using GPT-4. It wraps the core analysis functionality with batch processing capabilities, progress tracking, and both synchronous and asynchronous execution patterns for UI integration.</p>"},{"location":"modules/other-utils/#key-functionsclasses","title":"Key Functions/Classes","text":""},{"location":"modules/other-utils/#sentimentanalyzer-class","title":"SentimentAnalyzer Class","text":"<pre><code>class SentimentAnalyzer:\n    \"\"\"\n    High-level sentiment analysis for automotive clips.\n    Supports batch processing with progress tracking.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize analyzer (placeholder for future config).\"\"\"\n</code></pre>"},{"location":"modules/other-utils/#core-methods","title":"Core Methods","text":"<pre><code>async def analyze_clip_sentiment(self, clip_data: Dict) -&gt; Dict:\n    \"\"\"\n    Analyze single clip asynchronously.\n    Returns comprehensive sentiment analysis.\n    \"\"\"\n\ndef analyze_clips_sync(self, clips: List[Dict], \n                      progress_callback=None) -&gt; List[Dict]:\n    \"\"\"\n    Synchronous batch analysis with progress updates.\n    Processes in batches of 5 to respect rate limits.\n    \"\"\"\n\nasync def analyze_clips_batch(self, clips: List[Dict],\n                            batch_size: int = 5) -&gt; List[Dict]:\n    \"\"\"\n    Asynchronous batch processing.\n    Implements concurrent processing with rate limiting.\n    \"\"\"\n</code></pre>"},{"location":"modules/other-utils/#utility-functions","title":"Utility Functions","text":"<pre><code>def run_sentiment_analysis(clips: List[Dict], \n                          progress_callback=None) -&gt; List[Dict]:\n    \"\"\"\n    Main entry point for sentiment analysis.\n    Handles async loop creation for sync callers.\n    \"\"\"\n\ndef calculate_relevance_score_gpt(content: str, make: str, \n                                model: str) -&gt; float:\n    \"\"\"\n    Calculate relevance score using GPT.\n    Cost-optimized version for database pipeline.\n    \"\"\"\n</code></pre>"},{"location":"modules/other-utils/#expected-inputsoutputs","title":"Expected Inputs/Outputs","text":""},{"location":"modules/other-utils/#inputs","title":"Inputs","text":"<pre><code># Single clip\n{\n    'content': 'Article or transcript text...',\n    'url': 'https://example.com/review',\n    'make': 'Honda',\n    'model': 'Accord'\n}\n\n# Batch processing\nclips = [clip1, clip2, clip3, ...]\nprogress_callback = lambda current, total: print(f\"{current}/{total}\")\n</code></pre>"},{"location":"modules/other-utils/#outputs","title":"Outputs","text":"<pre><code>{\n    'relevance_score': 85,\n    'overall_score': 8,\n    'overall_sentiment': 'positive',\n    'brand_alignment': True,\n    'summary': 'Comprehensive review highlighting...',\n    'aspects': {\n        'performance': {'score': 9, 'note': 'Excellent acceleration'},\n        'design': {'score': 7, 'note': 'Conservative but elegant'},\n        # ... other aspects\n    },\n    'pros': ['Fuel efficiency', 'Reliability', 'Tech features'],\n    'cons': ['Road noise', 'Firm suspension'],\n    'recommendation': 'Strong buy for family sedan buyers'\n}\n</code></pre>"},{"location":"modules/other-utils/#dependencies","title":"Dependencies","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Optional, Callable\n\nfrom src.analysis.gpt_analysis import analyze_clip\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"modules/other-utils/#processing-features","title":"Processing Features","text":"<ol> <li>Batch Size Management: Processes 5 clips at a time to avoid rate limits</li> <li>Progress Tracking: Real-time updates for UI progress bars</li> <li>Error Resilience: Continues processing even if individual clips fail</li> <li>Async/Sync Bridge: Handles async operations for sync callers</li> <li>Delay Management: 1-second delay between batches</li> </ol>"},{"location":"modules/other-utils/#module-srcutilsnotificationspy","title":"Module: <code>src/utils/notifications.py</code>","text":""},{"location":"modules/other-utils/#purpose_1","title":"Purpose","text":"<p>The Notifications module provides Slack webhook integration for sending real-time alerts and status updates. It implements retry logic with exponential backoff and supports formatted messages for better visibility in Slack channels.</p>"},{"location":"modules/other-utils/#key-functionsclasses_1","title":"Key Functions/Classes","text":""},{"location":"modules/other-utils/#core-function","title":"Core Function","text":"<pre><code>def send_slack_message(message: str, webhook_url: str = None, \n                      max_retries: int = 3) -&gt; bool:\n    \"\"\"\n    Send notification to Slack channel.\n    Implements retry with exponential backoff.\n    \"\"\"\n</code></pre>"},{"location":"modules/other-utils/#message-formatting","title":"Message Formatting","text":"<ol> <li>Status Prefixes:</li> <li><code>\"approved\"</code> \u2192 \u2705 prefix</li> <li><code>\"rejected\"</code> \u2192 \u274c prefix</li> <li> <p>Others \u2192 no prefix</p> </li> <li> <p>Block Format:    <code>json    {        \"blocks\": [{            \"type\": \"section\",            \"text\": {                \"type\": \"mrkdwn\",                \"text\": \"\u2705 Clip approved for Honda Accord\"            }        }]    }</code></p> </li> </ol>"},{"location":"modules/other-utils/#expected-inputsoutputs_1","title":"Expected Inputs/Outputs","text":""},{"location":"modules/other-utils/#inputs_1","title":"Inputs","text":"<pre><code># Simple message\nsend_slack_message(\"Processing completed\")\n\n# Status message\nsend_slack_message(\"Clip approved for Honda Accord\")\n\n# Custom webhook\nsend_slack_message(\"Alert!\", webhook_url=\"https://hooks.slack.com/...\")\n</code></pre>"},{"location":"modules/other-utils/#outputs_1","title":"Outputs","text":"<ul> <li><code>True</code>: Message sent successfully</li> <li><code>False</code>: Failed after all retries</li> </ul>"},{"location":"modules/other-utils/#dependencies_1","title":"Dependencies","text":"<pre><code>import os\nimport json\nimport time\nimport requests\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"modules/other-utils/#configuration","title":"Configuration","text":"<pre><code># Environment variable\nexport SLACK_WEBHOOK_URL=\"https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\"\n</code></pre>"},{"location":"modules/other-utils/#error-handling","title":"Error Handling","text":"<ol> <li>Retry Strategy: 3 attempts with exponential backoff (1s, 2s, 4s)</li> <li>HTTP Errors: Catches and logs all request exceptions</li> <li>Missing Webhook: Returns False with warning log</li> <li>Non-blocking: Never raises exceptions to caller</li> </ol>"},{"location":"modules/other-utils/#module-srcutilsrate_limiterpy","title":"Module: <code>src/utils/rate_limiter.py</code>","text":""},{"location":"modules/other-utils/#purpose_2","title":"Purpose","text":"<p>The Rate Limiter implements a token bucket algorithm to enforce rate limits on external API calls. It provides per-domain rate limiting with automatic domain detection and thread-safe operation for concurrent requests.</p>"},{"location":"modules/other-utils/#key-functionsclasses_2","title":"Key Functions/Classes","text":""},{"location":"modules/other-utils/#ratelimiter-class","title":"RateLimiter Class","text":"<pre><code>class RateLimiter:\n    \"\"\"\n    Token bucket rate limiter with per-domain limits.\n    Thread-safe implementation for concurrent access.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with default rate configurations.\"\"\"\n</code></pre>"},{"location":"modules/other-utils/#core-methods_1","title":"Core Methods","text":"<pre><code>def wait_if_needed(self, url_or_domain: str, \n                  custom_rate: float = None,\n                  custom_per: float = None):\n    \"\"\"\n    Wait if rate limit would be exceeded.\n    Automatically detects and normalizes domains.\n    \"\"\"\n\ndef _ensure_bucket_exists(self, domain: str, rate: float, per: float):\n    \"\"\"\n    Create token bucket for new domain.\n    Initializes with full token capacity.\n    \"\"\"\n\ndef _refill_bucket(self, domain: str):\n    \"\"\"\n    Refill tokens based on elapsed time.\n    Implements token bucket algorithm.\n    \"\"\"\n</code></pre>"},{"location":"modules/other-utils/#default-rate-limits","title":"Default Rate Limits","text":"<pre><code>default_rates = {\n    'openai.com': (5, 60),      # 5 requests per minute\n    'youtube.com': (10, 60),    # 10 requests per minute\n    'googleapis.com': (10, 60), # 10 requests per minute\n    # Unknown domains: 1 request per 2 seconds\n}\n</code></pre>"},{"location":"modules/other-utils/#expected-inputsoutputs_2","title":"Expected Inputs/Outputs","text":""},{"location":"modules/other-utils/#inputs_2","title":"Inputs","text":"<pre><code># URL-based (auto-detects domain)\nrate_limiter.wait_if_needed(\"https://api.openai.com/v1/completions\")\n\n# Domain-based\nrate_limiter.wait_if_needed(\"openai.com\")\n\n# Custom rate\nrate_limiter.wait_if_needed(\"custom-api.com\", custom_rate=100, custom_per=60)\n</code></pre>"},{"location":"modules/other-utils/#outputs_2","title":"Outputs","text":"<ul> <li>No return value</li> <li>Blocks (sleeps) if rate limit would be exceeded</li> <li>Logs wait times when blocking</li> </ul>"},{"location":"modules/other-utils/#dependencies_2","title":"Dependencies","text":"<pre><code>import time\nimport threading\nfrom urllib.parse import urlparse\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"modules/other-utils/#token-bucket-algorithm","title":"Token Bucket Algorithm","text":"<ol> <li>Bucket Capacity: Equal to rate limit (e.g., 5 tokens)</li> <li>Refill Rate: Based on configured rate/period</li> <li>Token Consumption: 1 token per request</li> <li>Blocking: Waits for next token if bucket empty</li> <li>Thread Safety: Uses locks for concurrent access</li> </ol>"},{"location":"modules/other-utils/#module-srcutilscache_managerpy","title":"Module: <code>src/utils/cache_manager.py</code>","text":""},{"location":"modules/other-utils/#purpose_3","title":"Purpose","text":"<p>The Cache Manager provides SQLite-based caching for web scraping results with automatic expiration. IMPORTANT: Caching is currently DISABLED - the <code>get_cached_result</code> method always returns None to ensure fresh data retrieval.</p>"},{"location":"modules/other-utils/#key-functionsclasses_3","title":"Key Functions/Classes","text":""},{"location":"modules/other-utils/#cachemanager-class","title":"CacheManager Class","text":"<pre><code>class CacheManager:\n    \"\"\"\n    SQLite-based cache for scraping results.\n    Currently DISABLED - always returns cache miss.\n    \"\"\"\n\n    def __init__(self, db_path: str = None):\n        \"\"\"Initialize SQLite database with schema.\"\"\"\n</code></pre>"},{"location":"modules/other-utils/#core-methods_2","title":"Core Methods","text":"<pre><code>def get_cached_result(self, person_id: str, domain: str,\n                     make: str, model: str) -&gt; Optional[Dict]:\n    \"\"\"\n    Check for cached result. \n    CURRENTLY DISABLED - always returns None.\n    \"\"\"\n\ndef store_result(self, person_id: str, domain: str, make: str,\n                model: str, url: str, content: str,\n                metadata: Dict = None) -&gt; bool:\n    \"\"\"\n    Store scraping result with 24-hour TTL.\n    Still functional for future use.\n    \"\"\"\n\ndef cleanup_expired(self) -&gt; int:\n    \"\"\"\n    Remove expired cache entries.\n    Returns count of deleted entries.\n    \"\"\"\n\ndef get_cache_stats(self) -&gt; Dict:\n    \"\"\"\n    Get cache statistics.\n    Returns entry counts and sizes.\n    \"\"\"\n</code></pre>"},{"location":"modules/other-utils/#database-schema","title":"Database Schema","text":"<pre><code>CREATE TABLE scraping_cache (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    person_id TEXT NOT NULL,\n    domain TEXT NOT NULL,\n    make TEXT NOT NULL,\n    model TEXT NOT NULL,\n    url TEXT NOT NULL,\n    content TEXT NOT NULL,\n    metadata TEXT,  -- JSON string\n    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    expires_at TIMESTAMP NOT NULL,\n    UNIQUE(person_id, domain, make, model)\n);\n\n-- Indexes for performance\nCREATE INDEX idx_cache_lookup ON scraping_cache(person_id, domain, make, model);\nCREATE INDEX idx_cache_expiry ON scraping_cache(expires_at);\n</code></pre>"},{"location":"modules/other-utils/#configuration_1","title":"Configuration","text":"<ul> <li>Database Path: <code>data/scraping_cache.db</code></li> <li>TTL: 24 hours (86,400 seconds)</li> <li>Cache Status: DISABLED for reliability</li> </ul>"},{"location":"modules/other-utils/#dependencies_3","title":"Dependencies","text":"<pre><code>import sqlite3\nimport json\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom src.utils.logger import setup_logger\n</code></pre>"},{"location":"modules/other-utils/#module-srcutilsmodel_variationspy","title":"Module: <code>src/utils/model_variations.py</code>","text":""},{"location":"modules/other-utils/#purpose_4","title":"Purpose","text":"<p>The Model Variations module generates multiple formatting variations of vehicle model names to improve search coverage across different automotive websites. It handles common patterns like hyphenation, spacing, and make-model combinations.</p>"},{"location":"modules/other-utils/#key-functionsclasses_4","title":"Key Functions/Classes","text":""},{"location":"modules/other-utils/#core-function_1","title":"Core Function","text":"<pre><code>def generate_model_variations(make: str, model: str) -&gt; List[str]:\n    \"\"\"\n    Generate model name variations for better search coverage.\n    Handles spacing, hyphenation, and make combinations.\n    \"\"\"\n</code></pre>"},{"location":"modules/other-utils/#variation-strategies","title":"Variation Strategies","text":"<ol> <li>Space/Hyphen Variations:</li> <li>\"CX-90\" \u2192 [\"cx-90\", \"cx 90\", \"cx90\"]</li> <li> <p>Handles both directions: with/without spaces/hyphens</p> </li> <li> <p>Make Prefix Combinations:</p> </li> <li>Adds make name: \"accord\" \u2192 \"honda accord\"</li> <li> <p>Both spaced and unspaced versions</p> </li> <li> <p>Numeric Patterns:</p> </li> <li>\"3 Series\" \u2194 \"3series\"</li> <li> <p>\"ES 350\" \u2194 \"ES350\"</p> </li> <li> <p>Abbreviation Handling:</p> </li> <li>Very limited due to ambiguity concerns</li> <li>Only complete word matches</li> </ol>"},{"location":"modules/other-utils/#expected-inputsoutputs_3","title":"Expected Inputs/Outputs","text":""},{"location":"modules/other-utils/#inputs_3","title":"Inputs","text":"<pre><code>generate_model_variations(\"Mazda\", \"CX-90\")\n</code></pre>"},{"location":"modules/other-utils/#outputs_3","title":"Outputs","text":"<pre><code>[\n    \"cx-90\",\n    \"cx 90\", \n    \"cx90\",\n    \"mazda cx-90\",\n    \"mazda cx 90\",\n    \"mazda cx90\",\n    \"mazdacx-90\",\n    \"mazdacx 90\",\n    \"mazdacx90\"\n]\n</code></pre>"},{"location":"modules/other-utils/#dependencies_4","title":"Dependencies","text":"<pre><code>import re\nfrom typing import List\n</code></pre>"},{"location":"modules/other-utils/#variation-examples","title":"Variation Examples","text":"Make Model Key Variations Lexus ES 350 es350, lexus es 350 BMW 3 Series 3series, bmw 3 series Honda CR-V cr-v, crv, cr v, honda crv Tesla Model 3 model3, tesla model 3"},{"location":"modules/other-utils/#design-decisions","title":"Design Decisions","text":"<ol> <li>Conservative Approach: Avoids aggressive abbreviations</li> <li>Case Insensitive: All variations in lowercase</li> <li>De-duplication: Uses sets to prevent duplicates</li> <li>Performance: Pre-compiled regex patterns</li> </ol>"}]}